nohup: ignoring input
Namespace(model_name_or_path='meta-llama/Llama-2-7b-hf', trust_remote_code=False, use_auth_token=True, eval_dataset_size=10, max_train_samples=None, max_eval_samples=10, source_max_len=16, target_max_len=512, dataset='/home/dqwang/scratch/tongchen/qlora/distilled_dataset.json', dataset_format=None, output_dir='./output/llama-2-alpaca-clean-7b', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=5, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/llama-2-alpaca-clean-7b/runs/Jul24_07-09-41_d2', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=100, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=3, dataloader_num_workers=1, past_index=-1, run_name='llama2_small', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=['wandb'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=True, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 32,
  "transformers_version": "4.30.0.dev0"
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, wandb_project='qlora-buffer-try', save_interval=1, save_dir='/shared/dqwang/scratch/tongchen/qlora/try', distributed_state=Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0))
Detected that training was already completed!
loading base model meta-llama/Llama-2-7b-hf...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.41s/it]
Adding special tokens.
adding LoRA modules...
loaded model
base_model.model.model.embed_tokens.weight Parameter containing:
tensor([[ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,
         -6.5565e-06,  8.9407e-07],
        [ 1.8616e-03, -3.3722e-03,  3.9864e-04,  ..., -8.3008e-03,
          2.5787e-03, -3.9368e-03],
        [ 1.0986e-02,  9.8877e-03, -5.0964e-03,  ...,  2.5177e-03,
          7.7057e-04, -5.0049e-03],
        ...,
        [-1.0742e-02,  9.3384e-03,  1.2939e-02,  ..., -3.3203e-02,
         -1.6357e-02,  3.3875e-03],
        [-8.3008e-03, -4.0588e-03, -1.1063e-03,  ...,  3.4790e-03,
         -1.2939e-02,  3.1948e-05],
        [ 7.0953e-04,  6.8283e-04, -4.5013e-04,  ..., -4.2915e-04,
         -8.9645e-05, -4.7874e-04]], device='cuda:0', dtype=torch.bfloat16)
base_model.model.model.layers.0.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 83],
            [103],
            [ 74],
            ...,
            [114],
            [108],
            [181]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-1.2146e-02,  3.3417e-03, -9.5215e-03,  ..., -1.2939e-02,
          6.0654e-04,  1.4709e-02],
        [-8.4229e-03, -3.8147e-03,  8.6670e-03,  ..., -1.3367e-02,
         -2.0294e-03, -1.1230e-02],
        [-1.1902e-02,  1.2329e-02,  7.7515e-03,  ..., -6.5918e-03,
         -6.3896e-05, -1.2512e-02],
        ...,
        [ 1.4709e-02, -1.0620e-02, -9.5825e-03,  ..., -1.0925e-02,
          1.1719e-02,  7.2937e-03],
        [-1.2146e-02, -3.1891e-03,  1.4587e-02,  ..., -1.5106e-03,
          1.2451e-02, -2.2278e-03],
        [ 2.3499e-03, -6.2866e-03,  1.1292e-02,  ..., -3.5706e-03,
          7.5684e-03,  4.9438e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.0.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 73],
            [120],
            [120],
            ...,
            [101],
            [104],
            [104]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0067,  0.0053,  0.0123,  ..., -0.0110,  0.0060, -0.0002],
        [-0.0134,  0.0056,  0.0039,  ..., -0.0110, -0.0087,  0.0100],
        [-0.0032, -0.0111, -0.0076,  ..., -0.0037, -0.0084, -0.0121],
        ...,
        [ 0.0065, -0.0048,  0.0096,  ...,  0.0113,  0.0038,  0.0018],
        [-0.0022,  0.0058,  0.0126,  ..., -0.0145,  0.0103,  0.0149],
        [ 0.0040, -0.0101,  0.0110,  ...,  0.0035, -0.0109,  0.0068]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.0.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[119],
            [138],
            [153],
            ...,
            [205],
            [ 23],
            [120]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0096, -0.0021,  0.0147,  ..., -0.0118,  0.0066,  0.0010],
        [ 0.0039, -0.0034, -0.0033,  ..., -0.0117,  0.0056, -0.0075],
        [ 0.0148,  0.0050,  0.0021,  ..., -0.0018, -0.0107, -0.0103],
        ...,
        [-0.0040, -0.0143,  0.0033,  ..., -0.0054,  0.0077, -0.0056],
        [ 0.0135, -0.0068, -0.0016,  ..., -0.0015, -0.0101,  0.0052],
        [ 0.0140, -0.0153, -0.0071,  ..., -0.0082, -0.0142,  0.0138]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.0.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[117],
            [195],
            [ 87],
            ...,
            [158],
            [204],
            [ 34]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0031,  0.0006, -0.0071,  ..., -0.0097,  0.0061, -0.0005],
        [-0.0058, -0.0155,  0.0072,  ..., -0.0081, -0.0129,  0.0025],
        [-0.0018, -0.0001,  0.0067,  ..., -0.0051,  0.0127, -0.0042],
        ...,
        [ 0.0143,  0.0089, -0.0082,  ...,  0.0039,  0.0103,  0.0151],
        [ 0.0070, -0.0124, -0.0153,  ..., -0.0053,  0.0132,  0.0063],
        [-0.0040,  0.0115,  0.0067,  ...,  0.0084, -0.0127,  0.0067]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.0.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[204],
            [228],
            [226],
            ...,
            [ 25],
            [ 23],
            [146]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0020, -0.0090, -0.0112,  ..., -0.0005,  0.0151,  0.0137],
        [-0.0107, -0.0027, -0.0128,  ...,  0.0006,  0.0073,  0.0060],
        [-0.0137,  0.0134, -0.0116,  ...,  0.0039,  0.0097, -0.0110],
        ...,
        [ 0.0120,  0.0065,  0.0151,  ..., -0.0148,  0.0061, -0.0039],
        [ 0.0070, -0.0113,  0.0090,  ..., -0.0085, -0.0045,  0.0042],
        [-0.0052,  0.0073, -0.0140,  ..., -0.0117, -0.0088,  0.0132]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.0.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[132],
            [149],
            [204],
            ...,
            [ 73],
            [103],
            [ 98]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-1.2741e-03,  5.9204e-03,  8.9264e-04,  ...,  4.5471e-03,
          1.2589e-03, -4.2419e-03],
        [-1.8387e-03, -2.8076e-03,  4.3640e-03,  ...,  2.4261e-03,
         -9.1553e-03,  8.0566e-03],
        [-3.3569e-03, -5.3024e-04, -8.0566e-03,  ...,  4.7913e-03,
          5.7068e-03, -5.7373e-03],
        ...,
        [ 1.7014e-03, -7.1049e-05, -6.1035e-04,  ..., -1.6251e-03,
          6.3477e-03,  8.4839e-03],
        [-8.3008e-03,  9.0942e-03, -9.5215e-03,  ...,  8.2397e-03,
          1.5182e-03, -3.9978e-03],
        [ 4.6387e-03,  2.0142e-03,  4.0588e-03,  ..., -2.4915e-05,
         -2.0294e-03,  4.3030e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.0.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[113],
            [180],
            [ 20],
            ...,
            [ 57],
            [154],
            [ 48]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0126, -0.0100, -0.0022,  ...,  0.0076, -0.0012, -0.0080],
        [-0.0125, -0.0074,  0.0011,  ..., -0.0129,  0.0150,  0.0142],
        [-0.0143, -0.0146, -0.0100,  ..., -0.0038,  0.0132,  0.0143],
        ...,
        [ 0.0055,  0.0073,  0.0149,  ...,  0.0095,  0.0148,  0.0085],
        [-0.0077,  0.0124, -0.0059,  ..., -0.0066,  0.0090, -0.0023],
        [ 0.0074,  0.0105,  0.0115,  ...,  0.0148, -0.0012,  0.0066]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.0.input_layernorm.weight Parameter containing:
tensor([0.0297, 0.0136, 0.0020,  ..., 0.0103, 0.0110, 0.0061], device='cuda:0')
base_model.model.model.layers.0.post_attention_layernorm.weight Parameter containing:
tensor([0.0503, 0.0525, 0.0500,  ..., 0.0525, 0.0535, 0.0491], device='cuda:0')
base_model.model.model.layers.1.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 88],
            [ 40],
            [155],
            ...,
            [ 71],
            [148],
            [114]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0056,  0.0039, -0.0133,  ...,  0.0094, -0.0036,  0.0103],
        [-0.0082,  0.0047, -0.0079,  ...,  0.0027,  0.0066,  0.0071],
        [-0.0089,  0.0111,  0.0088,  ..., -0.0001, -0.0125, -0.0124],
        ...,
        [ 0.0033,  0.0139,  0.0139,  ..., -0.0023,  0.0038,  0.0089],
        [-0.0044, -0.0005, -0.0145,  ...,  0.0118, -0.0005, -0.0036],
        [-0.0115, -0.0066,  0.0113,  ...,  0.0035,  0.0049, -0.0090]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.1.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 38],
            [228],
            [ 71],
            ...,
            [212],
            [ 42],
            [107]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0078,  0.0146,  0.0041,  ...,  0.0066, -0.0040, -0.0144],
        [-0.0037, -0.0060,  0.0122,  ...,  0.0018,  0.0115,  0.0090],
        [-0.0096,  0.0058,  0.0087,  ..., -0.0050, -0.0097, -0.0147],
        ...,
        [ 0.0050,  0.0036, -0.0122,  ...,  0.0003,  0.0126,  0.0068],
        [-0.0142,  0.0035, -0.0135,  ...,  0.0134,  0.0038,  0.0005],
        [-0.0135, -0.0041,  0.0121,  ...,  0.0098,  0.0128, -0.0086]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.1.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[133],
            [104],
            [154],
            ...,
            [228],
            [ 89],
            [ 61]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0069, -0.0067, -0.0109,  ..., -0.0051,  0.0047, -0.0127],
        [ 0.0078,  0.0070, -0.0089,  ..., -0.0135, -0.0032, -0.0148],
        [-0.0114,  0.0006, -0.0129,  ...,  0.0035,  0.0011,  0.0093],
        ...,
        [ 0.0145, -0.0056,  0.0025,  ...,  0.0025, -0.0084,  0.0068],
        [ 0.0150,  0.0142,  0.0038,  ..., -0.0023, -0.0087,  0.0117],
        [-0.0010,  0.0156, -0.0054,  ..., -0.0050, -0.0033, -0.0112]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.1.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[145],
            [213],
            [198],
            ...,
            [ 71],
            [217],
            [ 42]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0041, -0.0071, -0.0074,  ...,  0.0098,  0.0074,  0.0073],
        [ 0.0145, -0.0080,  0.0015,  ..., -0.0091,  0.0128, -0.0093],
        [ 0.0128, -0.0074, -0.0049,  ..., -0.0067,  0.0071, -0.0130],
        ...,
        [-0.0139,  0.0129,  0.0119,  ...,  0.0040,  0.0008, -0.0030],
        [ 0.0106, -0.0118, -0.0107,  ..., -0.0101, -0.0011, -0.0005],
        [-0.0040, -0.0046,  0.0009,  ...,  0.0007,  0.0131, -0.0131]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.1.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[229],
            [246],
            [163],
            ...,
            [152],
            [ 78],
            [161]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0011,  0.0070, -0.0039,  ..., -0.0007,  0.0068,  0.0047],
        [-0.0142,  0.0132, -0.0083,  ...,  0.0050,  0.0048, -0.0109],
        [-0.0091,  0.0114,  0.0006,  ...,  0.0007,  0.0061, -0.0003],
        ...,
        [ 0.0023,  0.0128, -0.0091,  ..., -0.0154,  0.0027, -0.0109],
        [ 0.0050,  0.0108, -0.0128,  ..., -0.0090, -0.0120, -0.0123],
        [ 0.0070, -0.0086,  0.0074,  ...,  0.0139, -0.0067, -0.0069]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.1.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[153],
            [248],
            [186],
            ...,
            [ 70],
            [131],
            [ 62]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0080, -0.0014, -0.0080,  ...,  0.0057,  0.0074, -0.0015],
        [ 0.0017, -0.0057, -0.0036,  ...,  0.0032,  0.0058,  0.0062],
        [-0.0090, -0.0029,  0.0039,  ...,  0.0047,  0.0007, -0.0043],
        ...,
        [-0.0025, -0.0006, -0.0055,  ..., -0.0090, -0.0070,  0.0045],
        [-0.0026, -0.0023,  0.0054,  ..., -0.0084, -0.0023,  0.0020],
        [-0.0047,  0.0081,  0.0028,  ..., -0.0017, -0.0090,  0.0075]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.1.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[143],
            [ 61],
            [149],
            ...,
            [106],
            [ 18],
            [ 92]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0026, -0.0015,  0.0144,  ..., -0.0067, -0.0014, -0.0129],
        [-0.0065, -0.0049, -0.0115,  ...,  0.0051,  0.0101,  0.0083],
        [-0.0014, -0.0034, -0.0023,  ..., -0.0067, -0.0124, -0.0118],
        ...,
        [ 0.0062,  0.0131, -0.0085,  ...,  0.0143, -0.0038, -0.0068],
        [ 0.0151, -0.0055, -0.0096,  ...,  0.0117,  0.0086,  0.0135],
        [-0.0042,  0.0048,  0.0140,  ..., -0.0140, -0.0055,  0.0140]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.1.input_layernorm.weight Parameter containing:
tensor([0.1138, 0.1099, 0.1006,  ..., 0.0630, 0.0942, 0.0742], device='cuda:0')
base_model.model.model.layers.1.post_attention_layernorm.weight Parameter containing:
tensor([0.0996, 0.1006, 0.0962,  ..., 0.1074, 0.0996, 0.1016], device='cuda:0')
base_model.model.model.layers.2.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 20],
            [172],
            [210],
            ...,
            [199],
            [ 62],
            [ 23]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0138, -0.0070, -0.0018,  ...,  0.0106, -0.0127,  0.0004],
        [-0.0117,  0.0041,  0.0070,  ..., -0.0153,  0.0045, -0.0045],
        [ 0.0010, -0.0039,  0.0126,  ..., -0.0098,  0.0153,  0.0005],
        ...,
        [-0.0020,  0.0129, -0.0125,  ..., -0.0154, -0.0065,  0.0145],
        [-0.0004, -0.0094,  0.0004,  ...,  0.0059, -0.0078, -0.0112],
        [ 0.0131,  0.0073,  0.0125,  ...,  0.0098,  0.0089, -0.0095]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.2.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[107],
            [ 77],
            [ 91],
            ...,
            [125],
            [192],
            [ 79]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0082,  0.0097, -0.0015,  ..., -0.0103,  0.0039, -0.0085],
        [-0.0101, -0.0103, -0.0051,  ...,  0.0092,  0.0050, -0.0023],
        [ 0.0101, -0.0093, -0.0048,  ..., -0.0067, -0.0136,  0.0079],
        ...,
        [-0.0093, -0.0146,  0.0087,  ...,  0.0041,  0.0123, -0.0112],
        [-0.0017,  0.0141,  0.0100,  ...,  0.0117, -0.0045, -0.0091],
        [ 0.0018, -0.0118, -0.0082,  ..., -0.0090, -0.0150, -0.0049]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.2.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[103],
            [215],
            [133],
            ...,
            [152],
            [165],
            [197]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0133,  0.0010, -0.0043,  ...,  0.0060,  0.0018, -0.0084],
        [ 0.0045, -0.0139,  0.0079,  ...,  0.0156, -0.0142,  0.0068],
        [ 0.0099,  0.0125, -0.0150,  ...,  0.0077, -0.0145, -0.0052],
        ...,
        [ 0.0084,  0.0084,  0.0090,  ...,  0.0069,  0.0006, -0.0154],
        [ 0.0004, -0.0132,  0.0115,  ..., -0.0067, -0.0043,  0.0154],
        [-0.0011,  0.0075,  0.0015,  ..., -0.0013,  0.0088,  0.0117]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.2.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[123],
            [155],
            [ 38],
            ...,
            [120],
            [100],
            [179]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-1.0376e-02,  2.5177e-03,  7.1411e-03,  ...,  2.4319e-05,
          3.3875e-03, -4.2725e-03],
        [ 5.2643e-04,  7.5684e-03,  6.5308e-03,  ...,  1.2939e-02,
         -7.8735e-03,  5.0049e-03],
        [-1.4709e-02,  1.4771e-02, -1.0986e-02,  ..., -1.5015e-02,
          1.0071e-02, -8.8501e-04],
        ...,
        [ 1.1597e-02, -1.0529e-03,  9.4604e-03,  ..., -9.6512e-04,
          8.5449e-03, -2.6093e-03],
        [-1.4160e-02, -1.3062e-02, -9.7656e-03,  ..., -1.1826e-03,
          7.8125e-03,  4.4861e-03],
        [-1.4404e-02,  9.5215e-03, -9.2163e-03,  ...,  1.2207e-03,
         -1.5259e-02, -1.3367e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.2.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[139],
            [ 88],
            [105],
            ...,
            [123],
            [244],
            [ 25]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-1.4038e-03,  3.6163e-03,  1.0071e-02,  ..., -8.8501e-03,
          9.1553e-05, -3.0365e-03],
        [ 2.6245e-03,  8.4839e-03, -4.6997e-03,  ...,  1.4221e-02,
          9.8877e-03,  6.5613e-03],
        [ 3.3112e-03, -3.6316e-03, -1.1841e-02,  ...,  2.0599e-03,
          7.5989e-03, -4.8523e-03],
        ...,
        [ 7.4463e-03,  7.1106e-03,  9.0942e-03,  ...,  5.5790e-05,
         -1.4343e-02, -7.1411e-03],
        [-2.8839e-03,  9.1553e-03,  6.0730e-03,  ..., -6.1951e-03,
          1.1536e-02,  1.9989e-03],
        [-4.3640e-03, -1.3245e-02,  1.0254e-02,  ...,  3.9864e-04,
         -4.7874e-04,  8.4839e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.2.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[154],
            [212],
            [109],
            ...,
            [208],
            [109],
            [  5]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0071,  0.0074,  0.0012,  ...,  0.0059,  0.0010,  0.0087],
        [ 0.0055, -0.0013,  0.0006,  ...,  0.0063, -0.0077,  0.0049],
        [ 0.0067,  0.0085, -0.0079,  ...,  0.0035,  0.0040, -0.0014],
        ...,
        [ 0.0066, -0.0045,  0.0004,  ...,  0.0090,  0.0076, -0.0023],
        [ 0.0032, -0.0095,  0.0059,  ..., -0.0094, -0.0068,  0.0004],
        [-0.0039, -0.0063, -0.0070,  ...,  0.0080, -0.0022,  0.0018]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.2.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[119],
            [153],
            [139],
            ...,
            [168],
            [ 69],
            [117]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0056,  0.0041,  0.0093,  ..., -0.0020,  0.0101,  0.0028],
        [-0.0109,  0.0071, -0.0063,  ...,  0.0019, -0.0139, -0.0155],
        [ 0.0125, -0.0040, -0.0147,  ...,  0.0058,  0.0013,  0.0151],
        ...,
        [-0.0058, -0.0145, -0.0147,  ..., -0.0148, -0.0085,  0.0137],
        [ 0.0077,  0.0150, -0.0109,  ...,  0.0087,  0.0077,  0.0062],
        [ 0.0058,  0.0047,  0.0120,  ...,  0.0003, -0.0122,  0.0037]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.2.input_layernorm.weight Parameter containing:
tensor([0.1738, 0.1777, 0.1738,  ..., 0.1768, 0.1709, 0.1748], device='cuda:0')
base_model.model.model.layers.2.post_attention_layernorm.weight Parameter containing:
tensor([0.1338, 0.1367, 0.1357,  ..., 0.1357, 0.1387, 0.1357], device='cuda:0')
base_model.model.model.layers.3.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[172],
            [115],
            [246],
            ...,
            [ 89],
            [181],
            [102]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0137, -0.0026, -0.0142,  ...,  0.0121,  0.0135,  0.0115],
        [ 0.0103, -0.0036, -0.0051,  ..., -0.0106,  0.0033,  0.0146],
        [-0.0029, -0.0143,  0.0142,  ...,  0.0131, -0.0113,  0.0063],
        ...,
        [ 0.0099, -0.0132,  0.0100,  ...,  0.0017, -0.0082, -0.0148],
        [ 0.0067,  0.0048,  0.0033,  ...,  0.0069, -0.0007,  0.0080],
        [-0.0145, -0.0115,  0.0031,  ..., -0.0120,  0.0051, -0.0046]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.3.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[149],
            [ 81],
            [173],
            ...,
            [ 88],
            [150],
            [134]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 1.9684e-03,  1.1719e-02, -4.8218e-03,  ...,  7.2632e-03,
          1.1597e-02,  9.1553e-03],
        [-3.2349e-03,  9.9182e-04,  1.0071e-02,  ..., -1.2695e-02,
          7.2632e-03,  1.0925e-02],
        [-5.5237e-03, -7.3547e-03, -1.3306e-02,  ...,  1.4709e-02,
         -4.7302e-03,  1.5015e-02],
        ...,
        [ 7.3547e-03, -7.0801e-03,  1.3489e-02,  ..., -5.9204e-03,
          1.0864e-02, -6.3171e-03],
        [ 1.0559e-02, -1.0010e-02,  4.4861e-03,  ...,  1.3611e-02,
          1.1414e-02,  8.9111e-03],
        [-6.9618e-05,  7.4768e-03,  1.3306e-02,  ...,  6.7139e-03,
          7.7209e-03, -2.5787e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.3.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[108],
            [ 88],
            [ 77],
            ...,
            [105],
            [ 55],
            [142]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0131,  0.0010,  0.0149,  ...,  0.0074, -0.0063,  0.0117],
        [-0.0033,  0.0007,  0.0118,  ..., -0.0004,  0.0080, -0.0107],
        [-0.0143, -0.0126,  0.0059,  ...,  0.0112,  0.0052,  0.0044],
        ...,
        [-0.0082,  0.0147, -0.0018,  ..., -0.0074, -0.0006,  0.0145],
        [-0.0094,  0.0087,  0.0085,  ...,  0.0081,  0.0027, -0.0018],
        [ 0.0025, -0.0061, -0.0047,  ..., -0.0038, -0.0103, -0.0072]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.3.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[ 25],
            [ 38],
            [ 85],
            ...,
            [115],
            [201],
            [156]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0064, -0.0030,  0.0030,  ..., -0.0051, -0.0132, -0.0063],
        [ 0.0004, -0.0095, -0.0134,  ...,  0.0060, -0.0012, -0.0039],
        [ 0.0148, -0.0031, -0.0067,  ..., -0.0026, -0.0075,  0.0043],
        ...,
        [-0.0002,  0.0095, -0.0140,  ...,  0.0052, -0.0095,  0.0022],
        [-0.0139, -0.0132, -0.0085,  ...,  0.0087,  0.0143, -0.0030],
        [-0.0106, -0.0007,  0.0003,  ...,  0.0045,  0.0114, -0.0082]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.3.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[101],
            [ 68],
            [139],
            ...,
            [238],
            [173],
            [ 26]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0038, -0.0053, -0.0123,  ...,  0.0140,  0.0036, -0.0049],
        [-0.0148,  0.0122, -0.0049,  ...,  0.0121, -0.0028,  0.0135],
        [ 0.0147,  0.0017,  0.0098,  ..., -0.0038,  0.0079, -0.0035],
        ...,
        [-0.0118,  0.0046,  0.0052,  ...,  0.0119, -0.0016, -0.0053],
        [ 0.0155, -0.0122, -0.0054,  ...,  0.0111,  0.0095, -0.0145],
        [-0.0034,  0.0085, -0.0106,  ...,  0.0027, -0.0135,  0.0136]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.3.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[117],
            [150],
            [ 43],
            ...,
            [ 88],
            [167],
            [153]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0013, -0.0089, -0.0050,  ...,  0.0029, -0.0047, -0.0070],
        [-0.0003,  0.0061, -0.0072,  ..., -0.0086,  0.0094, -0.0063],
        [-0.0051,  0.0056, -0.0054,  ...,  0.0073, -0.0090,  0.0060],
        ...,
        [-0.0067,  0.0046,  0.0083,  ...,  0.0009,  0.0029,  0.0069],
        [ 0.0072, -0.0081, -0.0037,  ...,  0.0048,  0.0019, -0.0054],
        [-0.0093,  0.0009,  0.0056,  ..., -0.0055, -0.0048, -0.0040]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.3.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 75],
            [158],
            [108],
            ...,
            [ 59],
            [158],
            [152]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0084, -0.0065,  0.0117,  ...,  0.0018, -0.0037,  0.0075],
        [ 0.0006,  0.0101,  0.0150,  ...,  0.0121,  0.0084,  0.0107],
        [-0.0038, -0.0155, -0.0115,  ..., -0.0029,  0.0092, -0.0042],
        ...,
        [ 0.0101,  0.0001,  0.0041,  ...,  0.0007,  0.0047,  0.0128],
        [-0.0044,  0.0072,  0.0023,  ...,  0.0145, -0.0139, -0.0085],
        [-0.0089, -0.0084, -0.0103,  ..., -0.0003,  0.0008,  0.0039]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.3.input_layernorm.weight Parameter containing:
tensor([0.2832, 0.2832, 0.2812,  ..., 0.2793, 0.2891, 0.2910], device='cuda:0')
base_model.model.model.layers.3.post_attention_layernorm.weight Parameter containing:
tensor([0.1748, 0.1748, 0.1699,  ..., 0.1738, 0.1709, 0.1748], device='cuda:0')
base_model.model.model.layers.4.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 41],
            [101],
            [ 85],
            ...,
            [179],
            [158],
            [210]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 1.3199e-03, -1.3794e-02,  1.4038e-02,  ..., -3.0823e-03,
          1.1963e-02, -6.7139e-03],
        [-1.2878e-02, -1.1902e-03, -4.8523e-03,  ...,  1.5381e-02,
          1.1978e-03,  1.1902e-02],
        [ 1.2207e-02,  7.9956e-03, -1.0315e-02,  ..., -5.1498e-04,
         -1.2268e-02,  1.2329e-02],
        ...,
        [ 1.2634e-02,  6.2180e-04, -3.6469e-03,  ...,  5.4016e-03,
         -7.9956e-03, -1.3885e-03],
        [-5.3787e-04,  2.3804e-03,  1.2817e-02,  ...,  7.3242e-04,
         -1.3550e-02, -3.9978e-03],
        [ 8.7891e-03, -1.1108e-02, -9.2983e-05,  ...,  4.9438e-03,
          1.2939e-02,  7.0190e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.4.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 91],
            [ 73],
            [170],
            ...,
            [135],
            [151],
            [182]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-1.1963e-02, -4.9438e-03,  6.8970e-03,  ..., -1.4832e-02,
         -1.0559e-02, -2.6703e-03],
        [ 1.0925e-02, -6.4087e-04, -6.4392e-03,  ..., -1.4343e-02,
         -2.8381e-03, -1.6098e-03],
        [ 3.7842e-03, -4.3030e-03,  5.2490e-03,  ..., -3.7079e-03,
         -9.2773e-03,  9.7656e-04],
        ...,
        [ 2.2430e-03,  2.7008e-03,  7.2937e-03,  ..., -7.5684e-03,
          8.4229e-03,  2.3651e-03],
        [-1.4343e-02, -6.4087e-03,  1.2878e-02,  ...,  5.9366e-05,
          2.6398e-03,  2.0599e-03],
        [ 6.5231e-04,  5.5542e-03,  4.7607e-03,  ..., -8.9111e-03,
          9.2163e-03, -7.3547e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.4.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[118],
            [ 73],
            [148],
            ...,
            [ 83],
            [ 57],
            [107]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0011,  0.0028,  0.0044,  ..., -0.0005,  0.0129,  0.0084],
        [-0.0032,  0.0079, -0.0154,  ..., -0.0010, -0.0053,  0.0056],
        [ 0.0071,  0.0024,  0.0099,  ...,  0.0145, -0.0049, -0.0153],
        ...,
        [ 0.0124,  0.0128,  0.0006,  ..., -0.0040, -0.0135,  0.0140],
        [ 0.0063,  0.0112,  0.0002,  ..., -0.0102,  0.0067, -0.0017],
        [-0.0015, -0.0096, -0.0020,  ..., -0.0087, -0.0027,  0.0123]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.4.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[250],
            [105],
            [ 51],
            ...,
            [182],
            [118],
            [199]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-1.4648e-02, -8.9169e-05, -1.0376e-03,  ...,  7.6904e-03,
          3.1891e-03,  4.0588e-03],
        [ 1.8234e-03,  6.5918e-03,  7.9346e-03,  ..., -3.3951e-04,
         -2.9755e-03,  1.6556e-03],
        [-8.1787e-03, -1.5564e-02,  2.2888e-04,  ..., -7.0190e-03,
         -9.3384e-03, -1.0864e-02],
        ...,
        [ 1.3504e-03, -1.2451e-02,  1.8921e-03,  ...,  7.2021e-03,
          9.8267e-03, -6.3171e-03],
        [ 1.1047e-02, -1.3504e-03, -1.2329e-02,  ..., -7.1716e-03,
         -1.1780e-02, -5.8594e-03],
        [ 1.2512e-02, -1.4709e-02, -1.5198e-02,  ..., -1.5320e-02,
         -6.7444e-03, -1.5320e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.4.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[105],
            [ 89],
            [123],
            ...,
            [ 67],
            [ 69],
            [187]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 1.0010e-02,  1.1047e-02, -9.5825e-03,  ..., -2.6245e-03,
         -7.2937e-03,  5.3711e-03],
        [-6.6833e-03, -1.4404e-02, -7.8735e-03,  ...,  3.6621e-03,
          7.0801e-03,  1.2817e-02],
        [-6.0730e-03, -1.2146e-02,  4.4250e-03,  ..., -5.6458e-04,
         -1.0132e-02,  1.1963e-02],
        ...,
        [-6.7444e-03,  5.2490e-03, -9.0122e-05,  ...,  2.7161e-03,
         -1.9684e-03,  6.1035e-03],
        [-1.2817e-03, -1.1536e-02, -1.4343e-02,  ..., -7.6294e-03,
         -4.7913e-03,  3.0975e-03],
        [-9.9487e-03, -6.7444e-03, -1.9302e-03,  ..., -7.8125e-03,
          1.1368e-03, -5.9814e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.4.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[214],
            [226],
            [228],
            ...,
            [187],
            [230],
            [  2]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0090, -0.0008, -0.0067,  ..., -0.0093, -0.0016,  0.0064],
        [ 0.0068,  0.0063,  0.0021,  ..., -0.0012, -0.0081, -0.0087],
        [-0.0043,  0.0024,  0.0049,  ..., -0.0029,  0.0092, -0.0047],
        ...,
        [-0.0026, -0.0065,  0.0074,  ..., -0.0026,  0.0025,  0.0088],
        [-0.0020,  0.0002, -0.0060,  ..., -0.0013, -0.0034, -0.0071],
        [-0.0019, -0.0093, -0.0071,  ..., -0.0039, -0.0049,  0.0084]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.4.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 69],
            [  5],
            [ 92],
            ...,
            [ 55],
            [183],
            [118]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0046, -0.0051, -0.0073,  ...,  0.0031,  0.0014, -0.0140],
        [-0.0098,  0.0006, -0.0137,  ...,  0.0010,  0.0106,  0.0156],
        [ 0.0154, -0.0084,  0.0013,  ..., -0.0052, -0.0148, -0.0109],
        ...,
        [ 0.0126,  0.0021,  0.0017,  ..., -0.0137, -0.0081,  0.0075],
        [-0.0105, -0.0061, -0.0010,  ...,  0.0039,  0.0133, -0.0113],
        [ 0.0095, -0.0129, -0.0113,  ..., -0.0103,  0.0015,  0.0139]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.4.input_layernorm.weight Parameter containing:
tensor([0.2617, 0.2578, 0.2598,  ..., 0.2559, 0.2637, 0.2715], device='cuda:0')
base_model.model.model.layers.4.post_attention_layernorm.weight Parameter containing:
tensor([0.1885, 0.1865, 0.1816,  ..., 0.1885, 0.1865, 0.1855], device='cuda:0')
base_model.model.model.layers.5.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 85],
            [ 57],
            [165],
            ...,
            [187],
            [195],
            [ 87]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0010,  0.0074,  0.0145,  ..., -0.0115, -0.0017,  0.0150],
        [-0.0055, -0.0107,  0.0067,  ..., -0.0089, -0.0049,  0.0014],
        [-0.0156, -0.0089, -0.0040,  ..., -0.0115, -0.0063, -0.0087],
        ...,
        [-0.0079, -0.0147,  0.0014,  ...,  0.0149, -0.0146, -0.0054],
        [-0.0005,  0.0101,  0.0034,  ..., -0.0128, -0.0031,  0.0018],
        [ 0.0111,  0.0076,  0.0136,  ..., -0.0101, -0.0047, -0.0125]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.5.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 63],
            [156],
            [ 92],
            ...,
            [178],
            [ 85],
            [ 56]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0010, -0.0055, -0.0086,  ..., -0.0143, -0.0143,  0.0010],
        [-0.0064,  0.0006, -0.0083,  ...,  0.0146,  0.0084,  0.0013],
        [ 0.0140, -0.0126, -0.0123,  ...,  0.0044, -0.0049,  0.0156],
        ...,
        [-0.0101,  0.0137, -0.0005,  ..., -0.0001,  0.0027, -0.0117],
        [-0.0135, -0.0123,  0.0001,  ...,  0.0076,  0.0145, -0.0049],
        [-0.0059,  0.0038, -0.0109,  ...,  0.0114,  0.0141, -0.0150]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.5.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[  9],
            [198],
            [ 72],
            ...,
            [215],
            [212],
            [ 81]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0031, -0.0126,  0.0056,  ...,  0.0076, -0.0137, -0.0136],
        [-0.0068, -0.0125,  0.0056,  ..., -0.0138, -0.0051, -0.0135],
        [-0.0055, -0.0062,  0.0032,  ...,  0.0103,  0.0078, -0.0132],
        ...,
        [-0.0153, -0.0156, -0.0085,  ...,  0.0156,  0.0089,  0.0088],
        [ 0.0153,  0.0006, -0.0115,  ..., -0.0076, -0.0134,  0.0011],
        [-0.0090,  0.0101, -0.0155,  ..., -0.0005,  0.0009, -0.0071]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.5.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[ 37],
            [151],
            [100],
            ...,
            [100],
            [105],
            [ 82]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0114,  0.0082,  0.0032,  ..., -0.0155, -0.0075, -0.0025],
        [ 0.0104,  0.0131,  0.0071,  ...,  0.0045,  0.0121,  0.0049],
        [-0.0116, -0.0095,  0.0016,  ..., -0.0045,  0.0004, -0.0083],
        ...,
        [ 0.0005, -0.0061, -0.0138,  ..., -0.0124,  0.0021,  0.0038],
        [-0.0148, -0.0067,  0.0002,  ...,  0.0147, -0.0093, -0.0042],
        [ 0.0126, -0.0115, -0.0036,  ..., -0.0069,  0.0040,  0.0075]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.5.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[119],
            [ 44],
            [ 65],
            ...,
            [ 26],
            [ 86],
            [113]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0109, -0.0082,  0.0046,  ..., -0.0081, -0.0150, -0.0123],
        [-0.0132, -0.0066,  0.0107,  ...,  0.0003,  0.0070, -0.0120],
        [ 0.0026, -0.0060,  0.0060,  ..., -0.0090, -0.0012,  0.0074],
        ...,
        [-0.0058,  0.0125, -0.0032,  ...,  0.0132, -0.0072, -0.0085],
        [-0.0077,  0.0047,  0.0075,  ...,  0.0145, -0.0087, -0.0073],
        [-0.0081,  0.0020, -0.0050,  ..., -0.0153, -0.0129, -0.0096]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.5.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[ 97],
            [106],
            [120],
            ...,
            [ 54],
            [126],
            [148]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-1.2894e-03, -6.5994e-04,  6.4392e-03,  ...,  5.1575e-03,
          1.3123e-03,  6.2866e-03],
        [-9.3994e-03, -3.6621e-03,  6.1646e-03,  ...,  5.3406e-03,
         -7.8201e-04, -5.2490e-03],
        [ 6.7139e-03, -8.8501e-03, -7.2021e-03,  ..., -7.0496e-03,
         -8.2397e-03,  8.2397e-03],
        ...,
        [-6.3477e-03, -3.9368e-03,  5.1575e-03,  ..., -7.9346e-03,
          7.8735e-03,  4.6387e-03],
        [-4.2419e-03, -7.4158e-03, -5.1575e-03,  ..., -5.6763e-03,
          5.2452e-05, -1.2589e-03],
        [-6.1340e-03, -7.1106e-03, -4.4861e-03,  ...,  2.2793e-04,
          5.4016e-03, -7.4768e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.5.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[100],
            [122],
            [170],
            ...,
            [ 33],
            [ 20],
            [ 98]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0043, -0.0071,  0.0088,  ...,  0.0121, -0.0110, -0.0035],
        [-0.0122, -0.0044,  0.0056,  ..., -0.0089,  0.0045,  0.0151],
        [-0.0063,  0.0043, -0.0034,  ...,  0.0134, -0.0120,  0.0129],
        ...,
        [-0.0077, -0.0093,  0.0042,  ...,  0.0025, -0.0085, -0.0126],
        [ 0.0118, -0.0084, -0.0009,  ...,  0.0153,  0.0100,  0.0043],
        [-0.0040,  0.0129, -0.0108,  ..., -0.0050, -0.0018,  0.0033]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.5.input_layernorm.weight Parameter containing:
tensor([0.2637, 0.2637, 0.2598,  ..., 0.2520, 0.2676, 0.2695], device='cuda:0')
base_model.model.model.layers.5.post_attention_layernorm.weight Parameter containing:
tensor([0.2041, 0.1934, 0.1895,  ..., 0.2051, 0.1992, 0.2031], device='cuda:0')
base_model.model.model.layers.6.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[102],
            [142],
            [ 81],
            ...,
            [ 56],
            [121],
            [120]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-3.9368e-03, -9.3994e-03, -1.4893e-02,  ..., -1.0376e-02,
          1.7853e-03, -4.1962e-04],
        [ 1.1963e-02,  1.1414e-02, -1.3062e-02,  ..., -3.2501e-03,
         -9.7752e-05,  3.9673e-03],
        [ 9.0790e-04, -1.0254e-02, -1.3794e-02,  ..., -1.5259e-03,
         -1.4954e-02,  9.4223e-04],
        ...,
        [ 1.3062e-02,  1.0010e-02, -4.0894e-03,  ..., -6.1951e-03,
         -9.8267e-03,  9.8877e-03],
        [ 8.6670e-03, -5.0049e-03,  9.8877e-03,  ...,  1.5564e-02,
         -2.1973e-03,  5.5542e-03],
        [-6.0425e-03,  1.3794e-02, -4.7607e-03,  ..., -7.4463e-03,
          7.5989e-03, -1.4526e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.6.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[165],
            [162],
            [151],
            ...,
            [ 39],
            [ 74],
            [ 35]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0118,  0.0134,  0.0016,  ..., -0.0035, -0.0132, -0.0074],
        [ 0.0133,  0.0035,  0.0073,  ...,  0.0109,  0.0095, -0.0053],
        [-0.0137, -0.0066, -0.0125,  ..., -0.0148,  0.0143,  0.0142],
        ...,
        [ 0.0068, -0.0095, -0.0082,  ...,  0.0037, -0.0101,  0.0004],
        [-0.0098, -0.0145, -0.0146,  ...,  0.0077, -0.0076,  0.0022],
        [ 0.0095,  0.0003,  0.0067,  ..., -0.0112, -0.0068,  0.0006]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.6.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[191],
            [154],
            [191],
            ...,
            [183],
            [242],
            [118]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0123, -0.0046,  0.0004,  ...,  0.0067, -0.0131,  0.0145],
        [-0.0034, -0.0072, -0.0023,  ..., -0.0038,  0.0014,  0.0061],
        [ 0.0052, -0.0117, -0.0125,  ..., -0.0005,  0.0129,  0.0053],
        ...,
        [ 0.0024, -0.0005,  0.0145,  ..., -0.0109, -0.0035, -0.0065],
        [ 0.0123, -0.0140, -0.0143,  ...,  0.0076, -0.0150,  0.0123],
        [ 0.0080,  0.0020,  0.0098,  ..., -0.0106,  0.0150, -0.0027]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.6.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[179],
            [  9],
            [103],
            ...,
            [ 19],
            [195],
            [154]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0005, -0.0054,  0.0102,  ..., -0.0032,  0.0090, -0.0139],
        [ 0.0027,  0.0049,  0.0138,  ..., -0.0090,  0.0154, -0.0018],
        [-0.0051,  0.0089,  0.0145,  ...,  0.0129,  0.0147, -0.0015],
        ...,
        [ 0.0078,  0.0044, -0.0132,  ...,  0.0118, -0.0016,  0.0049],
        [-0.0125, -0.0022,  0.0016,  ...,  0.0089,  0.0075,  0.0011],
        [ 0.0091,  0.0061, -0.0098,  ...,  0.0146,  0.0046,  0.0153]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.6.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[221],
            [ 55],
            [231],
            ...,
            [161],
            [209],
            [123]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 1.5945e-03, -7.9346e-03, -6.8054e-03,  ...,  1.5640e-03,
         -1.3411e-05, -5.9814e-03],
        [-6.8970e-03, -9.0942e-03,  1.1520e-03,  ...,  1.0864e-02,
          8.6670e-03,  1.1597e-02],
        [-1.3611e-02, -4.3945e-03, -9.8267e-03,  ..., -2.3041e-03,
         -1.5015e-02,  7.1411e-03],
        ...,
        [ 4.8828e-03,  9.7046e-03,  5.0068e-06,  ..., -5.2795e-03,
          1.0010e-02,  1.2634e-02],
        [-1.0071e-02,  1.0864e-02, -6.0425e-03,  ...,  2.3193e-03,
         -3.6812e-04, -1.5564e-02],
        [-3.6316e-03,  1.4709e-02,  1.1963e-02,  ...,  4.3297e-04,
         -1.4038e-02,  1.3367e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.6.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[ 92],
            [105],
            [176],
            ...,
            [226],
            [175],
            [ 89]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0039,  0.0032, -0.0082,  ..., -0.0009, -0.0068,  0.0074],
        [ 0.0007, -0.0015, -0.0033,  ..., -0.0028, -0.0070,  0.0012],
        [-0.0031, -0.0044, -0.0062,  ...,  0.0056,  0.0076, -0.0033],
        ...,
        [ 0.0047, -0.0056, -0.0038,  ..., -0.0078,  0.0074,  0.0034],
        [ 0.0062, -0.0060,  0.0012,  ...,  0.0009,  0.0023, -0.0040],
        [ 0.0036,  0.0092,  0.0024,  ...,  0.0089,  0.0049,  0.0093]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.6.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 85],
            [ 73],
            [ 88],
            ...,
            [250],
            [185],
            [ 48]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0123,  0.0114, -0.0015,  ...,  0.0092, -0.0021, -0.0156],
        [-0.0090, -0.0076,  0.0065,  ..., -0.0051,  0.0049,  0.0021],
        [ 0.0107,  0.0023, -0.0081,  ...,  0.0098,  0.0023,  0.0074],
        ...,
        [-0.0045, -0.0017, -0.0092,  ...,  0.0125, -0.0121, -0.0008],
        [ 0.0014, -0.0104, -0.0154,  ...,  0.0114, -0.0067,  0.0085],
        [ 0.0155, -0.0052, -0.0142,  ...,  0.0027, -0.0039,  0.0086]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.6.input_layernorm.weight Parameter containing:
tensor([0.3184, 0.3555, 0.3281,  ..., 0.3164, 0.3359, 0.3203], device='cuda:0')
base_model.model.model.layers.6.post_attention_layernorm.weight Parameter containing:
tensor([0.2168, 0.2061, 0.2041,  ..., 0.2178, 0.2109, 0.2129], device='cuda:0')
base_model.model.model.layers.7.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[168],
            [131],
            [ 33],
            ...,
            [231],
            [158],
            [182]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0051,  0.0082,  0.0146,  ..., -0.0027, -0.0125,  0.0057],
        [-0.0038, -0.0096, -0.0088,  ...,  0.0043, -0.0153, -0.0041],
        [ 0.0020, -0.0014,  0.0109,  ...,  0.0120,  0.0032, -0.0094],
        ...,
        [ 0.0069, -0.0027,  0.0032,  ..., -0.0046,  0.0107, -0.0112],
        [-0.0055, -0.0016, -0.0049,  ..., -0.0008,  0.0081,  0.0144],
        [-0.0121, -0.0010, -0.0099,  ...,  0.0146, -0.0132,  0.0135]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.7.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[101],
            [ 52],
            [102],
            ...,
            [198],
            [184],
            [132]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 8.5449e-03, -1.3123e-02, -8.3160e-04,  ...,  7.0801e-03,
          1.4282e-02,  3.2349e-03],
        [-1.3428e-02,  9.6436e-03, -4.0283e-03,  ...,  9.4604e-03,
          8.0566e-03, -8.4229e-03],
        [ 7.8125e-03, -2.0599e-03, -2.7924e-03,  ..., -5.1575e-03,
         -9.0942e-03, -1.1475e-02],
        ...,
        [-1.0193e-02, -1.0559e-02,  1.5259e-02,  ...,  3.8910e-03,
          9.2773e-03, -6.5613e-03],
        [ 8.4839e-03, -8.9722e-03,  8.0566e-03,  ..., -2.5635e-03,
          2.9449e-03,  8.3447e-05],
        [ 1.2512e-02, -2.6398e-03, -1.4709e-02,  ...,  5.0354e-04,
         -3.4027e-03,  7.3242e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.7.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[ 56],
            [201],
            [ 74],
            ...,
            [108],
            [ 46],
            [171]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0148, -0.0020, -0.0050,  ...,  0.0045,  0.0155, -0.0052],
        [ 0.0080,  0.0031, -0.0090,  ..., -0.0104,  0.0118, -0.0041],
        [-0.0069, -0.0046,  0.0091,  ..., -0.0090, -0.0151,  0.0080],
        ...,
        [ 0.0146, -0.0106,  0.0093,  ...,  0.0084,  0.0073, -0.0104],
        [-0.0022,  0.0154, -0.0139,  ..., -0.0007, -0.0071, -0.0071],
        [ 0.0081, -0.0061,  0.0017,  ..., -0.0147, -0.0046,  0.0017]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.7.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[179],
            [101],
            [133],
            ...,
            [ 21],
            [ 74],
            [ 41]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[ 2.4719e-03, -2.3499e-03,  1.5198e-02,  ...,  1.1902e-02,
         -4.6387e-03,  8.9111e-03],
        [ 1.3245e-02, -9.9487e-03,  4.8828e-03,  ...,  7.5073e-03,
         -1.3428e-02, -8.1787e-03],
        [ 8.6670e-03, -8.2397e-03,  7.9346e-03,  ...,  4.2419e-03,
          9.8877e-03, -3.7842e-03],
        ...,
        [-1.0559e-02,  2.9144e-03, -1.8311e-04,  ..., -8.4229e-03,
         -1.0559e-02,  4.6730e-05],
        [-7.2937e-03, -4.9133e-03, -2.4567e-03,  ..., -5.0354e-03,
          2.9755e-03, -7.7515e-03],
        [-1.0803e-02, -9.5825e-03, -1.1230e-02,  ..., -5.0049e-03,
         -2.4414e-03,  2.9755e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.7.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[151],
            [ 57],
            [178],
            ...,
            [133],
            [ 75],
            [199]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0124,  0.0054,  0.0012,  ..., -0.0028,  0.0045, -0.0145],
        [-0.0064,  0.0080, -0.0096,  ...,  0.0057, -0.0101,  0.0049],
        [-0.0027, -0.0126,  0.0049,  ..., -0.0017,  0.0120,  0.0137],
        ...,
        [-0.0121, -0.0051, -0.0014,  ...,  0.0097, -0.0139,  0.0102],
        [ 0.0043, -0.0125,  0.0112,  ..., -0.0047,  0.0147, -0.0092],
        [ 0.0093,  0.0059,  0.0062,  ..., -0.0039, -0.0133, -0.0004]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.7.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[ 84],
            [124],
            [ 88],
            ...,
            [126],
            [120],
            [ 55]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0040, -0.0092,  0.0012,  ...,  0.0053, -0.0015,  0.0092],
        [ 0.0058,  0.0002, -0.0010,  ...,  0.0019,  0.0053, -0.0081],
        [-0.0075, -0.0067, -0.0061,  ...,  0.0031, -0.0025, -0.0047],
        ...,
        [-0.0043,  0.0016, -0.0087,  ..., -0.0072, -0.0087, -0.0045],
        [-0.0042, -0.0051, -0.0009,  ...,  0.0064, -0.0007, -0.0091],
        [ 0.0044,  0.0018,  0.0042,  ..., -0.0046, -0.0064,  0.0093]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.7.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[215],
            [ 97],
            [ 50],
            ...,
            [250],
            [172],
            [168]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0112, -0.0106, -0.0114,  ..., -0.0010,  0.0020, -0.0082],
        [ 0.0060,  0.0125,  0.0034,  ..., -0.0142,  0.0037,  0.0006],
        [-0.0042, -0.0075,  0.0067,  ..., -0.0080, -0.0015,  0.0055],
        ...,
        [ 0.0072,  0.0018, -0.0066,  ...,  0.0011,  0.0003,  0.0026],
        [ 0.0130, -0.0109, -0.0043,  ...,  0.0101, -0.0080,  0.0098],
        [ 0.0061,  0.0143, -0.0127,  ...,  0.0048, -0.0140, -0.0071]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.7.input_layernorm.weight Parameter containing:
tensor([0.3223, 0.3652, 0.3379,  ..., 0.3242, 0.3574, 0.3301], device='cuda:0')
base_model.model.model.layers.7.post_attention_layernorm.weight Parameter containing:
tensor([0.2314, 0.2158, 0.2178,  ..., 0.2256, 0.2246, 0.2236], device='cuda:0')
base_model.model.model.layers.8.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[148],
            [ 18],
            [169],
            ...,
            [ 53],
            [130],
            [181]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0024, -0.0142, -0.0028,  ..., -0.0048,  0.0026, -0.0063],
        [-0.0015,  0.0090, -0.0022,  ...,  0.0012, -0.0104,  0.0052],
        [-0.0065, -0.0014, -0.0087,  ..., -0.0149,  0.0039,  0.0092],
        ...,
        [-0.0031,  0.0095,  0.0024,  ...,  0.0058, -0.0048, -0.0060],
        [ 0.0022,  0.0007, -0.0066,  ..., -0.0125, -0.0067,  0.0126],
        [ 0.0006,  0.0051,  0.0038,  ...,  0.0015,  0.0101,  0.0004]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.8.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 89],
            [ 93],
            [198],
            ...,
            [ 36],
            [ 26],
            [149]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0149,  0.0030,  0.0136,  ...,  0.0133, -0.0086, -0.0022],
        [-0.0078, -0.0085, -0.0001,  ..., -0.0156,  0.0089,  0.0006],
        [-0.0108, -0.0101,  0.0129,  ..., -0.0047, -0.0070,  0.0010],
        ...,
        [-0.0116, -0.0074, -0.0129,  ...,  0.0062,  0.0047,  0.0137],
        [-0.0103, -0.0151, -0.0050,  ..., -0.0003,  0.0044, -0.0017],
        [ 0.0041,  0.0128,  0.0067,  ...,  0.0075, -0.0088,  0.0153]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.8.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[ 19],
            [133],
            [114],
            ...,
            [114],
            [ 38],
            [148]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0058,  0.0102, -0.0061,  ...,  0.0014, -0.0131, -0.0060],
        [-0.0086, -0.0064,  0.0070,  ..., -0.0080,  0.0098, -0.0096],
        [-0.0148, -0.0152, -0.0014,  ..., -0.0100,  0.0035,  0.0013],
        ...,
        [ 0.0021,  0.0032, -0.0037,  ..., -0.0080, -0.0052,  0.0002],
        [ 0.0066, -0.0137,  0.0134,  ...,  0.0103, -0.0109, -0.0014],
        [ 0.0013, -0.0072, -0.0114,  ...,  0.0083, -0.0134, -0.0071]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.8.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[204],
            [ 45],
            [178],
            ...,
            [120],
            [127],
            [ 54]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0065, -0.0071, -0.0114,  ...,  0.0156,  0.0105, -0.0005],
        [ 0.0012, -0.0047,  0.0128,  ...,  0.0030, -0.0140,  0.0105],
        [ 0.0125,  0.0145,  0.0099,  ...,  0.0054, -0.0154,  0.0087],
        ...,
        [ 0.0096, -0.0067, -0.0119,  ...,  0.0081,  0.0011, -0.0015],
        [ 0.0123, -0.0072,  0.0095,  ..., -0.0115,  0.0093,  0.0056],
        [-0.0153, -0.0072, -0.0109,  ...,  0.0061,  0.0139, -0.0024]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.8.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[152],
            [ 44],
            [153],
            ...,
            [ 52],
            [ 83],
            [183]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0137, -0.0111,  0.0054,  ...,  0.0102,  0.0093,  0.0156],
        [ 0.0095,  0.0005, -0.0117,  ..., -0.0036, -0.0065,  0.0057],
        [ 0.0106,  0.0132, -0.0053,  ...,  0.0016,  0.0151,  0.0156],
        ...,
        [ 0.0003, -0.0069,  0.0145,  ...,  0.0005, -0.0052, -0.0014],
        [-0.0122,  0.0082,  0.0068,  ..., -0.0049, -0.0112, -0.0042],
        [ 0.0016,  0.0078, -0.0050,  ..., -0.0128,  0.0069,  0.0140]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.8.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[ 86],
            [183],
            [168],
            ...,
            [148],
            [167],
            [135]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0061,  0.0027, -0.0089,  ..., -0.0029, -0.0052, -0.0074],
        [ 0.0007, -0.0054, -0.0069,  ..., -0.0079,  0.0033, -0.0059],
        [ 0.0016,  0.0095, -0.0024,  ..., -0.0065, -0.0021, -0.0077],
        ...,
        [-0.0065, -0.0046, -0.0057,  ..., -0.0074,  0.0031,  0.0067],
        [ 0.0095, -0.0069,  0.0060,  ...,  0.0006, -0.0060, -0.0013],
        [-0.0006, -0.0062,  0.0069,  ..., -0.0074,  0.0004,  0.0056]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.8.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 91],
            [ 70],
            [237],
            ...,
            [ 53],
            [148],
            [120]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0041,  0.0009,  0.0024,  ..., -0.0156,  0.0008, -0.0034],
        [-0.0092,  0.0120,  0.0139,  ..., -0.0021, -0.0141,  0.0079],
        [-0.0133,  0.0136,  0.0060,  ...,  0.0044, -0.0134,  0.0015],
        ...,
        [ 0.0153,  0.0002, -0.0106,  ..., -0.0039, -0.0118, -0.0153],
        [-0.0082,  0.0034, -0.0035,  ...,  0.0060, -0.0066,  0.0044],
        [ 0.0152, -0.0143, -0.0052,  ..., -0.0083, -0.0044,  0.0043]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.8.input_layernorm.weight Parameter containing:
tensor([0.3320, 0.3457, 0.3301,  ..., 0.3203, 0.3438, 0.3223], device='cuda:0')
base_model.model.model.layers.8.post_attention_layernorm.weight Parameter containing:
tensor([0.2383, 0.2236, 0.2178,  ..., 0.2363, 0.2285, 0.2256], device='cuda:0')
base_model.model.model.layers.9.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 56],
            [186],
            [178],
            ...,
            [ 17],
            [146],
            [214]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0016,  0.0117,  0.0074,  ..., -0.0153,  0.0148,  0.0021],
        [ 0.0019,  0.0024,  0.0009,  ...,  0.0093,  0.0140,  0.0015],
        [-0.0008, -0.0142,  0.0123,  ..., -0.0096,  0.0115,  0.0114],
        ...,
        [-0.0144,  0.0122,  0.0104,  ..., -0.0093,  0.0002, -0.0106],
        [ 0.0063, -0.0117,  0.0075,  ...,  0.0141, -0.0019,  0.0051],
        [-0.0099, -0.0148,  0.0125,  ..., -0.0027,  0.0075,  0.0026]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.9.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 10],
            [ 58],
            [ 98],
            ...,
            [ 58],
            [ 41],
            [180]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-1.0803e-02,  1.2939e-02, -4.3335e-03,  ...,  8.7280e-03,
         -1.5381e-02,  1.4038e-03],
        [ 1.2512e-02,  4.5776e-03, -8.3618e-03,  ..., -2.7161e-03,
         -5.1270e-03,  8.3008e-03],
        [ 4.1504e-03, -1.0864e-02, -5.0964e-03,  ..., -3.5248e-03,
          1.2817e-02, -1.2451e-02],
        ...,
        [ 8.8692e-05,  1.2024e-02, -8.5449e-04,  ...,  2.6398e-03,
         -4.1962e-04,  3.0212e-03],
        [ 6.9275e-03,  4.1199e-03,  2.1210e-03,  ..., -1.5015e-02,
          5.6152e-03, -5.6763e-03],
        [ 1.4893e-02, -8.3008e-03, -1.2684e-04,  ..., -3.1281e-03,
         -2.2888e-03,  1.3306e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.9.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[106],
            [120],
            [ 53],
            ...,
            [168],
            [131],
            [ 86]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0065, -0.0146,  0.0126,  ..., -0.0034, -0.0022, -0.0009],
        [ 0.0025, -0.0127, -0.0093,  ...,  0.0107, -0.0029, -0.0008],
        [-0.0075,  0.0156, -0.0004,  ...,  0.0076, -0.0042,  0.0123],
        ...,
        [ 0.0001, -0.0008, -0.0012,  ..., -0.0105, -0.0008, -0.0024],
        [-0.0049,  0.0135, -0.0109,  ...,  0.0153, -0.0042,  0.0151],
        [-0.0024,  0.0107, -0.0137,  ...,  0.0013,  0.0030, -0.0080]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.9.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[170],
            [145],
            [173],
            ...,
            [215],
            [229],
            [170]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0040,  0.0068, -0.0112,  ..., -0.0001, -0.0060,  0.0074],
        [-0.0152, -0.0109,  0.0072,  ...,  0.0111,  0.0088,  0.0089],
        [ 0.0039, -0.0019, -0.0116,  ...,  0.0071, -0.0128,  0.0156],
        ...,
        [ 0.0013,  0.0112, -0.0052,  ..., -0.0091, -0.0151,  0.0096],
        [-0.0115, -0.0132,  0.0123,  ..., -0.0145,  0.0116, -0.0092],
        [-0.0147, -0.0119,  0.0084,  ..., -0.0021,  0.0051, -0.0128]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.9.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[ 93],
            [ 89],
            [245],
            ...,
            [ 85],
            [ 58],
            [ 73]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 2.6093e-03,  1.5625e-02, -6.0120e-03,  ...,  5.8899e-03,
         -5.6763e-03,  1.4526e-02],
        [-8.6670e-03,  6.0730e-03,  2.8534e-03,  ...,  7.1411e-03,
          7.7515e-03,  1.0925e-02],
        [-7.9956e-03, -1.7929e-03,  2.0447e-03,  ..., -9.1553e-04,
         -1.0010e-02,  4.7922e-05],
        ...,
        [-8.5449e-03, -1.1902e-02,  1.1108e-02,  ...,  1.0803e-02,
          6.9046e-04,  1.2451e-02],
        [ 1.0925e-02,  1.5381e-02,  9.6893e-04,  ..., -1.3977e-02,
          3.7231e-03, -9.2163e-03],
        [ 3.2196e-03,  8.9111e-03,  6.1989e-05,  ...,  3.2654e-03,
         -1.3428e-02, -4.7302e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.9.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[109],
            [  4],
            [231],
            ...,
            [166],
            [174],
            [187]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-7.1106e-03,  4.6921e-04,  3.5400e-03,  ...,  8.4305e-04,
         -5.0964e-03, -7.8964e-04],
        [-9.3384e-03, -8.1177e-03, -5.7678e-03,  ..., -4.7302e-03,
          6.7139e-03, -7.3547e-03],
        [ 4.4250e-03,  4.6387e-03, -2.7924e-03,  ...,  9.0332e-03,
          7.4463e-03, -7.9956e-03],
        ...,
        [-4.7913e-03,  6.6833e-03, -3.5553e-03,  ...,  4.6158e-04,
          6.6528e-03, -2.3193e-03],
        [ 6.1951e-03,  9.0332e-03,  3.6774e-03,  ..., -9.2773e-03,
         -4.6158e-04,  1.9684e-03],
        [ 1.2054e-03, -9.2773e-03,  7.6904e-03,  ...,  4.1008e-05,
          6.3477e-03, -8.9722e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.9.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 90],
            [168],
            [117],
            ...,
            [ 67],
            [179],
            [231]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0017, -0.0063, -0.0026,  ...,  0.0123, -0.0102,  0.0037],
        [-0.0134,  0.0021, -0.0137,  ..., -0.0027,  0.0006, -0.0028],
        [-0.0033, -0.0009, -0.0038,  ...,  0.0100, -0.0079,  0.0076],
        ...,
        [ 0.0092,  0.0145, -0.0072,  ..., -0.0084, -0.0073,  0.0089],
        [ 0.0066, -0.0156,  0.0042,  ..., -0.0073,  0.0053, -0.0042],
        [-0.0068,  0.0023,  0.0044,  ...,  0.0041,  0.0100, -0.0028]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.9.input_layernorm.weight Parameter containing:
tensor([0.3516, 0.3594, 0.3223,  ..., 0.3496, 0.3457, 0.3398], device='cuda:0')
base_model.model.model.layers.9.post_attention_layernorm.weight Parameter containing:
tensor([0.2422, 0.2305, 0.2217,  ..., 0.2373, 0.2363, 0.2324], device='cuda:0')
base_model.model.model.layers.10.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[105],
            [102],
            [233],
            ...,
            [197],
            [166],
            [ 52]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0117, -0.0035, -0.0126,  ..., -0.0093, -0.0153,  0.0126],
        [-0.0112,  0.0029,  0.0111,  ...,  0.0062,  0.0027,  0.0077],
        [-0.0141,  0.0008,  0.0104,  ..., -0.0104,  0.0142,  0.0011],
        ...,
        [-0.0135, -0.0030,  0.0148,  ..., -0.0008,  0.0065, -0.0019],
        [ 0.0110,  0.0059,  0.0034,  ...,  0.0142, -0.0065, -0.0120],
        [-0.0052,  0.0109, -0.0053,  ...,  0.0080,  0.0148, -0.0072]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.10.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 11],
            [ 90],
            [106],
            ...,
            [ 20],
            [ 42],
            [229]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 9.6893e-04, -1.0132e-02, -9.7046e-03,  ..., -1.7242e-03,
          7.8735e-03, -1.1047e-02],
        [-1.2878e-02, -8.3923e-05, -4.4861e-03,  ..., -1.2573e-02,
         -7.9956e-03, -5.7220e-04],
        [ 9.7656e-03, -6.4392e-03,  1.1719e-02,  ...,  1.1475e-02,
          7.2937e-03,  1.8845e-03],
        ...,
        [-3.2196e-03, -4.9744e-03, -6.3782e-03,  ...,  1.4954e-02,
         -1.0620e-02, -9.6436e-03],
        [ 2.2125e-03,  1.2573e-02,  1.2024e-02,  ..., -4.1504e-03,
          1.5015e-02,  8.6060e-03],
        [-1.5015e-02, -1.3367e-02, -9.5825e-03,  ..., -7.7820e-03,
         -3.2806e-03,  3.3569e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.10.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[ 24],
            [107],
            [216],
            ...,
            [ 57],
            [198],
            [ 78]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0109,  0.0056, -0.0090,  ..., -0.0137, -0.0006, -0.0127],
        [-0.0075, -0.0050,  0.0024,  ..., -0.0073,  0.0080, -0.0128],
        [ 0.0132, -0.0104,  0.0048,  ...,  0.0131,  0.0040,  0.0084],
        ...,
        [ 0.0009, -0.0102, -0.0025,  ..., -0.0104,  0.0076,  0.0075],
        [ 0.0041,  0.0053,  0.0112,  ..., -0.0136, -0.0059,  0.0005],
        [-0.0036, -0.0141,  0.0071,  ..., -0.0139, -0.0004, -0.0150]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.10.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[103],
            [ 30],
            [ 72],
            ...,
            [176],
            [154],
            [120]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0129, -0.0060,  0.0082,  ..., -0.0134, -0.0099, -0.0104],
        [-0.0121, -0.0141, -0.0146,  ...,  0.0102, -0.0109,  0.0107],
        [-0.0054,  0.0096, -0.0015,  ...,  0.0070, -0.0070,  0.0139],
        ...,
        [ 0.0102,  0.0140,  0.0026,  ..., -0.0154, -0.0118, -0.0126],
        [-0.0016, -0.0125, -0.0100,  ...,  0.0116,  0.0087, -0.0117],
        [-0.0101, -0.0092,  0.0148,  ...,  0.0031, -0.0132,  0.0014]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.10.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[ 16],
            [ 41],
            [ 93],
            ...,
            [147],
            [181],
            [132]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0123,  0.0007, -0.0071,  ...,  0.0049,  0.0012, -0.0156],
        [ 0.0031,  0.0005,  0.0060,  ...,  0.0013,  0.0082, -0.0028],
        [ 0.0156, -0.0013,  0.0144,  ..., -0.0058,  0.0075, -0.0074],
        ...,
        [ 0.0131,  0.0030,  0.0051,  ...,  0.0132,  0.0014,  0.0053],
        [-0.0049,  0.0030,  0.0075,  ...,  0.0077,  0.0083, -0.0070],
        [-0.0108, -0.0004, -0.0023,  ..., -0.0145,  0.0139,  0.0043]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.10.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[ 82],
            [109],
            [150],
            ...,
            [ 59],
            [180],
            [ 73]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0079,  0.0075,  0.0043,  ..., -0.0045,  0.0083, -0.0047],
        [ 0.0042,  0.0090, -0.0093,  ...,  0.0014, -0.0064, -0.0020],
        [-0.0073, -0.0052,  0.0032,  ..., -0.0025,  0.0076, -0.0066],
        ...,
        [-0.0089,  0.0090, -0.0066,  ...,  0.0092,  0.0026, -0.0087],
        [-0.0010,  0.0042, -0.0082,  ...,  0.0062, -0.0043, -0.0005],
        [ 0.0040, -0.0065,  0.0067,  ..., -0.0078,  0.0085, -0.0061]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.10.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 22],
            [ 21],
            [ 87],
            ...,
            [  4],
            [215],
            [155]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0036,  0.0112,  0.0114,  ..., -0.0050, -0.0079, -0.0030],
        [-0.0156,  0.0090, -0.0133,  ..., -0.0137,  0.0004, -0.0146],
        [-0.0117, -0.0062,  0.0049,  ..., -0.0003, -0.0064, -0.0139],
        ...,
        [-0.0117,  0.0026, -0.0034,  ..., -0.0079,  0.0033, -0.0021],
        [ 0.0077,  0.0104,  0.0118,  ...,  0.0045,  0.0140,  0.0036],
        [ 0.0138, -0.0094,  0.0107,  ...,  0.0025, -0.0022, -0.0057]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.10.input_layernorm.weight Parameter containing:
tensor([0.3633, 0.3594, 0.3203,  ..., 0.3398, 0.3477, 0.3359], device='cuda:0')
base_model.model.model.layers.10.post_attention_layernorm.weight Parameter containing:
tensor([0.2461, 0.2324, 0.2236,  ..., 0.2402, 0.2373, 0.2373], device='cuda:0')
base_model.model.model.layers.11.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[205],
            [ 82],
            [ 34],
            ...,
            [156],
            [244],
            [222]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0127,  0.0061,  0.0145,  ...,  0.0089,  0.0014, -0.0140],
        [-0.0023,  0.0076, -0.0027,  ...,  0.0121, -0.0014, -0.0004],
        [ 0.0156,  0.0047,  0.0001,  ..., -0.0083, -0.0106,  0.0110],
        ...,
        [ 0.0109,  0.0124,  0.0059,  ..., -0.0033,  0.0064, -0.0036],
        [-0.0028,  0.0139,  0.0003,  ...,  0.0113, -0.0085, -0.0091],
        [-0.0121, -0.0009, -0.0066,  ...,  0.0136, -0.0047, -0.0040]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.11.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[140],
            [117],
            [139],
            ...,
            [187],
            [150],
            [251]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0085,  0.0120, -0.0127,  ...,  0.0020, -0.0035,  0.0025],
        [-0.0128,  0.0003,  0.0052,  ...,  0.0041,  0.0155, -0.0137],
        [-0.0068,  0.0153, -0.0110,  ...,  0.0044, -0.0027, -0.0111],
        ...,
        [ 0.0010,  0.0096,  0.0049,  ..., -0.0102, -0.0144, -0.0087],
        [ 0.0063,  0.0016, -0.0138,  ...,  0.0107,  0.0029, -0.0140],
        [ 0.0011,  0.0006,  0.0097,  ..., -0.0096, -0.0061,  0.0002]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.11.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[136],
            [ 84],
            [149],
            ...,
            [103],
            [128],
            [100]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0039, -0.0056, -0.0110,  ...,  0.0016, -0.0141, -0.0143],
        [-0.0093, -0.0048,  0.0045,  ...,  0.0027,  0.0104,  0.0029],
        [-0.0074, -0.0029, -0.0131,  ..., -0.0123,  0.0032,  0.0079],
        ...,
        [ 0.0125, -0.0007,  0.0036,  ...,  0.0104,  0.0042,  0.0079],
        [ 0.0038, -0.0090,  0.0051,  ...,  0.0076, -0.0130, -0.0063],
        [-0.0004, -0.0008,  0.0046,  ...,  0.0049, -0.0068,  0.0092]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.11.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[ 23],
            [160],
            [  1],
            ...,
            [193],
            [ 85],
            [116]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0002, -0.0091, -0.0123,  ...,  0.0076,  0.0067,  0.0033],
        [-0.0097, -0.0150, -0.0081,  ..., -0.0047, -0.0110,  0.0079],
        [-0.0075, -0.0041, -0.0048,  ..., -0.0048,  0.0070, -0.0066],
        ...,
        [ 0.0081,  0.0075, -0.0025,  ..., -0.0062, -0.0074,  0.0006],
        [ 0.0045,  0.0089, -0.0118,  ..., -0.0008, -0.0139, -0.0007],
        [ 0.0020, -0.0043, -0.0062,  ..., -0.0115,  0.0002, -0.0071]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.11.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[  5],
            [119],
            [169],
            ...,
            [ 88],
            [141],
            [100]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-9.9487e-03, -1.1475e-02, -1.3550e-02,  ..., -5.6152e-03,
          8.6060e-03, -1.4832e-02],
        [ 6.1417e-04,  1.2390e-02,  1.4282e-02,  ..., -1.0193e-02,
         -1.4465e-02, -1.2756e-02],
        [-5.7068e-03, -7.1049e-05,  5.9128e-04,  ..., -4.5204e-04,
          9.9487e-03,  1.1978e-03],
        ...,
        [ 4.4556e-03, -1.4343e-02,  8.6060e-03,  ..., -2.0752e-03,
         -1.0498e-02, -1.4343e-02],
        [ 5.3711e-03,  5.4321e-03, -7.0496e-03,  ...,  3.4180e-03,
         -8.8501e-03, -1.0803e-02],
        [ 1.0910e-03, -1.1841e-02, -2.7161e-03,  ...,  1.4771e-02,
          1.3580e-03,  6.2866e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.11.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[ 63],
            [183],
            [166],
            ...,
            [198],
            [  8],
            [ 79]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0007,  0.0049,  0.0019,  ...,  0.0035,  0.0027,  0.0058],
        [-0.0089,  0.0053, -0.0062,  ...,  0.0063,  0.0041,  0.0009],
        [-0.0027,  0.0056,  0.0072,  ...,  0.0056, -0.0079, -0.0092],
        ...,
        [-0.0042,  0.0005,  0.0052,  ...,  0.0087,  0.0060,  0.0067],
        [-0.0009, -0.0092,  0.0072,  ..., -0.0068,  0.0067,  0.0042],
        [-0.0030, -0.0037, -0.0093,  ...,  0.0017, -0.0061,  0.0032]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.11.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 87],
            [ 45],
            [ 57],
            ...,
            [187],
            [ 68],
            [157]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 6.8359e-03,  1.3123e-02, -2.4261e-03,  ..., -1.3550e-02,
         -7.1106e-03,  6.6528e-03],
        [ 1.4954e-02,  1.1475e-02, -9.3994e-03,  ...,  1.4648e-02,
          7.7820e-03, -1.3489e-02],
        [-8.3618e-03, -1.3184e-02, -7.7057e-04,  ...,  9.3384e-03,
          1.1475e-02, -7.3853e-03],
        ...,
        [-8.1787e-03,  1.3306e-02,  1.4893e-02,  ...,  2.4872e-03,
         -1.1597e-02, -9.1553e-03],
        [-4.1809e-03, -4.4556e-03,  3.4332e-04,  ...,  2.2736e-03,
          2.8610e-04, -1.3794e-02],
        [-1.3000e-02,  7.7209e-03,  8.4400e-05,  ..., -1.1536e-02,
         -1.4221e-02,  1.2939e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.11.input_layernorm.weight Parameter containing:
tensor([0.3945, 0.3926, 0.3594,  ..., 0.3828, 0.3770, 0.3672], device='cuda:0')
base_model.model.model.layers.11.post_attention_layernorm.weight Parameter containing:
tensor([0.2539, 0.2363, 0.2334,  ..., 0.2500, 0.2490, 0.2451], device='cuda:0')
base_model.model.model.layers.12.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 81],
            [185],
            [211],
            ...,
            [165],
            [116],
            [188]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 1.4465e-02,  3.4180e-03,  1.0147e-03,  ...,  1.2817e-02,
          1.7700e-03,  3.1586e-03],
        [-1.5335e-03, -6.2866e-03, -2.1057e-03,  ...,  6.0730e-03,
         -1.4221e-02, -2.9907e-03],
        [ 1.4465e-02,  5.9605e-06, -1.1841e-02,  ..., -9.5825e-03,
          5.7983e-03,  6.7902e-04],
        ...,
        [-3.4180e-03,  2.1057e-03, -3.3569e-03,  ..., -7.3242e-03,
          9.5215e-03,  5.0964e-03],
        [-1.2573e-02, -9.9487e-03,  1.3550e-02,  ..., -9.8267e-03,
          1.1475e-02, -2.6245e-03],
        [-8.4686e-04, -1.2817e-03, -9.3994e-03,  ...,  1.2085e-02,
          1.5625e-02,  6.4392e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.12.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[181],
            [103],
            [ 64],
            ...,
            [169],
            [105],
            [ 35]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0117,  0.0083,  0.0128,  ...,  0.0036,  0.0056, -0.0022],
        [-0.0002, -0.0066,  0.0085,  ...,  0.0120,  0.0026, -0.0011],
        [-0.0027, -0.0007, -0.0036,  ...,  0.0146,  0.0135,  0.0058],
        ...,
        [-0.0135,  0.0141,  0.0083,  ...,  0.0025,  0.0077, -0.0009],
        [-0.0014,  0.0113, -0.0066,  ...,  0.0039, -0.0057, -0.0132],
        [ 0.0063,  0.0150, -0.0022,  ..., -0.0031,  0.0094,  0.0077]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.12.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[153],
            [137],
            [101],
            ...,
            [202],
            [ 77],
            [148]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0156, -0.0139,  0.0004,  ..., -0.0068,  0.0068, -0.0065],
        [-0.0098, -0.0106, -0.0023,  ...,  0.0146,  0.0107, -0.0002],
        [ 0.0128, -0.0086,  0.0044,  ..., -0.0022, -0.0156,  0.0041],
        ...,
        [ 0.0048,  0.0023, -0.0120,  ...,  0.0027, -0.0131,  0.0110],
        [ 0.0012, -0.0056,  0.0126,  ...,  0.0064, -0.0073,  0.0038],
        [ 0.0010,  0.0134,  0.0087,  ...,  0.0013, -0.0095, -0.0004]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.12.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[233],
            [116],
            [218],
            ...,
            [197],
            [222],
            [ 50]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-1.2207e-02, -1.0376e-02, -1.3184e-02,  ..., -2.7008e-03,
          1.4160e-02,  8.7891e-03],
        [-3.3569e-03, -1.0010e-02, -1.3245e-02,  ..., -1.1169e-02,
         -2.4109e-03,  4.6253e-05],
        [-5.4626e-03,  9.1553e-03, -1.3245e-02,  ..., -4.1809e-03,
          1.3855e-02,  1.8616e-03],
        ...,
        [ 2.1820e-03,  1.5259e-02, -1.0315e-02,  ...,  9.7656e-03,
          1.1719e-02, -8.3618e-03],
        [-3.1853e-04, -1.7471e-03, -1.3489e-02,  ...,  9.7046e-03,
         -2.9297e-03,  7.3853e-03],
        [-1.3550e-02,  8.9722e-03,  1.5198e-02,  ..., -1.1780e-02,
         -1.5381e-02,  3.6163e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.12.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[147],
            [147],
            [179],
            ...,
            [100],
            [198],
            [ 83]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0018, -0.0088, -0.0072,  ...,  0.0091, -0.0103, -0.0031],
        [-0.0146, -0.0142,  0.0125,  ..., -0.0138,  0.0011,  0.0022],
        [ 0.0124, -0.0155,  0.0038,  ...,  0.0115, -0.0109, -0.0048],
        ...,
        [-0.0106,  0.0107, -0.0017,  ...,  0.0030,  0.0098, -0.0121],
        [-0.0050,  0.0124, -0.0117,  ...,  0.0154,  0.0065,  0.0116],
        [-0.0106, -0.0134,  0.0081,  ..., -0.0115, -0.0012,  0.0039]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.12.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[133],
            [ 26],
            [139],
            ...,
            [218],
            [ 64],
            [198]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 9.7752e-05,  3.7842e-03,  6.7139e-04,  ..., -2.1515e-03,
          4.0588e-03,  9.4604e-03],
        [ 3.0518e-03, -5.5237e-03,  3.9368e-03,  ..., -1.4114e-03,
          7.0496e-03, -5.9509e-03],
        [-4.4556e-03, -3.6011e-03,  7.7209e-03,  ...,  9.3994e-03,
         -5.7373e-03,  3.3569e-03],
        ...,
        [-9.3384e-03, -3.4637e-03,  9.0790e-04,  ..., -4.4250e-03,
         -5.7678e-03,  3.6049e-04],
        [ 4.0817e-04, -4.3640e-03,  5.8899e-03,  ..., -2.9144e-03,
         -1.8787e-04, -5.9814e-03],
        [-2.3499e-03, -3.8757e-03,  3.9673e-03,  ..., -8.7891e-03,
          4.4861e-03, -7.4158e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.12.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[139],
            [138],
            [137],
            ...,
            [ 91],
            [ 94],
            [ 97]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-1.1292e-02, -6.1646e-03,  9.7752e-05,  ...,  7.5684e-03,
         -1.7929e-03,  1.2939e-02],
        [-9.0332e-03, -1.0490e-04,  1.2512e-02,  ..., -1.3000e-02,
          1.4282e-02,  1.3184e-02],
        [-7.2327e-03, -1.0223e-03,  1.3000e-02,  ..., -1.0376e-02,
         -1.4343e-02,  1.4587e-02],
        ...,
        [-1.0376e-02,  4.4250e-03, -1.5564e-02,  ..., -1.3657e-03,
         -1.1683e-04, -3.7689e-03],
        [-8.1787e-03,  3.9673e-03,  6.5918e-03,  ..., -1.5076e-02,
          8.4229e-03,  9.2163e-03],
        [ 2.0142e-03, -4.3945e-03, -5.0659e-03,  ..., -2.4414e-03,
         -5.0049e-03,  1.2939e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.12.input_layernorm.weight Parameter containing:
tensor([0.4023, 0.3984, 0.3652,  ..., 0.3789, 0.3828, 0.3848], device='cuda:0')
base_model.model.model.layers.12.post_attention_layernorm.weight Parameter containing:
tensor([0.2578, 0.2441, 0.2373,  ..., 0.2559, 0.2539, 0.2539], device='cuda:0')
base_model.model.model.layers.13.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[101],
            [ 79],
            [176],
            ...,
            [166],
            [133],
            [182]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0154,  0.0123,  0.0005,  ..., -0.0075,  0.0017, -0.0120],
        [-0.0019, -0.0108,  0.0153,  ..., -0.0146,  0.0045,  0.0121],
        [-0.0069, -0.0135, -0.0131,  ...,  0.0114, -0.0047,  0.0138],
        ...,
        [-0.0069, -0.0015,  0.0151,  ...,  0.0140,  0.0072,  0.0056],
        [ 0.0035,  0.0016,  0.0005,  ...,  0.0132, -0.0150, -0.0061],
        [-0.0077, -0.0139,  0.0100,  ...,  0.0121, -0.0126,  0.0148]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.13.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[185],
            [107],
            [ 90],
            ...,
            [136],
            [ 84],
            [ 88]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0052, -0.0072, -0.0098,  ..., -0.0003, -0.0143, -0.0140],
        [-0.0118,  0.0018,  0.0136,  ...,  0.0037,  0.0117,  0.0082],
        [ 0.0112, -0.0048,  0.0090,  ..., -0.0123,  0.0057, -0.0085],
        ...,
        [-0.0152,  0.0013,  0.0087,  ..., -0.0003,  0.0032,  0.0109],
        [ 0.0054, -0.0103,  0.0039,  ...,  0.0106,  0.0134,  0.0073],
        [ 0.0042,  0.0134,  0.0056,  ..., -0.0109, -0.0075,  0.0017]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.13.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[200],
            [ 61],
            [ 86],
            ...,
            [ 23],
            [ 75],
            [126]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0156, -0.0005,  0.0072,  ..., -0.0003, -0.0006, -0.0145],
        [ 0.0109, -0.0137,  0.0007,  ...,  0.0102, -0.0008, -0.0032],
        [ 0.0061, -0.0107,  0.0096,  ..., -0.0121,  0.0031, -0.0085],
        ...,
        [-0.0035, -0.0056, -0.0151,  ..., -0.0078,  0.0077,  0.0070],
        [ 0.0078,  0.0123, -0.0044,  ...,  0.0050,  0.0015, -0.0125],
        [ 0.0050,  0.0119, -0.0023,  ..., -0.0143,  0.0032, -0.0107]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.13.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[103],
            [ 83],
            [ 70],
            ...,
            [ 73],
            [108],
            [ 65]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0078, -0.0104, -0.0052,  ..., -0.0029, -0.0088,  0.0146],
        [ 0.0145,  0.0040,  0.0126,  ..., -0.0041, -0.0017, -0.0015],
        [-0.0046, -0.0133,  0.0128,  ..., -0.0124, -0.0096,  0.0153],
        ...,
        [ 0.0134,  0.0118,  0.0079,  ...,  0.0011,  0.0106, -0.0154],
        [ 0.0069,  0.0027,  0.0012,  ...,  0.0041,  0.0052,  0.0088],
        [ 0.0013, -0.0035,  0.0081,  ...,  0.0145, -0.0126,  0.0098]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.13.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[218],
            [ 84],
            [ 57],
            ...,
            [182],
            [ 22],
            [222]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0103,  0.0020, -0.0036,  ..., -0.0108,  0.0120,  0.0074],
        [-0.0072,  0.0144, -0.0073,  ..., -0.0046, -0.0120, -0.0074],
        [ 0.0122,  0.0087, -0.0116,  ..., -0.0063,  0.0074,  0.0129],
        ...,
        [ 0.0143,  0.0006,  0.0082,  ..., -0.0049,  0.0143, -0.0094],
        [-0.0071,  0.0136,  0.0004,  ..., -0.0022, -0.0071, -0.0053],
        [ 0.0060, -0.0085,  0.0052,  ...,  0.0036, -0.0048, -0.0106]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.13.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[ 86],
            [104],
            [158],
            ...,
            [ 85],
            [136],
            [142]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0070, -0.0074, -0.0076,  ...,  0.0045, -0.0077, -0.0037],
        [ 0.0052, -0.0008,  0.0054,  ..., -0.0054, -0.0080, -0.0093],
        [ 0.0051, -0.0048,  0.0084,  ...,  0.0073, -0.0023, -0.0042],
        ...,
        [-0.0053, -0.0090, -0.0059,  ...,  0.0062, -0.0039,  0.0060],
        [ 0.0074, -0.0076,  0.0014,  ..., -0.0027, -0.0070, -0.0090],
        [-0.0007, -0.0093, -0.0039,  ...,  0.0087, -0.0067, -0.0087]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.13.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 28],
            [ 85],
            [ 39],
            ...,
            [  3],
            [115],
            [124]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0053, -0.0076,  0.0042,  ..., -0.0074,  0.0124,  0.0033],
        [ 0.0048, -0.0033,  0.0108,  ..., -0.0087, -0.0143,  0.0124],
        [-0.0024, -0.0020,  0.0046,  ..., -0.0107,  0.0142,  0.0120],
        ...,
        [-0.0121,  0.0013,  0.0066,  ..., -0.0123, -0.0015, -0.0103],
        [-0.0100, -0.0029, -0.0114,  ..., -0.0047, -0.0029,  0.0122],
        [ 0.0082,  0.0101,  0.0121,  ...,  0.0014, -0.0058,  0.0030]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.13.input_layernorm.weight Parameter containing:
tensor([0.4141, 0.4043, 0.3711,  ..., 0.3867, 0.3809, 0.3906], device='cuda:0')
base_model.model.model.layers.13.post_attention_layernorm.weight Parameter containing:
tensor([0.2637, 0.2520, 0.2451,  ..., 0.2637, 0.2656, 0.2598], device='cuda:0')
base_model.model.model.layers.14.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 87],
            [198],
            [ 82],
            ...,
            [125],
            [201],
            [145]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0095,  0.0096,  0.0030,  ..., -0.0111,  0.0080, -0.0100],
        [-0.0026,  0.0140, -0.0015,  ...,  0.0033,  0.0146, -0.0120],
        [-0.0050, -0.0078,  0.0108,  ...,  0.0037,  0.0088,  0.0102],
        ...,
        [-0.0092, -0.0073,  0.0039,  ..., -0.0067,  0.0121,  0.0048],
        [ 0.0035, -0.0092,  0.0022,  ..., -0.0047,  0.0099, -0.0143],
        [-0.0150,  0.0145, -0.0126,  ..., -0.0109,  0.0030,  0.0139]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.14.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 70],
            [151],
            [ 74],
            ...,
            [ 87],
            [175],
            [147]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0123,  0.0033, -0.0108,  ..., -0.0026,  0.0027, -0.0127],
        [-0.0089, -0.0152,  0.0144,  ...,  0.0076,  0.0097, -0.0022],
        [ 0.0111, -0.0002,  0.0087,  ..., -0.0072,  0.0114,  0.0093],
        ...,
        [ 0.0027, -0.0115,  0.0153,  ...,  0.0126, -0.0006,  0.0015],
        [-0.0111, -0.0065,  0.0056,  ...,  0.0063, -0.0109,  0.0023],
        [-0.0062, -0.0085,  0.0075,  ...,  0.0090,  0.0098,  0.0107]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.14.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[169],
            [  5],
            [103],
            ...,
            [166],
            [117],
            [ 92]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-2.8992e-04,  7.1716e-03,  1.3367e-02,  ...,  1.5320e-02,
         -1.0925e-02,  7.2327e-03],
        [-6.9580e-03,  3.1433e-03,  2.3041e-03,  ..., -8.8501e-03,
          7.5073e-03, -3.7537e-03],
        [ 6.5327e-05,  1.3580e-03, -5.1880e-03,  ...,  7.0190e-03,
         -9.0942e-03,  1.5320e-02],
        ...,
        [ 4.4250e-03,  4.9438e-03,  1.4893e-02,  ..., -1.2878e-02,
         -1.3062e-02, -2.8687e-03],
        [ 2.1553e-04, -1.8997e-03,  1.0315e-02,  ...,  6.5613e-04,
         -2.3346e-03,  1.2451e-02],
        [-9.7046e-03, -7.1411e-03,  1.2695e-02,  ...,  1.1597e-02,
         -1.3062e-02, -6.9885e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.14.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[ 57],
            [ 20],
            [140],
            ...,
            [ 45],
            [197],
            [152]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[ 5.8899e-03,  1.5442e-02, -1.3977e-02,  ...,  8.6670e-03,
          1.5488e-03, -8.0466e-06],
        [ 5.5847e-03, -1.2283e-03, -7.5989e-03,  ...,  2.8534e-03,
          1.4877e-03,  3.7079e-03],
        [-8.6670e-03,  6.1646e-03,  8.2397e-03,  ...,  1.2756e-02,
          1.0498e-02,  9.1553e-03],
        ...,
        [-6.4392e-03,  1.1108e-02, -1.1230e-02,  ...,  1.7700e-03,
         -1.2451e-02,  1.2024e-02],
        [-1.3733e-03, -1.6785e-03,  8.7280e-03,  ...,  7.7209e-03,
          3.0975e-03,  7.0190e-03],
        [ 1.3916e-02,  1.3184e-02, -2.6550e-03,  ...,  1.4099e-02,
         -6.9580e-03, -6.0425e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.14.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[58],
            [72],
            [19],
            ...,
            [69],
            [37],
            [29]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0027, -0.0001,  0.0069,  ...,  0.0091,  0.0121, -0.0093],
        [ 0.0041,  0.0056,  0.0085,  ...,  0.0068,  0.0070, -0.0128],
        [-0.0030, -0.0045,  0.0054,  ...,  0.0099, -0.0150,  0.0054],
        ...,
        [ 0.0098, -0.0129, -0.0013,  ...,  0.0030, -0.0009, -0.0131],
        [ 0.0121,  0.0092, -0.0081,  ...,  0.0002, -0.0037,  0.0080],
        [-0.0013,  0.0125,  0.0020,  ..., -0.0080,  0.0142,  0.0153]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.14.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[155],
            [134],
            [201],
            ...,
            [  2],
            [ 81],
            [172]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0033,  0.0078,  0.0008,  ..., -0.0067,  0.0036, -0.0067],
        [ 0.0082,  0.0055, -0.0093,  ...,  0.0035,  0.0029, -0.0091],
        [-0.0087,  0.0037, -0.0057,  ...,  0.0028,  0.0086, -0.0066],
        ...,
        [ 0.0027,  0.0022, -0.0029,  ..., -0.0055,  0.0007, -0.0091],
        [ 0.0073,  0.0043,  0.0082,  ..., -0.0056, -0.0002,  0.0026],
        [-0.0033,  0.0020,  0.0051,  ..., -0.0070,  0.0030,  0.0079]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.14.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[121],
            [168],
            [103],
            ...,
            [ 35],
            [ 91],
            [ 58]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0007,  0.0011, -0.0010,  ...,  0.0046, -0.0091, -0.0061],
        [-0.0002, -0.0093,  0.0056,  ...,  0.0117,  0.0098, -0.0003],
        [ 0.0056,  0.0029,  0.0096,  ..., -0.0011, -0.0011, -0.0009],
        ...,
        [ 0.0087,  0.0039, -0.0144,  ...,  0.0053,  0.0084, -0.0027],
        [ 0.0070,  0.0148,  0.0142,  ..., -0.0071,  0.0004, -0.0087],
        [ 0.0026, -0.0107, -0.0016,  ..., -0.0014, -0.0156, -0.0009]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.14.input_layernorm.weight Parameter containing:
tensor([0.4160, 0.4258, 0.3750,  ..., 0.4062, 0.3984, 0.3887], device='cuda:0')
base_model.model.model.layers.14.post_attention_layernorm.weight Parameter containing:
tensor([0.2734, 0.2617, 0.2598,  ..., 0.2773, 0.2734, 0.2695], device='cuda:0')
base_model.model.model.layers.15.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 82],
            [ 88],
            [226],
            ...,
            [ 93],
            [167],
            [ 69]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-7.7820e-03, -3.8605e-03,  1.1414e-02,  ...,  3.4637e-03,
         -1.4282e-02,  1.2024e-02],
        [ 2.9144e-03, -1.7395e-03, -2.8534e-03,  ..., -9.5215e-03,
         -2.1362e-03, -9.9487e-03],
        [ 8.3008e-03,  9.9487e-03, -6.7444e-03,  ...,  7.8125e-03,
         -2.5940e-03,  4.7302e-03],
        ...,
        [-1.3733e-03,  4.8218e-03, -3.5553e-03,  ..., -1.1169e-02,
          1.3000e-02,  9.6436e-03],
        [-5.3711e-03,  7.5989e-03, -5.6076e-04,  ...,  8.1177e-03,
          5.5237e-03, -1.4465e-02],
        [ 1.1658e-02,  3.4714e-04, -1.3367e-02,  ..., -1.9302e-03,
         -2.8534e-03,  1.1623e-05]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.15.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[213],
            [ 79],
            [244],
            ...,
            [121],
            [ 55],
            [ 87]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0016, -0.0109, -0.0114,  ..., -0.0009, -0.0124, -0.0130],
        [ 0.0128,  0.0093,  0.0075,  ...,  0.0130,  0.0118,  0.0044],
        [ 0.0153, -0.0031, -0.0110,  ...,  0.0071, -0.0129,  0.0114],
        ...,
        [ 0.0053, -0.0135, -0.0059,  ...,  0.0012,  0.0085,  0.0011],
        [-0.0033,  0.0031,  0.0078,  ..., -0.0009, -0.0073,  0.0150],
        [ 0.0046, -0.0078, -0.0022,  ..., -0.0155, -0.0029,  0.0041]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.15.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[151],
            [134],
            [158],
            ...,
            [199],
            [178],
            [137]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0137, -0.0004, -0.0033,  ..., -0.0038, -0.0082,  0.0147],
        [-0.0123,  0.0108, -0.0094,  ...,  0.0042,  0.0016, -0.0009],
        [-0.0125,  0.0062,  0.0149,  ..., -0.0149, -0.0078, -0.0093],
        ...,
        [-0.0109, -0.0049, -0.0059,  ...,  0.0117,  0.0128,  0.0143],
        [-0.0108,  0.0027, -0.0008,  ..., -0.0058,  0.0082, -0.0010],
        [ 0.0095, -0.0069, -0.0026,  ...,  0.0097, -0.0084,  0.0156]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.15.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[138],
            [ 48],
            [ 29],
            ...,
            [135],
            [150],
            [ 17]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0006,  0.0092,  0.0134,  ...,  0.0052, -0.0117, -0.0007],
        [-0.0059,  0.0011, -0.0142,  ..., -0.0139,  0.0128,  0.0119],
        [-0.0134, -0.0143, -0.0065,  ...,  0.0016, -0.0038, -0.0033],
        ...,
        [ 0.0031, -0.0032, -0.0140,  ..., -0.0097,  0.0015, -0.0062],
        [-0.0084,  0.0110, -0.0100,  ...,  0.0020,  0.0058,  0.0093],
        [-0.0095,  0.0074,  0.0007,  ...,  0.0010, -0.0060,  0.0054]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.15.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[ 94],
            [ 52],
            [106],
            ...,
            [  7],
            [103],
            [121]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0137,  0.0111,  0.0109,  ...,  0.0065,  0.0022,  0.0064],
        [ 0.0147,  0.0112,  0.0010,  ..., -0.0037,  0.0138,  0.0050],
        [-0.0130,  0.0058, -0.0030,  ..., -0.0047, -0.0043, -0.0098],
        ...,
        [-0.0030,  0.0007,  0.0111,  ..., -0.0093,  0.0047, -0.0056],
        [ 0.0081, -0.0079, -0.0120,  ..., -0.0099, -0.0151,  0.0035],
        [ 0.0088, -0.0031,  0.0138,  ..., -0.0058,  0.0102, -0.0063]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.15.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[ 83],
            [ 21],
            [105],
            ...,
            [196],
            [132],
            [ 34]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-7.5073e-03,  2.6321e-04,  2.3193e-03,  ..., -3.3264e-03,
          8.1787e-03,  1.0061e-04],
        [ 1.9531e-03,  1.6708e-03, -7.1411e-03,  ...,  8.7280e-03,
         -2.9297e-03, -3.8300e-03],
        [ 3.7842e-03, -2.0790e-04, -4.9438e-03,  ...,  7.5912e-04,
         -1.4267e-03,  9.5215e-03],
        ...,
        [-9.3994e-03,  6.6223e-03,  6.1951e-03,  ...,  1.5335e-03,
          1.6556e-03, -2.6550e-03],
        [-7.1716e-03,  7.7057e-04, -3.0670e-03,  ..., -3.7689e-03,
         -8.6060e-03, -1.5106e-03],
        [ 2.8491e-05, -3.9368e-03,  3.7384e-04,  ...,  9.0332e-03,
          1.5335e-03, -4.5776e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.15.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 85],
            [199],
            [ 34],
            ...,
            [ 53],
            [216],
            [ 55]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-1.7242e-03,  5.2185e-03, -4.6387e-03,  ...,  8.0566e-03,
          3.2654e-03,  1.3245e-02],
        [ 8.3618e-03,  4.6387e-03,  8.6060e-03,  ..., -1.0010e-02,
          2.6855e-03,  1.2390e-02],
        [ 2.2736e-03, -1.2573e-02,  5.6763e-03,  ..., -9.4604e-03,
         -4.6082e-03, -1.4038e-02],
        ...,
        [-9.3994e-03,  1.3123e-02,  1.4160e-02,  ...,  4.3335e-03,
         -9.3994e-03, -8.3447e-05],
        [ 1.4893e-02,  1.3351e-03,  1.3367e-02,  ...,  5.0049e-03,
          1.3184e-02,  3.5400e-03],
        [-6.1340e-03, -4.6082e-03,  2.9602e-03,  ..., -1.4526e-02,
         -9.5215e-03, -1.2451e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.15.input_layernorm.weight Parameter containing:
tensor([0.4062, 0.4004, 0.3770,  ..., 0.3848, 0.3848, 0.3887], device='cuda:0')
base_model.model.model.layers.15.post_attention_layernorm.weight Parameter containing:
tensor([0.2852, 0.2715, 0.2715,  ..., 0.2852, 0.2812, 0.2812], device='cuda:0')
base_model.model.model.layers.16.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[169],
            [ 34],
            [208],
            ...,
            [123],
            [ 54],
            [ 53]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0088, -0.0038,  0.0042,  ..., -0.0059, -0.0045,  0.0057],
        [ 0.0089, -0.0078,  0.0058,  ..., -0.0131,  0.0066, -0.0034],
        [-0.0132,  0.0079,  0.0104,  ..., -0.0156, -0.0151,  0.0138],
        ...,
        [-0.0026,  0.0103, -0.0088,  ...,  0.0150, -0.0048, -0.0073],
        [ 0.0002, -0.0015,  0.0077,  ..., -0.0006,  0.0004, -0.0056],
        [ 0.0137,  0.0015, -0.0082,  ..., -0.0143, -0.0044, -0.0051]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.16.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 60],
            [151],
            [125],
            ...,
            [ 71],
            [119],
            [ 92]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-1.0620e-02,  5.5237e-03, -1.4648e-02,  ...,  1.3062e-02,
         -3.6163e-03, -1.4267e-03],
        [ 1.4282e-02,  6.9275e-03,  1.3123e-02,  ...,  2.5635e-03,
         -9.0790e-04, -1.0559e-02],
        [-8.6670e-03,  1.1658e-02,  6.2256e-03,  ..., -2.0599e-03,
          9.3460e-04,  8.2397e-03],
        ...,
        [ 1.3916e-02, -6.5002e-03,  6.7139e-03,  ..., -6.6528e-03,
         -6.2466e-05, -9.8877e-03],
        [-1.5015e-02, -5.2795e-03,  1.2024e-02,  ..., -7.5073e-03,
         -1.5564e-02, -1.4099e-02],
        [-5.2795e-03, -4.8828e-04,  1.5015e-02,  ...,  4.0894e-03,
         -7.7209e-03,  3.9368e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.16.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[ 88],
            [ 99],
            [ 76],
            ...,
            [126],
            [180],
            [113]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0073,  0.0006,  0.0070,  ..., -0.0059,  0.0153, -0.0118],
        [ 0.0151,  0.0136,  0.0017,  ..., -0.0101, -0.0117, -0.0148],
        [ 0.0145,  0.0019, -0.0028,  ...,  0.0050,  0.0118, -0.0009],
        ...,
        [-0.0005,  0.0096,  0.0082,  ..., -0.0085,  0.0066,  0.0082],
        [ 0.0140,  0.0049, -0.0005,  ...,  0.0059, -0.0121, -0.0062],
        [-0.0107,  0.0081, -0.0105,  ...,  0.0042,  0.0082, -0.0074]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.16.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[231],
            [ 70],
            [ 57],
            ...,
            [ 87],
            [215],
            [103]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0008, -0.0041, -0.0111,  ..., -0.0153, -0.0062, -0.0085],
        [ 0.0015, -0.0090,  0.0129,  ..., -0.0073, -0.0079, -0.0025],
        [ 0.0011, -0.0063,  0.0131,  ..., -0.0074,  0.0055, -0.0019],
        ...,
        [ 0.0041,  0.0040, -0.0011,  ..., -0.0008,  0.0064,  0.0045],
        [-0.0131, -0.0038, -0.0024,  ...,  0.0082, -0.0145,  0.0031],
        [-0.0043, -0.0024,  0.0048,  ...,  0.0144, -0.0066, -0.0027]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.16.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[ 51],
            [ 45],
            [ 62],
            ...,
            [121],
            [113],
            [197]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 1.3489e-02, -7.3242e-03,  9.5825e-03,  ...,  3.7537e-03,
         -1.3306e-02,  1.4221e-02],
        [ 3.0518e-03, -1.4282e-02,  1.5564e-02,  ...,  1.1169e-02,
         -2.6093e-03, -1.9150e-03],
        [ 3.6621e-03, -8.0566e-03, -1.0986e-02,  ...,  4.6387e-03,
         -1.1981e-05,  1.1368e-03],
        ...,
        [-6.6223e-03,  4.2114e-03, -1.2451e-02,  ..., -4.3335e-03,
         -1.4465e-02,  1.2634e-02],
        [ 4.8447e-04,  4.6387e-03, -3.6621e-03,  ..., -6.7139e-03,
          1.1108e-02, -9.5215e-03],
        [-3.3875e-03,  8.6060e-03,  7.2937e-03,  ..., -9.3994e-03,
          1.2878e-02, -3.4637e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.16.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[ 55],
            [188],
            [ 53],
            ...,
            [135],
            [125],
            [132]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-7.6904e-03,  2.5787e-03, -1.6937e-03,  ...,  4.6387e-03,
          5.7678e-03, -6.8054e-03],
        [-4.6997e-03, -7.9956e-03,  9.4604e-03,  ..., -1.8768e-03,
          8.3618e-03, -8.2397e-03],
        [-3.4790e-03,  2.1362e-03,  9.5215e-03,  ...,  6.5994e-04,
          4.8523e-03,  1.9226e-03],
        ...,
        [-8.5449e-03,  4.4861e-03, -1.4842e-05,  ..., -4.1504e-03,
          5.3101e-03,  8.1253e-04],
        [ 5.0354e-03,  7.9956e-03,  2.6245e-03,  ...,  3.4943e-03,
         -6.5308e-03, -9.3384e-03],
        [-2.2736e-03,  5.0964e-03,  4.6692e-03,  ...,  3.0365e-03,
         -8.6212e-04, -4.7493e-04]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.16.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 26],
            [141],
            [111],
            ...,
            [125],
            [183],
            [ 51]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0110, -0.0118,  0.0061,  ...,  0.0078, -0.0030, -0.0006],
        [-0.0154,  0.0105, -0.0065,  ...,  0.0030, -0.0002, -0.0110],
        [ 0.0027,  0.0019, -0.0142,  ...,  0.0081,  0.0089,  0.0091],
        ...,
        [ 0.0073,  0.0010, -0.0134,  ..., -0.0008,  0.0150,  0.0059],
        [ 0.0037,  0.0125, -0.0010,  ..., -0.0102, -0.0148,  0.0077],
        [ 0.0031,  0.0080, -0.0128,  ..., -0.0120, -0.0046,  0.0096]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.16.input_layernorm.weight Parameter containing:
tensor([0.4102, 0.4160, 0.3867,  ..., 0.3867, 0.4023, 0.4004], device='cuda:0')
base_model.model.model.layers.16.post_attention_layernorm.weight Parameter containing:
tensor([0.3027, 0.2891, 0.2910,  ..., 0.3027, 0.3066, 0.2969], device='cuda:0')
base_model.model.model.layers.17.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 34],
            [215],
            [150],
            ...,
            [216],
            [219],
            [126]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0055, -0.0140, -0.0011,  ..., -0.0086, -0.0125,  0.0082],
        [ 0.0010,  0.0053,  0.0028,  ...,  0.0144, -0.0035, -0.0015],
        [ 0.0126, -0.0125,  0.0022,  ...,  0.0131, -0.0072, -0.0058],
        ...,
        [ 0.0153, -0.0118,  0.0068,  ..., -0.0085, -0.0015,  0.0106],
        [ 0.0125,  0.0023,  0.0121,  ...,  0.0133,  0.0099,  0.0047],
        [-0.0131, -0.0141,  0.0035,  ..., -0.0154,  0.0140,  0.0093]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.17.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 41],
            [153],
            [194],
            ...,
            [182],
            [ 58],
            [150]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0138, -0.0028, -0.0126,  ...,  0.0107, -0.0138, -0.0038],
        [ 0.0128, -0.0008,  0.0010,  ...,  0.0115,  0.0148,  0.0099],
        [ 0.0057,  0.0134, -0.0075,  ..., -0.0118, -0.0013,  0.0053],
        ...,
        [ 0.0117, -0.0096, -0.0109,  ...,  0.0087,  0.0085, -0.0138],
        [ 0.0142,  0.0067,  0.0126,  ..., -0.0024, -0.0076, -0.0069],
        [ 0.0128,  0.0116, -0.0087,  ..., -0.0008, -0.0028,  0.0080]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.17.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[ 73],
            [156],
            [ 86],
            ...,
            [157],
            [186],
            [169]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 8.1787e-03,  1.0376e-02, -1.3504e-03,  ..., -6.9885e-03,
         -3.1586e-03,  9.8267e-03],
        [-2.9907e-03,  4.7913e-03, -1.1841e-02,  ..., -8.7280e-03,
          1.2512e-02, -5.9128e-05],
        [-1.2939e-02,  9.8877e-03, -3.6316e-03,  ..., -1.4343e-02,
         -8.1787e-03,  1.3046e-03],
        ...,
        [-4.1199e-03,  7.2632e-03,  6.1340e-03,  ...,  8.1177e-03,
         -9.5825e-03, -7.4158e-03],
        [-9.3384e-03,  9.9487e-03, -1.8597e-04,  ..., -5.2185e-03,
         -4.3945e-03,  1.9531e-03],
        [ 6.8665e-03, -3.4027e-03, -4.8828e-03,  ...,  6.4087e-03,
          1.0437e-02,  1.3062e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.17.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[101],
            [164],
            [ 87],
            ...,
            [ 39],
            [ 47],
            [ 44]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0051, -0.0129, -0.0047,  ..., -0.0009, -0.0043,  0.0127],
        [-0.0043,  0.0083,  0.0093,  ...,  0.0121, -0.0138,  0.0087],
        [-0.0109,  0.0109,  0.0041,  ...,  0.0151, -0.0011,  0.0092],
        ...,
        [-0.0135, -0.0084,  0.0092,  ...,  0.0048,  0.0050, -0.0002],
        [-0.0079, -0.0084, -0.0069,  ...,  0.0098, -0.0073, -0.0105],
        [-0.0153, -0.0131, -0.0028,  ...,  0.0109,  0.0128,  0.0098]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.17.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[ 44],
            [171],
            [169],
            ...,
            [103],
            [ 68],
            [150]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0015,  0.0035, -0.0109,  ...,  0.0011,  0.0066, -0.0133],
        [ 0.0045, -0.0115, -0.0037,  ..., -0.0096, -0.0129,  0.0115],
        [-0.0128,  0.0042,  0.0066,  ...,  0.0120, -0.0055,  0.0118],
        ...,
        [ 0.0045,  0.0073,  0.0061,  ..., -0.0067,  0.0078, -0.0116],
        [-0.0083,  0.0052, -0.0037,  ...,  0.0018, -0.0133, -0.0011],
        [-0.0039, -0.0151, -0.0020,  ...,  0.0031,  0.0084,  0.0064]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.17.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[195],
            [156],
            [245],
            ...,
            [104],
            [ 66],
            [ 88]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0064, -0.0015,  0.0064,  ..., -0.0047,  0.0031,  0.0020],
        [-0.0019, -0.0022,  0.0077,  ..., -0.0040, -0.0082,  0.0078],
        [-0.0051, -0.0035,  0.0027,  ...,  0.0085,  0.0034, -0.0007],
        ...,
        [ 0.0045, -0.0040,  0.0008,  ..., -0.0040, -0.0088,  0.0064],
        [ 0.0070, -0.0005,  0.0072,  ..., -0.0043,  0.0095,  0.0092],
        [-0.0030,  0.0003,  0.0034,  ...,  0.0064,  0.0018, -0.0047]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.17.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 82],
            [ 83],
            [ 55],
            ...,
            [ 81],
            [184],
            [  3]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0121,  0.0012, -0.0057,  ...,  0.0084,  0.0141, -0.0142],
        [ 0.0071, -0.0019, -0.0059,  ...,  0.0120,  0.0109,  0.0145],
        [-0.0048, -0.0078,  0.0013,  ...,  0.0050, -0.0085, -0.0073],
        ...,
        [-0.0013,  0.0039, -0.0139,  ...,  0.0134, -0.0088,  0.0081],
        [ 0.0059, -0.0147,  0.0056,  ...,  0.0085,  0.0129, -0.0109],
        [ 0.0085, -0.0154,  0.0078,  ..., -0.0128,  0.0010,  0.0140]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.17.input_layernorm.weight Parameter containing:
tensor([0.4238, 0.4277, 0.4004,  ..., 0.4199, 0.4219, 0.4043], device='cuda:0')
base_model.model.model.layers.17.post_attention_layernorm.weight Parameter containing:
tensor([0.3203, 0.3125, 0.3105,  ..., 0.3223, 0.3242, 0.3145], device='cuda:0')
base_model.model.model.layers.18.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[152],
            [141],
            [  8],
            ...,
            [ 67],
            [202],
            [209]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 8.7891e-03, -2.0294e-03, -9.8877e-03,  ...,  2.5988e-05,
          1.5198e-02, -1.0681e-02],
        [-1.1963e-02, -3.0060e-03, -9.4604e-03,  ...,  5.5237e-03,
          4.4556e-03,  1.3367e-02],
        [-4.7302e-03, -2.2507e-04,  7.0190e-03,  ...,  1.8539e-03,
         -5.8899e-03,  1.0010e-02],
        ...,
        [ 1.6479e-03,  1.5320e-02,  8.6060e-03,  ..., -6.4087e-03,
          1.2970e-03,  5.1270e-03],
        [ 8.4229e-03,  7.0190e-03,  1.4526e-02,  ...,  5.2490e-03,
         -1.3672e-02, -3.0899e-04],
        [ 1.5259e-02, -1.6632e-03,  2.1515e-03,  ..., -2.7924e-03,
          1.4648e-02, -9.7656e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.18.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 62],
            [ 66],
            [ 54],
            ...,
            [145],
            [156],
            [ 83]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0145,  0.0087,  0.0134,  ...,  0.0134, -0.0047, -0.0087],
        [-0.0060, -0.0079, -0.0134,  ...,  0.0140, -0.0130, -0.0055],
        [-0.0053, -0.0139, -0.0040,  ..., -0.0079,  0.0144,  0.0142],
        ...,
        [-0.0098,  0.0098,  0.0073,  ...,  0.0022, -0.0048,  0.0042],
        [-0.0075, -0.0084,  0.0153,  ..., -0.0136,  0.0135,  0.0104],
        [-0.0112, -0.0143, -0.0156,  ..., -0.0123,  0.0032, -0.0134]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.18.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[ 43],
            [ 53],
            [119],
            ...,
            [ 44],
            [149],
            [ 71]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0155, -0.0023,  0.0093,  ...,  0.0019,  0.0130, -0.0034],
        [ 0.0135, -0.0106, -0.0127,  ...,  0.0111, -0.0056,  0.0019],
        [ 0.0035,  0.0051,  0.0045,  ..., -0.0105,  0.0053, -0.0128],
        ...,
        [-0.0055,  0.0126, -0.0062,  ...,  0.0020, -0.0015, -0.0025],
        [-0.0082, -0.0060,  0.0058,  ...,  0.0022, -0.0030,  0.0033],
        [ 0.0022,  0.0047,  0.0151,  ...,  0.0002, -0.0088,  0.0021]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.18.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[ 58],
            [ 67],
            [206],
            ...,
            [ 64],
            [211],
            [178]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0135, -0.0010, -0.0079,  ...,  0.0139,  0.0065, -0.0148],
        [ 0.0027,  0.0057, -0.0077,  ..., -0.0090,  0.0082, -0.0082],
        [-0.0021,  0.0145,  0.0049,  ...,  0.0073, -0.0120,  0.0045],
        ...,
        [ 0.0131, -0.0029,  0.0097,  ...,  0.0082,  0.0142, -0.0106],
        [-0.0013, -0.0079, -0.0124,  ..., -0.0102, -0.0113, -0.0056],
        [ 0.0049,  0.0018,  0.0077,  ...,  0.0086,  0.0054,  0.0085]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.18.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[166],
            [167],
            [115],
            ...,
            [ 23],
            [105],
            [103]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0084,  0.0042,  0.0033,  ...,  0.0117, -0.0027, -0.0132],
        [-0.0144, -0.0097, -0.0125,  ...,  0.0056, -0.0142,  0.0128],
        [-0.0141,  0.0045, -0.0127,  ..., -0.0002, -0.0145, -0.0122],
        ...,
        [-0.0038, -0.0073, -0.0132,  ...,  0.0050,  0.0006, -0.0016],
        [-0.0058,  0.0019, -0.0096,  ...,  0.0026, -0.0043,  0.0037],
        [ 0.0052,  0.0028, -0.0117,  ..., -0.0091,  0.0150, -0.0084]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.18.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[218],
            [ 31],
            [127],
            ...,
            [106],
            [214],
            [186]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0095, -0.0013,  0.0060,  ..., -0.0033, -0.0072,  0.0043],
        [-0.0025,  0.0074,  0.0045,  ..., -0.0064, -0.0017, -0.0023],
        [ 0.0072,  0.0082, -0.0055,  ..., -0.0014, -0.0022, -0.0080],
        ...,
        [ 0.0076,  0.0041, -0.0080,  ...,  0.0093, -0.0069, -0.0077],
        [-0.0003,  0.0049, -0.0046,  ...,  0.0062, -0.0023, -0.0021],
        [ 0.0026,  0.0063, -0.0066,  ...,  0.0062,  0.0033, -0.0082]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.18.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[170],
            [220],
            [178],
            ...,
            [ 98],
            [164],
            [187]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0151,  0.0106,  0.0142,  ...,  0.0078, -0.0145,  0.0092],
        [-0.0084, -0.0140,  0.0079,  ...,  0.0104, -0.0070, -0.0017],
        [-0.0079, -0.0106,  0.0142,  ..., -0.0067,  0.0079,  0.0142],
        ...,
        [ 0.0085,  0.0071,  0.0066,  ...,  0.0082, -0.0031,  0.0042],
        [ 0.0045, -0.0081,  0.0103,  ...,  0.0050,  0.0067,  0.0069],
        [ 0.0035,  0.0143, -0.0099,  ...,  0.0094,  0.0114,  0.0087]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.18.input_layernorm.weight Parameter containing:
tensor([0.4473, 0.4473, 0.4277,  ..., 0.4297, 0.4414, 0.4258], device='cuda:0')
base_model.model.model.layers.18.post_attention_layernorm.weight Parameter containing:
tensor([0.3398, 0.3301, 0.3281,  ..., 0.3359, 0.3398, 0.3340], device='cuda:0')
base_model.model.model.layers.19.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[137],
            [ 82],
            [154],
            ...,
            [200],
            [215],
            [ 37]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0077, -0.0054,  0.0141,  ...,  0.0044, -0.0016, -0.0044],
        [ 0.0144, -0.0074, -0.0084,  ..., -0.0017, -0.0034, -0.0105],
        [-0.0053,  0.0106,  0.0131,  ..., -0.0088,  0.0129,  0.0082],
        ...,
        [ 0.0151, -0.0031, -0.0025,  ..., -0.0111, -0.0112,  0.0115],
        [ 0.0070,  0.0001, -0.0007,  ...,  0.0065,  0.0134,  0.0151],
        [ 0.0035,  0.0143,  0.0021,  ...,  0.0115, -0.0117,  0.0034]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.19.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[219],
            [133],
            [ 69],
            ...,
            [200],
            [ 84],
            [155]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0146,  0.0103, -0.0013,  ...,  0.0024, -0.0156, -0.0058],
        [ 0.0080,  0.0079,  0.0079,  ..., -0.0026, -0.0118, -0.0027],
        [ 0.0127,  0.0143, -0.0128,  ..., -0.0130, -0.0004,  0.0138],
        ...,
        [ 0.0076,  0.0029, -0.0129,  ..., -0.0062,  0.0087,  0.0089],
        [-0.0156, -0.0106,  0.0010,  ..., -0.0108,  0.0014, -0.0098],
        [-0.0061, -0.0076,  0.0118,  ...,  0.0124,  0.0118, -0.0058]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.19.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[134],
            [102],
            [153],
            ...,
            [105],
            [130],
            [ 75]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0037, -0.0014,  0.0130,  ..., -0.0102, -0.0031, -0.0115],
        [-0.0106,  0.0042, -0.0075,  ...,  0.0031,  0.0098, -0.0119],
        [-0.0075, -0.0145,  0.0059,  ..., -0.0117, -0.0115, -0.0029],
        ...,
        [ 0.0023,  0.0154,  0.0062,  ...,  0.0105, -0.0088,  0.0139],
        [ 0.0151, -0.0067,  0.0084,  ...,  0.0101, -0.0063, -0.0116],
        [-0.0023, -0.0131,  0.0137,  ..., -0.0084, -0.0146, -0.0144]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.19.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[192],
            [195],
            [ 93],
            ...,
            [213],
            [195],
            [ 75]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[ 8.5449e-03, -9.9487e-03, -1.9302e-03,  ..., -1.1841e-02,
          2.4261e-03, -1.4038e-03],
        [-1.1780e-02, -4.0283e-03,  5.3711e-03,  ..., -4.0436e-04,
         -6.0425e-03,  6.9580e-03],
        [ 1.3916e-02,  1.4572e-03,  5.9509e-03,  ..., -6.7444e-03,
          9.8877e-03,  1.0803e-02],
        ...,
        [-1.3184e-02,  8.7357e-04, -3.2043e-03,  ...,  5.2185e-03,
          7.2632e-03, -6.0730e-03],
        [-1.5564e-02,  5.0354e-03,  1.3733e-02,  ...,  6.6223e-03,
         -1.3611e-02,  1.0193e-02],
        [ 6.0730e-03,  4.3945e-03, -1.3123e-02,  ..., -7.5684e-03,
         -2.6822e-05, -1.0803e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.19.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[ 72],
            [152],
            [163],
            ...,
            [151],
            [ 87],
            [151]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0005,  0.0062,  0.0074,  ...,  0.0002, -0.0059, -0.0143],
        [ 0.0071,  0.0076,  0.0066,  ..., -0.0004, -0.0089,  0.0156],
        [-0.0140, -0.0026,  0.0115,  ..., -0.0106, -0.0134,  0.0155],
        ...,
        [-0.0137, -0.0097, -0.0121,  ..., -0.0092,  0.0041,  0.0068],
        [-0.0006, -0.0151, -0.0056,  ...,  0.0130,  0.0063,  0.0041],
        [-0.0080,  0.0021,  0.0056,  ...,  0.0029,  0.0079, -0.0014]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.19.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[ 98],
            [150],
            [118],
            ...,
            [236],
            [103],
            [153]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0045, -0.0091,  0.0045,  ..., -0.0088, -0.0090, -0.0029],
        [-0.0020, -0.0065,  0.0074,  ..., -0.0018,  0.0020,  0.0039],
        [-0.0058,  0.0009,  0.0052,  ...,  0.0040, -0.0005,  0.0018],
        ...,
        [-0.0050, -0.0053,  0.0026,  ...,  0.0071, -0.0047,  0.0086],
        [-0.0057, -0.0046, -0.0052,  ...,  0.0070, -0.0018,  0.0086],
        [ 0.0082,  0.0061,  0.0006,  ..., -0.0082,  0.0051, -0.0004]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.19.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[114],
            [184],
            [ 58],
            ...,
            [  6],
            [102],
            [121]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0031, -0.0009, -0.0085,  ...,  0.0136,  0.0132,  0.0032],
        [-0.0037, -0.0150, -0.0084,  ..., -0.0042,  0.0098,  0.0039],
        [ 0.0119,  0.0007, -0.0140,  ..., -0.0096,  0.0035, -0.0068],
        ...,
        [-0.0038,  0.0077, -0.0064,  ...,  0.0140,  0.0076, -0.0148],
        [ 0.0070, -0.0027,  0.0002,  ..., -0.0013, -0.0086, -0.0126],
        [ 0.0134,  0.0038,  0.0045,  ..., -0.0056, -0.0153, -0.0111]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.19.input_layernorm.weight Parameter containing:
tensor([0.4512, 0.4570, 0.4375,  ..., 0.4258, 0.4336, 0.4375], device='cuda:0')
base_model.model.model.layers.19.post_attention_layernorm.weight Parameter containing:
tensor([0.3535, 0.3398, 0.3418,  ..., 0.3496, 0.3496, 0.3477], device='cuda:0')
base_model.model.model.layers.20.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[133],
            [226],
            [214],
            ...,
            [159],
            [181],
            [ 87]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0085, -0.0006,  0.0108,  ..., -0.0082,  0.0033,  0.0069],
        [-0.0010,  0.0094, -0.0150,  ...,  0.0027,  0.0049,  0.0009],
        [-0.0052,  0.0077,  0.0035,  ..., -0.0061, -0.0065, -0.0037],
        ...,
        [ 0.0117, -0.0154,  0.0062,  ..., -0.0066, -0.0003, -0.0084],
        [-0.0137, -0.0056,  0.0012,  ...,  0.0092,  0.0094, -0.0055],
        [ 0.0014,  0.0049,  0.0108,  ..., -0.0089,  0.0110,  0.0059]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.20.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 90],
            [194],
            [101],
            ...,
            [135],
            [ 27],
            [ 61]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 5.0354e-03, -1.2451e-02,  4.1504e-03,  ...,  7.6599e-03,
          1.0193e-02,  2.7924e-03],
        [ 1.1658e-02,  1.2573e-02, -1.0010e-02,  ..., -4.3335e-03,
         -7.0496e-03,  1.1230e-02],
        [-3.5400e-03, -5.5542e-03, -6.4373e-05,  ...,  1.0925e-02,
          1.3489e-02, -7.3242e-03],
        ...,
        [-5.8594e-03, -1.1902e-03, -1.5015e-02,  ...,  8.9722e-03,
          8.6670e-03, -7.6904e-03],
        [-9.7046e-03,  1.0376e-03, -1.2573e-02,  ...,  6.7139e-03,
         -8.3923e-05,  2.9755e-03],
        [-4.0588e-03,  1.3611e-02,  1.3977e-02,  ..., -7.9346e-03,
          7.0496e-03, -6.1951e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.20.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[100],
            [184],
            [ 39],
            ...,
            [172],
            [ 85],
            [ 64]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0129, -0.0037, -0.0008,  ..., -0.0084, -0.0099, -0.0062],
        [-0.0077, -0.0025,  0.0063,  ..., -0.0059,  0.0109,  0.0016],
        [-0.0116,  0.0153, -0.0035,  ...,  0.0133, -0.0028,  0.0135],
        ...,
        [-0.0112, -0.0147,  0.0140,  ..., -0.0065, -0.0104,  0.0008],
        [-0.0059,  0.0083, -0.0052,  ...,  0.0009, -0.0001, -0.0102],
        [-0.0061,  0.0095,  0.0106,  ..., -0.0049, -0.0149, -0.0081]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.20.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[152],
            [214],
            [169],
            ...,
            [117],
            [ 43],
            [ 42]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0119,  0.0004, -0.0106,  ...,  0.0078, -0.0137,  0.0067],
        [-0.0022,  0.0042,  0.0049,  ...,  0.0027, -0.0140, -0.0145],
        [-0.0148, -0.0092,  0.0133,  ...,  0.0059,  0.0008, -0.0002],
        ...,
        [-0.0086, -0.0145, -0.0062,  ...,  0.0022, -0.0131, -0.0019],
        [ 0.0148, -0.0110,  0.0019,  ...,  0.0027, -0.0019, -0.0020],
        [-0.0043,  0.0137,  0.0008,  ..., -0.0067,  0.0143,  0.0045]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.20.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[118],
            [ 57],
            [107],
            ...,
            [ 17],
            [119],
            [ 38]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0016, -0.0009, -0.0117,  ..., -0.0087,  0.0152,  0.0105],
        [ 0.0043,  0.0089,  0.0001,  ...,  0.0017, -0.0067, -0.0081],
        [ 0.0117,  0.0029, -0.0121,  ...,  0.0150, -0.0055, -0.0101],
        ...,
        [-0.0091,  0.0107,  0.0109,  ..., -0.0019,  0.0153,  0.0146],
        [-0.0055, -0.0090,  0.0090,  ..., -0.0129, -0.0089,  0.0146],
        [ 0.0020,  0.0058, -0.0142,  ..., -0.0112,  0.0054,  0.0104]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.20.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[109],
            [101],
            [150],
            ...,
            [149],
            [247],
            [ 86]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0041,  0.0080, -0.0018,  ..., -0.0041,  0.0025, -0.0053],
        [-0.0028, -0.0009,  0.0004,  ..., -0.0087,  0.0067, -0.0034],
        [-0.0059, -0.0060,  0.0078,  ...,  0.0081,  0.0067, -0.0080],
        ...,
        [-0.0070, -0.0015,  0.0067,  ...,  0.0070, -0.0048,  0.0059],
        [-0.0030,  0.0072, -0.0010,  ...,  0.0068,  0.0005,  0.0065],
        [-0.0082,  0.0014, -0.0061,  ..., -0.0056,  0.0034, -0.0089]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.20.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[  7],
            [120],
            [102],
            ...,
            [ 66],
            [245],
            [ 53]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0089,  0.0123,  0.0027,  ...,  0.0082,  0.0153,  0.0140],
        [-0.0118,  0.0118, -0.0003,  ...,  0.0095,  0.0126,  0.0154],
        [-0.0068, -0.0032, -0.0127,  ..., -0.0122, -0.0112,  0.0129],
        ...,
        [-0.0029, -0.0012,  0.0126,  ...,  0.0101,  0.0016, -0.0050],
        [-0.0029,  0.0143, -0.0080,  ..., -0.0116,  0.0115, -0.0012],
        [-0.0129, -0.0095,  0.0014,  ...,  0.0006,  0.0061, -0.0092]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.20.input_layernorm.weight Parameter containing:
tensor([0.4531, 0.4668, 0.4375,  ..., 0.4336, 0.4336, 0.4473], device='cuda:0')
base_model.model.model.layers.20.post_attention_layernorm.weight Parameter containing:
tensor([0.3633, 0.3574, 0.3516,  ..., 0.3633, 0.3613, 0.3574], device='cuda:0')
base_model.model.model.layers.21.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 37],
            [219],
            [ 81],
            ...,
            [ 74],
            [ 70],
            [162]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0142, -0.0013,  0.0135,  ..., -0.0064, -0.0107, -0.0060],
        [ 0.0071,  0.0135,  0.0084,  ..., -0.0021, -0.0043,  0.0020],
        [-0.0089,  0.0011,  0.0150,  ...,  0.0115, -0.0056, -0.0080],
        ...,
        [-0.0052, -0.0153,  0.0119,  ..., -0.0144, -0.0091,  0.0070],
        [ 0.0112,  0.0085, -0.0036,  ..., -0.0132,  0.0085, -0.0141],
        [-0.0075, -0.0134,  0.0013,  ..., -0.0081,  0.0134,  0.0007]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.21.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[156],
            [157],
            [231],
            ...,
            [149],
            [215],
            [ 79]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0088,  0.0060,  0.0027,  ..., -0.0078,  0.0073,  0.0054],
        [-0.0109, -0.0133, -0.0056,  ...,  0.0004,  0.0033,  0.0035],
        [-0.0042,  0.0151, -0.0053,  ...,  0.0139, -0.0116, -0.0046],
        ...,
        [ 0.0035, -0.0020, -0.0098,  ..., -0.0075, -0.0017, -0.0123],
        [ 0.0129,  0.0018,  0.0067,  ..., -0.0012,  0.0093, -0.0150],
        [-0.0039, -0.0084, -0.0002,  ..., -0.0076, -0.0071,  0.0099]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.21.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[121],
            [101],
            [ 83],
            ...,
            [ 36],
            [152],
            [ 71]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-8.7280e-03, -3.0708e-04, -5.8289e-03,  ...,  1.5335e-03,
         -1.0010e-02, -1.1536e-02],
        [ 3.6163e-03, -7.8735e-03,  1.2939e-02,  ...,  2.6855e-03,
          1.4282e-02, -1.4954e-02],
        [ 2.8076e-03,  3.3417e-03, -1.1292e-02,  ..., -4.3945e-03,
         -1.0498e-02, -1.3672e-02],
        ...,
        [-6.2256e-03, -1.1353e-02, -3.6011e-03,  ...,  5.7983e-03,
          3.8147e-03,  7.4158e-03],
        [ 1.1719e-02, -7.2021e-03,  1.5381e-02,  ...,  9.2163e-03,
         -4.5166e-03, -1.8311e-03],
        [ 1.4404e-02, -4.5471e-03, -6.0730e-03,  ..., -6.2943e-05,
          1.5442e-02,  5.6152e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.21.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[ 71],
            [166],
            [124],
            ...,
            [ 36],
            [174],
            [ 56]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0129, -0.0110, -0.0090,  ...,  0.0103, -0.0075, -0.0022],
        [-0.0121,  0.0112,  0.0072,  ..., -0.0126,  0.0122,  0.0132],
        [ 0.0051, -0.0072,  0.0017,  ...,  0.0151,  0.0074,  0.0049],
        ...,
        [-0.0011, -0.0091,  0.0139,  ..., -0.0097, -0.0100, -0.0045],
        [ 0.0084,  0.0109,  0.0003,  ...,  0.0142, -0.0065,  0.0132],
        [ 0.0059, -0.0011, -0.0024,  ...,  0.0070, -0.0153,  0.0085]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.21.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[ 98],
            [162],
            [165],
            ...,
            [  7],
            [ 35],
            [121]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0056,  0.0156,  0.0145,  ..., -0.0041, -0.0055,  0.0025],
        [ 0.0083,  0.0123, -0.0091,  ...,  0.0138, -0.0084, -0.0014],
        [-0.0009,  0.0061,  0.0080,  ...,  0.0048, -0.0010, -0.0110],
        ...,
        [-0.0033, -0.0032,  0.0006,  ..., -0.0117,  0.0021, -0.0154],
        [-0.0127, -0.0121,  0.0148,  ...,  0.0104, -0.0035, -0.0016],
        [-0.0011, -0.0007,  0.0013,  ...,  0.0135,  0.0132, -0.0049]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.21.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[171],
            [104],
            [141],
            ...,
            [ 59],
            [116],
            [179]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0017, -0.0029,  0.0047,  ...,  0.0018, -0.0033,  0.0049],
        [-0.0064,  0.0067, -0.0083,  ...,  0.0088,  0.0023,  0.0054],
        [-0.0067,  0.0063, -0.0085,  ...,  0.0001,  0.0013, -0.0042],
        ...,
        [-0.0049,  0.0029, -0.0051,  ...,  0.0018,  0.0038,  0.0045],
        [ 0.0046, -0.0090,  0.0094,  ..., -0.0056,  0.0050, -0.0085],
        [-0.0031, -0.0068, -0.0073,  ...,  0.0002,  0.0051,  0.0004]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.21.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[209],
            [ 55],
            [217],
            ...,
            [ 22],
            [133],
            [ 97]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-1.0071e-02, -4.4250e-03,  5.0964e-03,  ..., -1.7700e-03,
         -9.7046e-03,  7.8125e-03],
        [ 8.0872e-04, -1.1230e-02,  1.4343e-02,  ..., -1.2878e-02,
          4.0894e-03,  1.3000e-02],
        [-1.3794e-02, -1.3245e-02, -9.3384e-03,  ..., -1.7929e-03,
         -9.3994e-03, -3.3722e-03],
        ...,
        [ 1.9073e-03, -1.3123e-02,  5.9814e-03,  ...,  6.6833e-03,
         -1.4954e-02,  1.3000e-02],
        [-2.7657e-05, -1.2207e-02,  1.0376e-02,  ...,  1.2360e-03,
          8.8501e-03,  6.0730e-03],
        [-2.3499e-03, -8.9722e-03,  5.0354e-03,  ...,  7.9346e-03,
          8.5449e-03, -1.5259e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.21.input_layernorm.weight Parameter containing:
tensor([0.4785, 0.4863, 0.4688,  ..., 0.4551, 0.4707, 0.4766], device='cuda:0')
base_model.model.model.layers.21.post_attention_layernorm.weight Parameter containing:
tensor([0.3730, 0.3691, 0.3633,  ..., 0.3789, 0.3711, 0.3730], device='cuda:0')
base_model.model.model.layers.22.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 52],
            [ 84],
            [ 26],
            ...,
            [152],
            [146],
            [154]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-9.8877e-03, -2.3041e-03,  1.3245e-02,  ...,  1.2878e-02,
          9.8267e-03, -1.3611e-02],
        [-1.4160e-02,  1.4114e-03,  3.6469e-03,  ..., -1.3885e-03,
          3.7003e-04, -1.2207e-03],
        [-1.8768e-03, -1.3184e-02,  5.3406e-03,  ...,  1.4282e-02,
          3.2501e-03, -6.9885e-03],
        ...,
        [ 1.5076e-02,  1.5497e-05, -9.3994e-03,  ...,  8.7891e-03,
         -1.2390e-02, -9.2163e-03],
        [-1.3062e-02, -1.2268e-02, -1.4587e-02,  ..., -7.9956e-03,
          1.4771e-02, -1.5015e-02],
        [ 1.5137e-02,  1.1597e-02, -2.4109e-03,  ..., -1.8311e-03,
         -3.1738e-03, -9.4604e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.22.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 55],
            [102],
            [ 87],
            ...,
            [241],
            [107],
            [ 57]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0135,  0.0063, -0.0116,  ..., -0.0051, -0.0015, -0.0006],
        [ 0.0148, -0.0128,  0.0048,  ..., -0.0128, -0.0025, -0.0005],
        [-0.0048, -0.0121, -0.0118,  ...,  0.0117,  0.0084,  0.0093],
        ...,
        [ 0.0051,  0.0016,  0.0009,  ..., -0.0074, -0.0024, -0.0014],
        [-0.0025,  0.0027, -0.0106,  ...,  0.0119, -0.0075, -0.0020],
        [ 0.0127,  0.0140, -0.0149,  ...,  0.0024, -0.0023, -0.0117]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.22.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[ 56],
            [106],
            [251],
            ...,
            [137],
            [153],
            [125]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0137, -0.0014, -0.0010,  ..., -0.0005, -0.0097,  0.0022],
        [ 0.0122, -0.0091,  0.0010,  ...,  0.0132,  0.0129,  0.0119],
        [-0.0089,  0.0136,  0.0111,  ..., -0.0127, -0.0143,  0.0039],
        ...,
        [ 0.0151, -0.0026, -0.0150,  ...,  0.0008, -0.0025, -0.0070],
        [ 0.0135, -0.0024, -0.0006,  ...,  0.0027, -0.0004, -0.0108],
        [-0.0007,  0.0143, -0.0052,  ...,  0.0057,  0.0004,  0.0097]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.22.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[202],
            [ 54],
            [185],
            ...,
            [152],
            [169],
            [170]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-4.7607e-03, -3.8300e-03, -1.1597e-02,  ..., -3.4714e-04,
          5.6152e-03,  2.7313e-03],
        [ 1.3428e-02, -1.3275e-03,  1.0254e-02,  ..., -5.2185e-03,
         -1.1719e-02, -1.2024e-02],
        [-2.7771e-03,  1.2024e-02,  1.1047e-02,  ..., -1.0315e-02,
          7.4387e-05, -5.1270e-03],
        ...,
        [ 1.4648e-02, -7.4463e-03,  2.5749e-04,  ...,  1.1963e-02,
         -6.8665e-03,  1.7548e-03],
        [-1.0376e-02,  6.9885e-03,  1.1597e-03,  ...,  7.0953e-04,
          1.8311e-04, -6.4087e-03],
        [ 1.3306e-02,  1.8845e-03, -7.9956e-03,  ..., -6.0425e-03,
          1.9989e-03, -6.9580e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.22.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[ 68],
            [195],
            [ 68],
            ...,
            [102],
            [117],
            [187]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0029, -0.0081,  0.0072,  ...,  0.0033,  0.0110,  0.0009],
        [-0.0145,  0.0010,  0.0064,  ...,  0.0132,  0.0156, -0.0107],
        [ 0.0023, -0.0060, -0.0065,  ...,  0.0148,  0.0099, -0.0112],
        ...,
        [ 0.0099,  0.0081, -0.0065,  ..., -0.0003, -0.0085, -0.0027],
        [ 0.0093, -0.0114,  0.0072,  ..., -0.0111,  0.0135,  0.0102],
        [-0.0067, -0.0084, -0.0063,  ...,  0.0027, -0.0037, -0.0129]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.22.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[165],
            [169],
            [ 60],
            ...,
            [217],
            [ 54],
            [ 41]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0052,  0.0094,  0.0070,  ...,  0.0062,  0.0035, -0.0048],
        [-0.0003,  0.0033,  0.0075,  ...,  0.0079,  0.0036, -0.0054],
        [-0.0001, -0.0081,  0.0076,  ..., -0.0084,  0.0076, -0.0044],
        ...,
        [ 0.0066,  0.0060,  0.0004,  ...,  0.0074,  0.0015, -0.0076],
        [-0.0025, -0.0004,  0.0054,  ..., -0.0051, -0.0021,  0.0062],
        [ 0.0082, -0.0021, -0.0008,  ..., -0.0028,  0.0082,  0.0027]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.22.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[237],
            [132],
            [222],
            ...,
            [ 83],
            [ 49],
            [ 10]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0070,  0.0095,  0.0110,  ...,  0.0130,  0.0064,  0.0099],
        [-0.0060, -0.0155, -0.0148,  ...,  0.0123, -0.0140, -0.0051],
        [ 0.0133, -0.0097,  0.0009,  ...,  0.0102,  0.0054, -0.0085],
        ...,
        [ 0.0006, -0.0119, -0.0131,  ..., -0.0033,  0.0134, -0.0039],
        [ 0.0114,  0.0077,  0.0086,  ..., -0.0032,  0.0098, -0.0125],
        [ 0.0010, -0.0023, -0.0094,  ...,  0.0096, -0.0047,  0.0055]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.22.input_layernorm.weight Parameter containing:
tensor([0.4863, 0.4863, 0.4746,  ..., 0.4688, 0.4863, 0.4863], device='cuda:0')
base_model.model.model.layers.22.post_attention_layernorm.weight Parameter containing:
tensor([0.3848, 0.3809, 0.3828,  ..., 0.3926, 0.3867, 0.3887], device='cuda:0')
base_model.model.model.layers.23.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[131],
            [126],
            [172],
            ...,
            [165],
            [ 89],
            [214]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0104,  0.0079, -0.0146,  ..., -0.0054,  0.0134,  0.0040],
        [-0.0031,  0.0074,  0.0034,  ..., -0.0139,  0.0113,  0.0151],
        [-0.0103, -0.0101, -0.0103,  ...,  0.0044, -0.0036, -0.0046],
        ...,
        [ 0.0043, -0.0087, -0.0092,  ..., -0.0116, -0.0045,  0.0119],
        [ 0.0055,  0.0020, -0.0082,  ...,  0.0086, -0.0104, -0.0066],
        [ 0.0033, -0.0144, -0.0120,  ..., -0.0093,  0.0007,  0.0125]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.23.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[106],
            [154],
            [157],
            ...,
            [192],
            [232],
            [117]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0135, -0.0085, -0.0091,  ..., -0.0150, -0.0122,  0.0014],
        [ 0.0069, -0.0072,  0.0070,  ..., -0.0028, -0.0115, -0.0093],
        [ 0.0123,  0.0050, -0.0003,  ...,  0.0086, -0.0044, -0.0109],
        ...,
        [ 0.0096, -0.0128,  0.0139,  ..., -0.0087, -0.0034, -0.0123],
        [-0.0044,  0.0021, -0.0058,  ..., -0.0126,  0.0134,  0.0012],
        [-0.0129,  0.0032, -0.0027,  ..., -0.0103,  0.0034, -0.0023]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.23.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[141],
            [103],
            [138],
            ...,
            [150],
            [ 26],
            [145]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0076, -0.0146, -0.0013,  ...,  0.0050,  0.0031, -0.0004],
        [ 0.0147,  0.0003, -0.0034,  ...,  0.0089,  0.0099,  0.0032],
        [-0.0142, -0.0055,  0.0097,  ..., -0.0030,  0.0151,  0.0066],
        ...,
        [-0.0106, -0.0029,  0.0027,  ..., -0.0096,  0.0141, -0.0075],
        [ 0.0103,  0.0110, -0.0044,  ..., -0.0051,  0.0070,  0.0112],
        [-0.0109,  0.0154, -0.0096,  ..., -0.0110, -0.0056, -0.0093]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.23.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[102],
            [ 73],
            [144],
            ...,
            [148],
            [134],
            [115]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0048, -0.0154,  0.0084,  ..., -0.0092, -0.0093, -0.0128],
        [ 0.0148,  0.0029,  0.0090,  ..., -0.0006, -0.0015, -0.0083],
        [ 0.0136,  0.0071,  0.0122,  ..., -0.0020, -0.0119,  0.0043],
        ...,
        [-0.0058, -0.0129, -0.0151,  ..., -0.0080,  0.0068,  0.0145],
        [-0.0062,  0.0034,  0.0019,  ..., -0.0022, -0.0132, -0.0042],
        [-0.0098,  0.0010,  0.0154,  ..., -0.0095, -0.0041,  0.0018]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.23.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[ 93],
            [116],
            [120],
            ...,
            [106],
            [ 86],
            [134]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0017, -0.0132, -0.0012,  ...,  0.0135, -0.0007,  0.0115],
        [ 0.0128, -0.0146, -0.0043,  ...,  0.0071, -0.0081, -0.0001],
        [ 0.0120, -0.0048, -0.0057,  ..., -0.0069, -0.0094, -0.0013],
        ...,
        [-0.0126,  0.0110,  0.0029,  ..., -0.0046, -0.0032,  0.0122],
        [ 0.0059, -0.0013,  0.0112,  ..., -0.0036,  0.0085,  0.0065],
        [ 0.0022,  0.0140,  0.0127,  ...,  0.0062, -0.0053,  0.0056]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.23.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[122],
            [ 45],
            [ 89],
            ...,
            [102],
            [ 37],
            [181]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0074, -0.0011, -0.0054,  ...,  0.0027, -0.0001,  0.0075],
        [ 0.0035, -0.0054,  0.0002,  ..., -0.0008, -0.0060,  0.0011],
        [ 0.0085, -0.0030,  0.0007,  ..., -0.0087, -0.0084,  0.0079],
        ...,
        [ 0.0092,  0.0003,  0.0034,  ..., -0.0017,  0.0056,  0.0017],
        [ 0.0066,  0.0032, -0.0064,  ...,  0.0023, -0.0011, -0.0052],
        [-0.0071, -0.0026, -0.0064,  ...,  0.0026, -0.0021, -0.0029]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.23.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[218],
            [211],
            [101],
            ...,
            [214],
            [ 23],
            [ 60]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0073,  0.0055, -0.0123,  ...,  0.0144, -0.0143,  0.0125],
        [ 0.0053, -0.0121, -0.0025,  ...,  0.0042,  0.0011, -0.0152],
        [-0.0076, -0.0148, -0.0061,  ..., -0.0121, -0.0039,  0.0126],
        ...,
        [-0.0056, -0.0119, -0.0122,  ...,  0.0066,  0.0053, -0.0093],
        [ 0.0113, -0.0049,  0.0098,  ..., -0.0154, -0.0045,  0.0141],
        [-0.0053, -0.0089, -0.0062,  ..., -0.0021,  0.0005, -0.0055]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.23.input_layernorm.weight Parameter containing:
tensor([0.5117, 0.5234, 0.5078,  ..., 0.5039, 0.5195, 0.5234], device='cuda:0')
base_model.model.model.layers.23.post_attention_layernorm.weight Parameter containing:
tensor([0.4004, 0.3926, 0.3945,  ..., 0.3984, 0.4004, 0.4023], device='cuda:0')
base_model.model.model.layers.24.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[180],
            [153],
            [ 37],
            ...,
            [154],
            [101],
            [207]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0050,  0.0063, -0.0134,  ..., -0.0131, -0.0074,  0.0049],
        [-0.0063, -0.0134,  0.0068,  ...,  0.0127,  0.0099, -0.0109],
        [-0.0011,  0.0098,  0.0080,  ...,  0.0145,  0.0136, -0.0096],
        ...,
        [ 0.0097, -0.0148, -0.0101,  ..., -0.0014,  0.0096,  0.0047],
        [ 0.0016, -0.0151,  0.0067,  ..., -0.0087, -0.0085,  0.0125],
        [-0.0015, -0.0071,  0.0092,  ..., -0.0010, -0.0028,  0.0120]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.24.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[199],
            [ 88],
            [ 52],
            ...,
            [180],
            [184],
            [111]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0008,  0.0035,  0.0012,  ...,  0.0076, -0.0131,  0.0084],
        [ 0.0069, -0.0079,  0.0119,  ...,  0.0093, -0.0055,  0.0153],
        [ 0.0062,  0.0005, -0.0024,  ...,  0.0063,  0.0026, -0.0008],
        ...,
        [-0.0034, -0.0018,  0.0027,  ..., -0.0056, -0.0142, -0.0115],
        [-0.0068,  0.0038, -0.0091,  ...,  0.0046,  0.0151, -0.0005],
        [-0.0093, -0.0063, -0.0141,  ...,  0.0051,  0.0078,  0.0016]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.24.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[211],
            [103],
            [ 34],
            ...,
            [231],
            [123],
            [ 69]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0092, -0.0031,  0.0058,  ...,  0.0058, -0.0037, -0.0042],
        [-0.0016,  0.0071, -0.0070,  ..., -0.0026, -0.0154, -0.0057],
        [-0.0135,  0.0012, -0.0121,  ..., -0.0036, -0.0011, -0.0095],
        ...,
        [ 0.0002, -0.0086, -0.0044,  ..., -0.0146,  0.0142,  0.0016],
        [ 0.0133,  0.0151,  0.0062,  ..., -0.0002,  0.0122,  0.0071],
        [-0.0095,  0.0140,  0.0089,  ..., -0.0145,  0.0096, -0.0150]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.24.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[ 91],
            [209],
            [174],
            ...,
            [136],
            [120],
            [ 58]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0074, -0.0035, -0.0125,  ..., -0.0030,  0.0030, -0.0041],
        [ 0.0058, -0.0089, -0.0145,  ...,  0.0137,  0.0051, -0.0051],
        [-0.0054,  0.0149, -0.0024,  ...,  0.0113,  0.0030, -0.0083],
        ...,
        [ 0.0070,  0.0105, -0.0147,  ...,  0.0092,  0.0139, -0.0088],
        [ 0.0107,  0.0050, -0.0017,  ..., -0.0042,  0.0043, -0.0038],
        [ 0.0047, -0.0057,  0.0011,  ...,  0.0050, -0.0136,  0.0033]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.24.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[ 36],
            [165],
            [119],
            ...,
            [ 75],
            [174],
            [ 64]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0062, -0.0043,  0.0092,  ..., -0.0016,  0.0048, -0.0107],
        [-0.0025,  0.0017,  0.0133,  ...,  0.0061, -0.0059,  0.0074],
        [-0.0045,  0.0001,  0.0117,  ..., -0.0037,  0.0036,  0.0095],
        ...,
        [-0.0010, -0.0067, -0.0104,  ...,  0.0127,  0.0024, -0.0015],
        [-0.0044, -0.0125, -0.0120,  ...,  0.0096,  0.0092, -0.0154],
        [ 0.0002,  0.0081,  0.0139,  ..., -0.0025,  0.0121, -0.0024]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.24.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[101],
            [ 44],
            [124],
            ...,
            [143],
            [106],
            [ 81]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0029, -0.0024,  0.0057,  ..., -0.0067,  0.0034, -0.0027],
        [ 0.0040,  0.0024,  0.0082,  ..., -0.0034, -0.0089,  0.0054],
        [-0.0078,  0.0045,  0.0005,  ..., -0.0055,  0.0041, -0.0058],
        ...,
        [ 0.0026, -0.0032, -0.0043,  ..., -0.0018,  0.0093,  0.0078],
        [ 0.0025, -0.0045, -0.0011,  ..., -0.0093, -0.0046,  0.0074],
        [-0.0072,  0.0035,  0.0022,  ...,  0.0088,  0.0085, -0.0045]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.24.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[135],
            [199],
            [ 94],
            ...,
            [ 51],
            [140],
            [219]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 2.8849e-05, -1.4420e-03, -1.2573e-02,  ..., -1.0376e-02,
          7.0190e-03,  3.7079e-03],
        [ 1.0620e-02, -8.1177e-03,  1.5259e-02,  ..., -9.5215e-03,
          1.2268e-02, -1.5320e-02],
        [-5.0964e-03, -6.0425e-03, -1.9150e-03,  ..., -4.5471e-03,
          1.2390e-02,  7.4768e-03],
        ...,
        [ 6.2256e-03,  6.3171e-03, -9.6436e-03,  ..., -2.0599e-03,
         -7.2021e-03,  3.8757e-03],
        [ 1.4526e-02,  3.4180e-03,  2.9907e-03,  ..., -1.3809e-03,
         -1.0193e-02,  7.9346e-03],
        [ 1.5259e-02, -8.9722e-03,  1.1902e-02,  ..., -1.2756e-02,
          1.2512e-02, -1.5137e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.24.input_layernorm.weight Parameter containing:
tensor([0.4941, 0.5195, 0.5117,  ..., 0.4863, 0.5156, 0.5078], device='cuda:0')
base_model.model.model.layers.24.post_attention_layernorm.weight Parameter containing:
tensor([0.4102, 0.4062, 0.4082,  ..., 0.4121, 0.4160, 0.4102], device='cuda:0')
base_model.model.model.layers.25.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[146],
            [ 20],
            [210],
            ...,
            [179],
            [135],
            [163]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 7.0572e-04, -3.6163e-03, -1.3000e-02,  ..., -1.3550e-02,
         -9.5825e-03, -5.6458e-03],
        [-4.2114e-03,  4.8828e-03, -1.5137e-02,  ..., -5.7068e-03,
         -1.0254e-02, -1.1353e-02],
        [-4.0588e-03,  3.6163e-03,  1.1414e-02,  ..., -5.0049e-03,
          8.7891e-03, -1.4832e-02],
        ...,
        [-3.2654e-03, -4.9744e-03, -1.2634e-02,  ...,  1.1658e-02,
          1.3428e-02,  1.2695e-02],
        [ 7.4463e-03,  1.2451e-02,  1.4160e-02,  ...,  1.3245e-02,
         -3.8757e-03, -2.5635e-03],
        [ 3.4790e-03, -1.4099e-02,  1.5320e-02,  ...,  1.9073e-06,
         -5.6982e-05,  1.3428e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.25.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 67],
            [ 85],
            [171],
            ...,
            [176],
            [ 76],
            [132]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0011,  0.0013,  0.0046,  ..., -0.0032, -0.0086,  0.0005],
        [-0.0042,  0.0056,  0.0131,  ..., -0.0091,  0.0132,  0.0114],
        [-0.0020, -0.0146, -0.0061,  ...,  0.0063,  0.0071, -0.0082],
        ...,
        [ 0.0133, -0.0025, -0.0092,  ...,  0.0103,  0.0081,  0.0033],
        [-0.0027,  0.0032,  0.0007,  ..., -0.0048,  0.0081,  0.0020],
        [-0.0101, -0.0117, -0.0060,  ...,  0.0062, -0.0135, -0.0007]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.25.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[154],
            [150],
            [117],
            ...,
            [200],
            [ 23],
            [188]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0005, -0.0009,  0.0056,  ...,  0.0109,  0.0067, -0.0020],
        [ 0.0078, -0.0007, -0.0021,  ...,  0.0108, -0.0148, -0.0114],
        [ 0.0037,  0.0152, -0.0151,  ...,  0.0127, -0.0107, -0.0058],
        ...,
        [ 0.0142, -0.0055,  0.0109,  ..., -0.0034,  0.0007, -0.0123],
        [-0.0090, -0.0045,  0.0014,  ..., -0.0128, -0.0018, -0.0049],
        [ 0.0034, -0.0005, -0.0122,  ...,  0.0107, -0.0007,  0.0155]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.25.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[170],
            [117],
            [104],
            ...,
            [ 29],
            [225],
            [ 52]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-1.0498e-02,  6.0730e-03,  8.7891e-03,  ...,  6.2256e-03,
         -8.9722e-03, -9.7752e-05],
        [-4.8447e-04,  8.1787e-03, -8.4839e-03,  ...,  8.6060e-03,
         -1.4771e-02, -1.0315e-02],
        [-2.3346e-03,  1.1536e-02, -3.3875e-03,  ..., -7.8735e-03,
          1.4404e-02, -1.1169e-02],
        ...,
        [ 1.4221e-02, -8.6670e-03, -8.9111e-03,  ..., -3.5553e-03,
          6.4087e-04,  3.2654e-03],
        [-1.1978e-03, -2.1973e-03,  8.7280e-03,  ..., -8.3618e-03,
         -1.5442e-02,  1.2817e-02],
        [ 7.6294e-04, -1.3733e-02,  1.4587e-02,  ..., -6.6833e-03,
         -1.4038e-02,  5.9509e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.25.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[112],
            [241],
            [177],
            ...,
            [180],
            [174],
            [134]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0031, -0.0091, -0.0090,  ...,  0.0065,  0.0112,  0.0086],
        [-0.0074, -0.0037, -0.0003,  ...,  0.0039,  0.0022,  0.0029],
        [ 0.0069,  0.0116,  0.0078,  ..., -0.0108, -0.0119, -0.0041],
        ...,
        [ 0.0010,  0.0024, -0.0106,  ...,  0.0150, -0.0092,  0.0060],
        [-0.0051, -0.0005, -0.0081,  ..., -0.0147,  0.0037, -0.0039],
        [-0.0044, -0.0138, -0.0142,  ...,  0.0126,  0.0097,  0.0118]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.25.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[229],
            [162],
            [138],
            ...,
            [219],
            [ 97],
            [102]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0073, -0.0069, -0.0074,  ..., -0.0007, -0.0045, -0.0056],
        [-0.0008, -0.0011, -0.0030,  ..., -0.0029, -0.0007,  0.0022],
        [-0.0043, -0.0075, -0.0087,  ...,  0.0028,  0.0064, -0.0033],
        ...,
        [ 0.0074,  0.0080,  0.0015,  ...,  0.0063,  0.0004,  0.0038],
        [ 0.0004, -0.0075, -0.0045,  ...,  0.0063,  0.0033, -0.0044],
        [-0.0081, -0.0077,  0.0019,  ...,  0.0093,  0.0074, -0.0079]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.25.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 38],
            [172],
            [176],
            ...,
            [185],
            [125],
            [157]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0056,  0.0148, -0.0139,  ...,  0.0108,  0.0147, -0.0124],
        [-0.0001,  0.0057,  0.0022,  ...,  0.0121,  0.0145,  0.0075],
        [ 0.0085, -0.0101, -0.0053,  ...,  0.0016, -0.0125, -0.0151],
        ...,
        [-0.0045, -0.0098, -0.0106,  ..., -0.0134, -0.0051,  0.0129],
        [ 0.0029, -0.0037, -0.0065,  ...,  0.0103, -0.0155,  0.0094],
        [ 0.0017, -0.0053,  0.0035,  ...,  0.0028, -0.0087,  0.0025]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.25.input_layernorm.weight Parameter containing:
tensor([0.5469, 0.5547, 0.5430,  ..., 0.5508, 0.5625, 0.5508], device='cuda:0')
base_model.model.model.layers.25.post_attention_layernorm.weight Parameter containing:
tensor([0.4180, 0.4160, 0.4199,  ..., 0.4277, 0.4238, 0.4238], device='cuda:0')
base_model.model.model.layers.26.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[230],
            [ 84],
            [180],
            ...,
            [167],
            [ 37],
            [ 17]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 8.4229e-03, -4.2725e-03, -7.0496e-03,  ..., -1.0742e-02,
         -3.1433e-03,  4.0894e-03],
        [-3.4790e-03, -1.4221e-02,  1.1414e-02,  ...,  4.2114e-03,
          7.2021e-03, -1.5335e-03],
        [ 1.3611e-02,  6.8188e-05,  4.2114e-03,  ...,  1.5793e-03,
          1.0620e-02,  6.3705e-04],
        ...,
        [ 1.0254e-02, -1.1719e-02,  6.4697e-03,  ..., -1.4648e-02,
          5.0354e-03,  1.0620e-02],
        [-9.1934e-04, -3.5553e-03,  2.9144e-03,  ..., -1.0559e-02,
          7.2632e-03,  1.1292e-02],
        [ 2.0905e-03, -1.2695e-02,  5.0354e-03,  ...,  1.0315e-02,
          5.5847e-03,  1.3428e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.26.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[203],
            [ 70],
            [164],
            ...,
            [151],
            [ 23],
            [ 85]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0153, -0.0092, -0.0044,  ...,  0.0156,  0.0125, -0.0076],
        [ 0.0120, -0.0018,  0.0008,  ...,  0.0116, -0.0052, -0.0048],
        [ 0.0131,  0.0078,  0.0134,  ..., -0.0091,  0.0151,  0.0126],
        ...,
        [ 0.0084,  0.0044,  0.0072,  ..., -0.0104, -0.0024, -0.0066],
        [-0.0127, -0.0154,  0.0031,  ..., -0.0123,  0.0112,  0.0033],
        [ 0.0153, -0.0079, -0.0104,  ..., -0.0089,  0.0040,  0.0095]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.26.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[243],
            [ 54],
            [180],
            ...,
            [ 77],
            [105],
            [124]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0134,  0.0040,  0.0081,  ..., -0.0045, -0.0090,  0.0134],
        [ 0.0127,  0.0026, -0.0106,  ...,  0.0064, -0.0072, -0.0012],
        [-0.0053, -0.0051,  0.0111,  ..., -0.0108,  0.0152, -0.0131],
        ...,
        [-0.0027, -0.0008,  0.0150,  ..., -0.0098, -0.0056, -0.0044],
        [ 0.0015,  0.0007, -0.0104,  ..., -0.0064, -0.0034,  0.0091],
        [ 0.0113, -0.0143,  0.0103,  ...,  0.0063, -0.0011,  0.0149]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.26.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[168],
            [ 37],
            [ 77],
            ...,
            [ 40],
            [210],
            [195]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0148,  0.0072, -0.0016,  ...,  0.0131,  0.0120, -0.0078],
        [-0.0069, -0.0112,  0.0121,  ...,  0.0077,  0.0104, -0.0039],
        [ 0.0117, -0.0106, -0.0154,  ...,  0.0006, -0.0076,  0.0070],
        ...,
        [-0.0083, -0.0057,  0.0003,  ..., -0.0099,  0.0126,  0.0112],
        [ 0.0056, -0.0094, -0.0015,  ..., -0.0013,  0.0133, -0.0034],
        [ 0.0019, -0.0091, -0.0124,  ..., -0.0005,  0.0034,  0.0056]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.26.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[165],
            [167],
            [ 27],
            ...,
            [ 75],
            [130],
            [102]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-6.1035e-05,  7.2632e-03, -1.0986e-02,  ..., -1.1110e-04,
          7.5912e-04, -1.3062e-02],
        [-1.4526e-02,  1.9836e-03, -1.4160e-02,  ..., -5.5847e-03,
          6.2866e-03,  7.1106e-03],
        [-4.6921e-04,  8.5449e-03, -5.9509e-03,  ...,  2.0752e-03,
          4.2419e-03,  1.1536e-02],
        ...,
        [ 1.5625e-02, -7.5073e-03,  3.9368e-03,  ..., -7.6294e-03,
         -8.3008e-03,  1.5625e-02],
        [-1.1536e-02, -1.0742e-02,  1.3275e-03,  ..., -1.2573e-02,
         -3.0060e-03, -1.0529e-03],
        [-9.1553e-03, -1.4465e-02,  1.3123e-02,  ..., -1.0620e-02,
         -1.2024e-02, -1.0193e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.26.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[ 68],
            [ 72],
            [141],
            ...,
            [ 29],
            [219],
            [203]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0012,  0.0040, -0.0074,  ...,  0.0001, -0.0060, -0.0019],
        [-0.0044,  0.0035, -0.0061,  ..., -0.0095, -0.0026, -0.0019],
        [ 0.0062, -0.0008,  0.0095,  ...,  0.0052, -0.0021,  0.0020],
        ...,
        [-0.0043,  0.0071,  0.0028,  ...,  0.0087,  0.0077,  0.0007],
        [-0.0034,  0.0007, -0.0084,  ..., -0.0075,  0.0065, -0.0050],
        [ 0.0043,  0.0056,  0.0026,  ...,  0.0075,  0.0060,  0.0070]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.26.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 78],
            [121],
            [156],
            ...,
            [ 89],
            [ 52],
            [166]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0117, -0.0016,  0.0025,  ..., -0.0055, -0.0121,  0.0010],
        [-0.0077,  0.0114,  0.0117,  ...,  0.0131,  0.0118,  0.0135],
        [-0.0155, -0.0106, -0.0070,  ...,  0.0044,  0.0078, -0.0106],
        ...,
        [-0.0150,  0.0005,  0.0079,  ..., -0.0046,  0.0012,  0.0154],
        [ 0.0104, -0.0048, -0.0118,  ...,  0.0009, -0.0081,  0.0119],
        [ 0.0112,  0.0132,  0.0101,  ..., -0.0023,  0.0156,  0.0045]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.26.input_layernorm.weight Parameter containing:
tensor([0.5156, 0.5352, 0.5352,  ..., 0.5195, 0.5430, 0.5312], device='cuda:0')
base_model.model.model.layers.26.post_attention_layernorm.weight Parameter containing:
tensor([0.4355, 0.4316, 0.4336,  ..., 0.4434, 0.4414, 0.4395], device='cuda:0')
base_model.model.model.layers.27.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 43],
            [153],
            [ 19],
            ...,
            [ 23],
            [ 76],
            [181]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0067, -0.0137, -0.0099,  ..., -0.0072,  0.0014, -0.0022],
        [ 0.0016,  0.0042,  0.0119,  ...,  0.0143,  0.0137,  0.0038],
        [-0.0032, -0.0085,  0.0114,  ..., -0.0084, -0.0074, -0.0029],
        ...,
        [ 0.0156, -0.0138,  0.0026,  ...,  0.0128, -0.0041,  0.0075],
        [ 0.0141,  0.0095,  0.0086,  ...,  0.0053, -0.0135,  0.0032],
        [-0.0083,  0.0039, -0.0050,  ...,  0.0145, -0.0123,  0.0120]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.27.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[102],
            [154],
            [  4],
            ...,
            [220],
            [131],
            [ 41]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0156,  0.0147, -0.0116,  ...,  0.0002,  0.0121,  0.0154],
        [ 0.0063,  0.0041,  0.0146,  ...,  0.0014, -0.0067, -0.0051],
        [-0.0111,  0.0123, -0.0092,  ..., -0.0020,  0.0084,  0.0079],
        ...,
        [-0.0076,  0.0024,  0.0112,  ...,  0.0026, -0.0062, -0.0093],
        [ 0.0075, -0.0121, -0.0027,  ...,  0.0149,  0.0135, -0.0062],
        [-0.0085, -0.0054, -0.0050,  ..., -0.0116,  0.0052,  0.0081]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.27.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[182],
            [ 22],
            [ 45],
            ...,
            [ 72],
            [ 83],
            [ 59]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0121, -0.0141, -0.0035,  ...,  0.0023, -0.0064,  0.0081],
        [ 0.0023,  0.0100, -0.0051,  ..., -0.0031, -0.0152,  0.0092],
        [-0.0010,  0.0079, -0.0079,  ..., -0.0033,  0.0045,  0.0099],
        ...,
        [-0.0141, -0.0063,  0.0026,  ...,  0.0052,  0.0146,  0.0059],
        [-0.0047,  0.0026, -0.0023,  ..., -0.0077, -0.0003,  0.0062],
        [ 0.0006,  0.0108, -0.0043,  ...,  0.0024,  0.0128, -0.0099]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.27.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[133],
            [146],
            [ 84],
            ...,
            [137],
            [117],
            [131]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0030, -0.0011, -0.0116,  ...,  0.0077,  0.0034, -0.0079],
        [ 0.0121,  0.0059,  0.0017,  ..., -0.0059, -0.0107,  0.0029],
        [-0.0113, -0.0071,  0.0003,  ..., -0.0017,  0.0002,  0.0045],
        ...,
        [ 0.0047,  0.0087,  0.0044,  ...,  0.0138,  0.0093,  0.0103],
        [ 0.0099, -0.0145, -0.0141,  ...,  0.0082,  0.0035, -0.0006],
        [ 0.0069,  0.0024,  0.0135,  ...,  0.0052, -0.0034, -0.0087]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.27.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[235],
            [ 83],
            [140],
            ...,
            [200],
            [165],
            [ 71]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0020,  0.0049,  0.0098,  ..., -0.0051,  0.0104,  0.0043],
        [-0.0107,  0.0036, -0.0096,  ..., -0.0029,  0.0003,  0.0127],
        [ 0.0002, -0.0056, -0.0153,  ..., -0.0029,  0.0112,  0.0153],
        ...,
        [-0.0123,  0.0013, -0.0090,  ...,  0.0114,  0.0140,  0.0016],
        [ 0.0074,  0.0027, -0.0072,  ..., -0.0086,  0.0098, -0.0089],
        [-0.0009, -0.0020, -0.0039,  ..., -0.0146,  0.0105, -0.0153]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.27.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[162],
            [224],
            [161],
            ...,
            [147],
            [ 93],
            [ 88]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 2.5330e-03, -4.3869e-04, -7.0496e-03,  ..., -1.5564e-03,
          3.0975e-03, -7.8125e-03],
        [-2.5482e-03,  7.8735e-03,  7.1716e-03,  ...,  2.9144e-03,
         -6.7749e-03, -9.2163e-03],
        [-3.9978e-03, -3.5858e-03,  2.0294e-03,  ..., -1.5030e-03,
         -6.4392e-03, -1.5945e-03],
        ...,
        [-3.2654e-03, -9.6893e-04, -6.8054e-03,  ...,  9.3994e-03,
          6.3782e-03, -8.9111e-03],
        [ 8.2397e-03,  8.9722e-03,  5.7373e-03,  ...,  9.4604e-03,
         -6.5002e-03, -8.6670e-03],
        [-1.4572e-03,  7.4463e-03, -1.6308e-04,  ...,  1.7166e-03,
          2.0752e-03, -4.3809e-06]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.27.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[107],
            [214],
            [ 38],
            ...,
            [ 35],
            [148],
            [ 39]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0085,  0.0121,  0.0030,  ..., -0.0079,  0.0048, -0.0025],
        [ 0.0014,  0.0008,  0.0025,  ..., -0.0007,  0.0117,  0.0063],
        [ 0.0104, -0.0077, -0.0059,  ..., -0.0152, -0.0109, -0.0080],
        ...,
        [-0.0049, -0.0132,  0.0060,  ..., -0.0067,  0.0148, -0.0070],
        [ 0.0153, -0.0117, -0.0110,  ..., -0.0016, -0.0145,  0.0141],
        [-0.0002,  0.0076, -0.0146,  ...,  0.0095, -0.0142, -0.0005]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.27.input_layernorm.weight Parameter containing:
tensor([0.5430, 0.5508, 0.5508,  ..., 0.5508, 0.5508, 0.5547], device='cuda:0')
base_model.model.model.layers.27.post_attention_layernorm.weight Parameter containing:
tensor([0.4551, 0.4453, 0.4434,  ..., 0.4512, 0.4551, 0.4492], device='cuda:0')
base_model.model.model.layers.28.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[114],
            [169],
            [ 52],
            ...,
            [ 39],
            [158],
            [162]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0117, -0.0074, -0.0145,  ..., -0.0060, -0.0044,  0.0052],
        [ 0.0138,  0.0015, -0.0123,  ..., -0.0052,  0.0137,  0.0009],
        [ 0.0016,  0.0125, -0.0063,  ..., -0.0078,  0.0098,  0.0023],
        ...,
        [-0.0076, -0.0043,  0.0091,  ...,  0.0121, -0.0101,  0.0111],
        [-0.0019,  0.0022, -0.0149,  ...,  0.0035, -0.0042,  0.0123],
        [-0.0137,  0.0126,  0.0055,  ..., -0.0132,  0.0064, -0.0101]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.28.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[119],
            [200],
            [219],
            ...,
            [124],
            [135],
            [ 69]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0015,  0.0007,  0.0116,  ...,  0.0135, -0.0110,  0.0136],
        [-0.0037,  0.0144, -0.0063,  ...,  0.0036, -0.0056,  0.0156],
        [ 0.0142,  0.0028,  0.0083,  ..., -0.0140,  0.0074, -0.0112],
        ...,
        [ 0.0127, -0.0140, -0.0127,  ...,  0.0129, -0.0101,  0.0108],
        [ 0.0042,  0.0134,  0.0075,  ..., -0.0136,  0.0131,  0.0055],
        [-0.0034, -0.0146, -0.0128,  ..., -0.0035, -0.0007, -0.0085]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.28.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[ 57],
            [205],
            [113],
            ...,
            [128],
            [ 29],
            [135]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0048,  0.0071,  0.0131,  ...,  0.0056, -0.0131, -0.0013],
        [ 0.0014,  0.0118, -0.0052,  ...,  0.0107, -0.0080,  0.0049],
        [-0.0110,  0.0073, -0.0020,  ..., -0.0022,  0.0073, -0.0013],
        ...,
        [ 0.0042, -0.0017, -0.0044,  ...,  0.0154, -0.0040,  0.0082],
        [-0.0018, -0.0066,  0.0128,  ..., -0.0067,  0.0019,  0.0070],
        [ 0.0082, -0.0141,  0.0015,  ..., -0.0020,  0.0120,  0.0089]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.28.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[155],
            [ 20],
            [ 77],
            ...,
            [102],
            [195],
            [120]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-1.0132e-02, -1.0864e-02, -5.5075e-05,  ...,  9.8877e-03,
         -5.8594e-03,  1.5030e-03],
        [ 1.1841e-02, -1.5076e-02,  1.2634e-02,  ..., -8.0566e-03,
          9.4604e-04, -1.3977e-02],
        [-2.6855e-03, -9.9487e-03, -2.3956e-03,  ..., -3.5248e-03,
         -1.1597e-02, -9.8877e-03],
        ...,
        [-1.0315e-02, -2.1362e-03, -6.1340e-03,  ...,  2.9182e-04,
         -8.4229e-03, -1.8692e-03],
        [ 3.3417e-03,  3.4180e-03,  1.4221e-02,  ..., -5.3101e-03,
         -6.0120e-03, -1.3611e-02],
        [-1.0559e-02,  8.5449e-03, -1.0254e-02,  ..., -1.1108e-02,
          6.5613e-03,  1.2512e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.28.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[120],
            [111],
            [ 98],
            ...,
            [165],
            [149],
            [ 75]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0139,  0.0127, -0.0048,  ..., -0.0077, -0.0013,  0.0107],
        [ 0.0043, -0.0127, -0.0033,  ...,  0.0118, -0.0137, -0.0020],
        [-0.0058,  0.0024,  0.0067,  ..., -0.0007, -0.0072,  0.0148],
        ...,
        [-0.0093,  0.0049,  0.0050,  ...,  0.0069,  0.0078, -0.0087],
        [ 0.0022, -0.0015,  0.0067,  ..., -0.0031, -0.0048, -0.0057],
        [-0.0033, -0.0092,  0.0050,  ...,  0.0148, -0.0101, -0.0073]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.28.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[158],
            [ 91],
            [136],
            ...,
            [149],
            [108],
            [228]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 8.3618e-03,  4.3030e-03,  6.4087e-03,  ..., -8.4229e-03,
          7.6294e-03,  5.0049e-03],
        [-4.7302e-03,  3.3264e-03, -6.2256e-03,  ...,  8.3008e-03,
         -9.2163e-03, -7.0801e-03],
        [ 3.7994e-03,  2.9755e-03, -9.5215e-03,  ...,  2.5749e-04,
          7.3853e-03,  2.9449e-03],
        ...,
        [-7.1106e-03,  2.1820e-03,  6.8359e-03,  ...,  4.2114e-03,
         -8.5449e-03, -1.6479e-03],
        [ 2.2888e-05, -7.4463e-03,  6.1646e-03,  ...,  3.8605e-03,
         -5.5542e-03,  1.4267e-03],
        [-3.4332e-03,  7.7209e-03, -1.8616e-03,  ..., -8.6060e-03,
          2.7847e-04,  7.8735e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.28.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[216],
            [231],
            [ 46],
            ...,
            [130],
            [229],
            [150]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0113,  0.0057,  0.0115,  ...,  0.0033, -0.0119,  0.0077],
        [-0.0040,  0.0142, -0.0147,  ..., -0.0027,  0.0019,  0.0054],
        [ 0.0037,  0.0028, -0.0148,  ...,  0.0071,  0.0028, -0.0024],
        ...,
        [ 0.0142,  0.0056, -0.0077,  ...,  0.0150, -0.0091,  0.0025],
        [ 0.0114,  0.0016, -0.0036,  ..., -0.0084, -0.0084, -0.0060],
        [ 0.0127, -0.0029, -0.0153,  ..., -0.0035,  0.0060,  0.0054]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.28.input_layernorm.weight Parameter containing:
tensor([0.5625, 0.5664, 0.5586,  ..., 0.5469, 0.5664, 0.5586], device='cuda:0')
base_model.model.model.layers.28.post_attention_layernorm.weight Parameter containing:
tensor([0.4629, 0.4629, 0.4551,  ..., 0.4668, 0.4590, 0.4570], device='cuda:0')
base_model.model.model.layers.29.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[138],
            [ 87],
            [ 40],
            ...,
            [101],
            [202],
            [145]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0058, -0.0078, -0.0148,  ...,  0.0078, -0.0141,  0.0024],
        [-0.0117, -0.0134,  0.0034,  ...,  0.0110, -0.0125,  0.0047],
        [-0.0034,  0.0065,  0.0041,  ..., -0.0074,  0.0090,  0.0134],
        ...,
        [-0.0109,  0.0090,  0.0136,  ...,  0.0146,  0.0058, -0.0142],
        [ 0.0067, -0.0067,  0.0035,  ...,  0.0137,  0.0082,  0.0139],
        [ 0.0081, -0.0036,  0.0011,  ..., -0.0108, -0.0119,  0.0019]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.29.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[244],
            [ 59],
            [156],
            ...,
            [198],
            [ 48],
            [ 56]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0017, -0.0056,  0.0154,  ...,  0.0042, -0.0146, -0.0028],
        [-0.0135,  0.0115, -0.0071,  ...,  0.0112,  0.0100,  0.0019],
        [ 0.0124,  0.0078, -0.0070,  ..., -0.0070, -0.0103, -0.0044],
        ...,
        [-0.0058,  0.0132,  0.0069,  ...,  0.0150,  0.0085, -0.0123],
        [ 0.0021, -0.0036,  0.0003,  ...,  0.0128,  0.0054,  0.0125],
        [-0.0113,  0.0074,  0.0077,  ..., -0.0008,  0.0057,  0.0077]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.29.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[152],
            [197],
            [  3],
            ...,
            [154],
            [104],
            [ 76]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0085, -0.0028,  0.0056,  ...,  0.0007, -0.0079,  0.0120],
        [-0.0156,  0.0005,  0.0071,  ..., -0.0002,  0.0110, -0.0092],
        [ 0.0065, -0.0095, -0.0081,  ...,  0.0102,  0.0081,  0.0074],
        ...,
        [ 0.0114,  0.0108,  0.0006,  ..., -0.0050, -0.0029,  0.0139],
        [-0.0079, -0.0056,  0.0137,  ...,  0.0018, -0.0128, -0.0009],
        [-0.0060, -0.0098,  0.0087,  ..., -0.0066,  0.0077,  0.0097]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.29.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[110],
            [230],
            [ 91],
            ...,
            [ 86],
            [137],
            [178]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-5.3101e-03, -6.3477e-03,  3.3264e-03,  ..., -1.1108e-02,
          7.9346e-03,  2.6550e-03],
        [ 1.5793e-03, -1.0071e-02, -1.0010e-02,  ...,  7.9346e-03,
         -1.4221e-02, -3.5286e-04],
        [-7.9956e-03,  1.6785e-03, -6.4087e-03,  ...,  2.6093e-03,
          2.5940e-03,  1.3733e-02],
        ...,
        [-1.5411e-03, -1.8387e-03, -1.4587e-02,  ...,  1.4160e-02,
         -7.8125e-03,  4.0588e-03],
        [ 5.1575e-03,  9.8877e-03,  1.4709e-02,  ...,  1.0681e-02,
          9.5825e-03, -3.7994e-03],
        [-3.0823e-03,  7.9632e-05,  1.4404e-02,  ..., -1.0498e-02,
         -1.4893e-02, -5.4016e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.29.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[173],
            [ 72],
            [184],
            ...,
            [ 42],
            [ 75],
            [ 68]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0067, -0.0014,  0.0110,  ..., -0.0155,  0.0082,  0.0131],
        [-0.0056, -0.0130, -0.0089,  ...,  0.0098,  0.0054,  0.0005],
        [ 0.0118,  0.0115, -0.0073,  ..., -0.0067,  0.0070,  0.0078],
        ...,
        [-0.0019, -0.0052, -0.0024,  ..., -0.0063,  0.0140, -0.0109],
        [ 0.0029,  0.0006, -0.0096,  ..., -0.0126, -0.0026, -0.0114],
        [-0.0084, -0.0027, -0.0016,  ..., -0.0114,  0.0092,  0.0129]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.29.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[233],
            [115],
            [197],
            ...,
            [199],
            [164],
            [ 87]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0057,  0.0053,  0.0065,  ...,  0.0057,  0.0046,  0.0020],
        [ 0.0025,  0.0069, -0.0017,  ...,  0.0067, -0.0028, -0.0070],
        [-0.0009, -0.0028, -0.0045,  ...,  0.0058,  0.0067, -0.0035],
        ...,
        [ 0.0018, -0.0001,  0.0081,  ...,  0.0092,  0.0092,  0.0087],
        [-0.0050,  0.0031, -0.0060,  ...,  0.0085, -0.0012, -0.0043],
        [-0.0022,  0.0033, -0.0055,  ...,  0.0042, -0.0009, -0.0086]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.29.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[103],
            [146],
            [102],
            ...,
            [ 23],
            [ 70],
            [184]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0088, -0.0148, -0.0115,  ..., -0.0098, -0.0073, -0.0087],
        [ 0.0027, -0.0101,  0.0138,  ..., -0.0155, -0.0077, -0.0152],
        [-0.0014,  0.0060,  0.0020,  ..., -0.0088,  0.0123, -0.0053],
        ...,
        [-0.0135,  0.0024, -0.0018,  ...,  0.0107,  0.0013,  0.0003],
        [ 0.0125,  0.0063, -0.0072,  ...,  0.0035,  0.0084, -0.0090],
        [ 0.0044, -0.0047, -0.0150,  ..., -0.0004,  0.0134, -0.0039]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.29.input_layernorm.weight Parameter containing:
tensor([0.5273, 0.5391, 0.5312,  ..., 0.5273, 0.5352, 0.5547], device='cuda:0')
base_model.model.model.layers.29.post_attention_layernorm.weight Parameter containing:
tensor([0.4688, 0.4707, 0.4668,  ..., 0.4727, 0.4746, 0.4727], device='cuda:0')
base_model.model.model.layers.30.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[114],
            [119],
            [176],
            ...,
            [137],
            [ 70],
            [118]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0040,  0.0004, -0.0114,  ...,  0.0041, -0.0144,  0.0051],
        [ 0.0069,  0.0051, -0.0109,  ...,  0.0059, -0.0132, -0.0062],
        [-0.0071,  0.0032, -0.0117,  ...,  0.0045,  0.0121,  0.0017],
        ...,
        [-0.0141,  0.0147,  0.0024,  ..., -0.0136,  0.0032, -0.0132],
        [ 0.0009,  0.0032, -0.0009,  ...,  0.0118, -0.0128, -0.0106],
        [-0.0042, -0.0090, -0.0147,  ...,  0.0127,  0.0042,  0.0112]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.30.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[211],
            [116],
            [197],
            ...,
            [217],
            [184],
            [179]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0058, -0.0103, -0.0063,  ...,  0.0033,  0.0044, -0.0038],
        [-0.0052, -0.0089, -0.0089,  ..., -0.0037, -0.0154, -0.0148],
        [ 0.0034,  0.0013, -0.0092,  ..., -0.0136,  0.0134, -0.0094],
        ...,
        [ 0.0109, -0.0070, -0.0059,  ...,  0.0047,  0.0052, -0.0150],
        [ 0.0035, -0.0103, -0.0025,  ..., -0.0123,  0.0018,  0.0103],
        [ 0.0141, -0.0150,  0.0109,  ...,  0.0059, -0.0109, -0.0115]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.30.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[ 97],
            [205],
            [142],
            ...,
            [ 71],
            [ 85],
            [ 20]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0003,  0.0045, -0.0052,  ...,  0.0061, -0.0074, -0.0093],
        [ 0.0110, -0.0146, -0.0084,  ..., -0.0031,  0.0047,  0.0069],
        [-0.0046,  0.0024,  0.0086,  ...,  0.0121,  0.0034,  0.0139],
        ...,
        [-0.0132,  0.0144,  0.0072,  ..., -0.0092, -0.0009,  0.0011],
        [-0.0089,  0.0116,  0.0083,  ..., -0.0030,  0.0074, -0.0033],
        [ 0.0121, -0.0126, -0.0074,  ..., -0.0071,  0.0081, -0.0085]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.30.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[105],
            [ 84],
            [151],
            ...,
            [213],
            [183],
            [164]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0112, -0.0112,  0.0049,  ...,  0.0120,  0.0106,  0.0143],
        [ 0.0123,  0.0103, -0.0106,  ...,  0.0055,  0.0148, -0.0072],
        [ 0.0129,  0.0057,  0.0129,  ...,  0.0070, -0.0017, -0.0063],
        ...,
        [ 0.0130,  0.0014, -0.0060,  ...,  0.0073,  0.0013,  0.0118],
        [ 0.0009,  0.0124, -0.0086,  ...,  0.0137, -0.0030, -0.0114],
        [ 0.0033, -0.0061, -0.0010,  ..., -0.0055, -0.0009, -0.0138]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.30.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[ 17],
            [ 98],
            [105],
            ...,
            [177],
            [119],
            [162]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0027,  0.0060,  0.0007,  ..., -0.0074,  0.0038,  0.0044],
        [ 0.0148, -0.0016, -0.0085,  ..., -0.0078,  0.0010, -0.0137],
        [-0.0041,  0.0038, -0.0073,  ..., -0.0089,  0.0034,  0.0154],
        ...,
        [-0.0011,  0.0116, -0.0077,  ...,  0.0064, -0.0074, -0.0153],
        [-0.0078, -0.0116,  0.0010,  ...,  0.0047,  0.0079, -0.0123],
        [-0.0045, -0.0044,  0.0034,  ...,  0.0011, -0.0033, -0.0114]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.30.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[195],
            [ 30],
            [ 89],
            ...,
            [182],
            [153],
            [100]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0020,  0.0071, -0.0031,  ...,  0.0005, -0.0022, -0.0069],
        [ 0.0002,  0.0078, -0.0083,  ...,  0.0022, -0.0062,  0.0045],
        [-0.0003,  0.0095, -0.0075,  ..., -0.0077, -0.0005,  0.0045],
        ...,
        [ 0.0020, -0.0066, -0.0092,  ...,  0.0057,  0.0047, -0.0039],
        [-0.0051, -0.0058, -0.0095,  ..., -0.0045,  0.0033, -0.0087],
        [-0.0034, -0.0067,  0.0011,  ...,  0.0082,  0.0020,  0.0006]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.30.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[121],
            [149],
            [201],
            ...,
            [195],
            [132],
            [123]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-1.1414e-02,  1.2451e-02, -3.6011e-03,  ...,  1.5020e-05,
         -8.6060e-03, -4.4556e-03],
        [ 1.2085e-02,  1.4221e-02,  4.6692e-03,  ..., -6.3477e-03,
         -1.3000e-02,  6.5002e-03],
        [-1.2268e-02, -1.2512e-02, -3.9978e-03,  ...,  2.2430e-03,
          3.5248e-03, -1.0071e-02],
        ...,
        [-1.9741e-04,  1.0132e-02,  8.1177e-03,  ...,  4.0894e-03,
         -1.1902e-03,  3.0518e-03],
        [ 3.3951e-04,  8.1177e-03, -1.8311e-03,  ..., -1.0620e-02,
          6.6757e-04,  5.2795e-03],
        [ 2.5177e-03,  8.7891e-03,  6.1951e-03,  ...,  1.3245e-02,
          1.3611e-02,  3.8910e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.30.input_layernorm.weight Parameter containing:
tensor([0.5742, 0.5820, 0.5625,  ..., 0.5508, 0.5625, 0.5820], device='cuda:0')
base_model.model.model.layers.30.post_attention_layernorm.weight Parameter containing:
tensor([0.4785, 0.4883, 0.4785,  ..., 0.4805, 0.4824, 0.4785], device='cuda:0')
base_model.model.model.layers.31.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 12],
            [222],
            [168],
            ...,
            [180],
            [ 29],
            [156]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0091, -0.0104, -0.0061,  ...,  0.0060, -0.0083,  0.0074],
        [-0.0098, -0.0095, -0.0016,  ..., -0.0112,  0.0104,  0.0065],
        [ 0.0142,  0.0092,  0.0151,  ..., -0.0022, -0.0097,  0.0023],
        ...,
        [-0.0024,  0.0101,  0.0118,  ...,  0.0003, -0.0070,  0.0093],
        [ 0.0038, -0.0104,  0.0108,  ..., -0.0028,  0.0031,  0.0061],
        [-0.0147,  0.0010,  0.0003,  ..., -0.0114,  0.0034, -0.0086]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.31.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 33],
            [222],
            [182],
            ...,
            [148],
            [166],
            [120]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0069, -0.0080,  0.0128,  ...,  0.0085, -0.0079,  0.0117],
        [ 0.0007, -0.0103,  0.0118,  ...,  0.0027, -0.0009, -0.0041],
        [ 0.0131, -0.0135, -0.0025,  ...,  0.0015, -0.0036,  0.0021],
        ...,
        [-0.0081,  0.0029,  0.0115,  ...,  0.0130, -0.0032, -0.0055],
        [-0.0137, -0.0134, -0.0099,  ...,  0.0079,  0.0070,  0.0078],
        [-0.0118,  0.0120, -0.0092,  ..., -0.0076, -0.0105,  0.0128]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.31.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[177],
            [ 81],
            [ 83],
            ...,
            [115],
            [252],
            [111]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0112,  0.0063,  0.0116,  ..., -0.0095,  0.0024, -0.0008],
        [ 0.0043,  0.0019, -0.0057,  ..., -0.0126, -0.0115, -0.0125],
        [-0.0152, -0.0089,  0.0123,  ..., -0.0135,  0.0023, -0.0099],
        ...,
        [ 0.0016,  0.0001,  0.0065,  ..., -0.0009,  0.0089,  0.0108],
        [ 0.0059,  0.0148, -0.0111,  ...,  0.0106,  0.0021, -0.0114],
        [ 0.0153,  0.0093, -0.0030,  ..., -0.0045,  0.0133,  0.0114]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.31.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[157],
            [102],
            [116],
            ...,
            [ 67],
            [189],
            [ 56]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0153, -0.0129, -0.0034,  ...,  0.0119, -0.0091,  0.0118],
        [-0.0053,  0.0078, -0.0062,  ..., -0.0038,  0.0041, -0.0125],
        [ 0.0149,  0.0056, -0.0111,  ...,  0.0137,  0.0053, -0.0145],
        ...,
        [-0.0002,  0.0140,  0.0092,  ...,  0.0128,  0.0090, -0.0118],
        [ 0.0011,  0.0142, -0.0115,  ..., -0.0096, -0.0137, -0.0009],
        [-0.0038, -0.0105, -0.0061,  ...,  0.0061,  0.0074, -0.0016]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.31.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[132],
            [225],
            [124],
            ...,
            [188],
            [ 68],
            [168]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0035, -0.0109,  0.0071,  ...,  0.0011,  0.0060,  0.0070],
        [ 0.0044,  0.0062, -0.0059,  ..., -0.0116,  0.0049,  0.0131],
        [ 0.0107, -0.0064,  0.0079,  ..., -0.0068,  0.0025,  0.0052],
        ...,
        [ 0.0017, -0.0093, -0.0152,  ...,  0.0136,  0.0143,  0.0038],
        [-0.0036,  0.0066, -0.0012,  ..., -0.0053,  0.0040, -0.0039],
        [-0.0053, -0.0002,  0.0033,  ...,  0.0068,  0.0016,  0.0143]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.31.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[ 97],
            [ 83],
            [231],
            ...,
            [ 70],
            [ 56],
            [125]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0010,  0.0042, -0.0030,  ...,  0.0033,  0.0023, -0.0088],
        [-0.0005, -0.0084,  0.0012,  ..., -0.0092,  0.0028, -0.0050],
        [-0.0033,  0.0067,  0.0005,  ..., -0.0013,  0.0044,  0.0020],
        ...,
        [ 0.0068,  0.0092, -0.0053,  ..., -0.0058, -0.0079, -0.0093],
        [-0.0025, -0.0050,  0.0065,  ..., -0.0093,  0.0017,  0.0025],
        [-0.0042, -0.0087, -0.0020,  ...,  0.0090, -0.0074, -0.0015]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.31.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 20],
            [210],
            [149],
            ...,
            [ 86],
            [134],
            [139]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0070,  0.0007,  0.0072,  ..., -0.0044, -0.0017,  0.0143],
        [-0.0064,  0.0093,  0.0128,  ..., -0.0087,  0.0002, -0.0084],
        [ 0.0089, -0.0066, -0.0090,  ..., -0.0092, -0.0042,  0.0122],
        ...,
        [-0.0076,  0.0047, -0.0049,  ..., -0.0022,  0.0098, -0.0039],
        [-0.0010,  0.0095, -0.0088,  ..., -0.0116,  0.0026, -0.0146],
        [-0.0126,  0.0075,  0.0086,  ...,  0.0001, -0.0122, -0.0036]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.31.input_layernorm.weight Parameter containing:
tensor([0.4863, 0.4844, 0.4355,  ..., 0.4316, 0.4551, 0.4805], device='cuda:0')
base_model.model.model.layers.31.post_attention_layernorm.weight Parameter containing:
tensor([0.4336, 0.4375, 0.4414,  ..., 0.4238, 0.4102, 0.4277], device='cuda:0')
base_model.model.model.norm.weight Parameter containing:
tensor([1.8672, 1.8672, 1.8047,  ..., 1.7188, 1.8281, 1.6016], device='cuda:0')
base_model.model.lm_head.weight Parameter containing:
tensor([[-0.0039,  0.0032, -0.0071,  ...,  0.0053, -0.0082,  0.0070],
        [-0.0315,  0.0466, -0.0023,  ..., -0.0211,  0.0173,  0.0334],
        [-0.0125,  0.0036,  0.0195,  ..., -0.0271,  0.0143, -0.0082],
        ...,
        [ 0.0229,  0.0255,  0.0315,  ...,  0.0067, -0.0092, -0.0058],
        [ 0.0080, -0.0088,  0.0063,  ..., -0.0293, -0.0200,  0.0337],
        [-0.0007,  0.0006, -0.0011,  ...,  0.0015, -0.0033,  0.0028]],
       device='cuda:0', dtype=torch.bfloat16)
