nohup: ignoring input
Namespace(model_name_or_path='meta-llama/Llama-2-7b-hf', trust_remote_code=False, use_auth_token=True, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=1000, source_max_len=16, target_max_len=512, dataset='alpaca-clean', dataset_format=None, output_dir='./output/llama-2-alpaca-clean-7b', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=1875, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/llama-2-alpaca-clean-7b/runs/Jul24_04-50-21_d2', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=100, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=187, dataloader_num_workers=1, past_index=-1, run_name='llama2_alpaca-clean_7b', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=['wandb'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=True, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 32,
  "transformers_version": "4.30.0.dev0"
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=True, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, wandb_project='qlora-buffer', save_interval=1, save_dir='/shared/dqwang/scratch/tongchen/qlora/alpaca-clean', distributed_state=Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0))
Detected that training was already completed!
loading base model meta-llama/Llama-2-7b-hf...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.25s/it]
Found cached dataset json (/home/dqwang/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
Adding special tokens.
adding LoRA modules...
loaded model
base_model.model.model.embed_tokens.weight Parameter containing:
tensor([[ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,
         -6.5565e-06,  8.9407e-07],
        [ 1.8616e-03, -3.3722e-03,  3.9864e-04,  ..., -8.3008e-03,
          2.5787e-03, -3.9368e-03],
        [ 1.0986e-02,  9.8877e-03, -5.0964e-03,  ...,  2.5177e-03,
          7.7057e-04, -5.0049e-03],
        ...,
        [-1.0742e-02,  9.3384e-03,  1.2939e-02,  ..., -3.3203e-02,
         -1.6357e-02,  3.3875e-03],
        [-8.3008e-03, -4.0588e-03, -1.1063e-03,  ...,  3.4790e-03,
         -1.2939e-02,  3.1948e-05],
        [ 7.0953e-04,  6.8283e-04, -4.5013e-04,  ..., -4.2915e-04,
         -8.9645e-05, -4.7874e-04]], device='cuda:0', dtype=torch.bfloat16)
base_model.model.model.layers.0.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 83],
            [103],
            [ 74],
            ...,
            [114],
            [108],
            [181]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-1.0681e-02,  2.4605e-04, -2.6550e-03,  ...,  3.4027e-03,
          9.0408e-04, -4.3945e-03],
        [-2.8229e-03, -3.7994e-03,  6.4392e-03,  ...,  9.4604e-03,
         -3.1281e-03, -5.8746e-04],
        [-4.9744e-03,  4.1504e-03,  4.4556e-03,  ...,  1.2994e-05,
         -3.1281e-04,  1.7090e-03],
        ...,
        [ 1.6937e-03,  9.1553e-04,  1.3123e-02,  ...,  5.6458e-03,
         -1.3489e-02, -3.1662e-04],
        [ 6.1035e-03,  1.1169e-02,  3.8719e-04,  ...,  3.7384e-03,
         -7.6599e-03,  1.0132e-02],
        [ 5.1575e-03,  9.5215e-03, -2.9144e-03,  ..., -6.8970e-03,
          3.7231e-03, -1.4709e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.0.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 73],
            [120],
            [120],
            ...,
            [101],
            [104],
            [104]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0040, -0.0126, -0.0070,  ...,  0.0024,  0.0052,  0.0103],
        [ 0.0020, -0.0111,  0.0121,  ...,  0.0025, -0.0099,  0.0112],
        [ 0.0032, -0.0077, -0.0089,  ...,  0.0080, -0.0114,  0.0085],
        ...,
        [-0.0064,  0.0087, -0.0031,  ..., -0.0121, -0.0023, -0.0093],
        [ 0.0015, -0.0030,  0.0054,  ..., -0.0141, -0.0030, -0.0120],
        [ 0.0061, -0.0022, -0.0105,  ...,  0.0121,  0.0107,  0.0121]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.0.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[119],
            [138],
            [153],
            ...,
            [205],
            [ 23],
            [120]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0147,  0.0134,  0.0054,  ..., -0.0028,  0.0063,  0.0039],
        [-0.0039,  0.0082, -0.0047,  ..., -0.0124,  0.0060, -0.0067],
        [-0.0003, -0.0019, -0.0087,  ...,  0.0008,  0.0088, -0.0039],
        ...,
        [ 0.0098,  0.0049,  0.0060,  ...,  0.0065,  0.0051,  0.0034],
        [ 0.0030, -0.0143,  0.0043,  ..., -0.0114, -0.0111, -0.0131],
        [ 0.0125, -0.0059, -0.0047,  ..., -0.0036, -0.0072, -0.0056]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.0.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[117],
            [195],
            [ 87],
            ...,
            [158],
            [204],
            [ 34]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-1.1780e-02, -1.5076e-02,  8.4229e-03,  ...,  1.4221e-02,
         -1.2634e-02, -8.7891e-03],
        [ 1.6174e-03, -9.0599e-05,  1.3794e-02,  ...,  7.4387e-04,
          1.0010e-02,  9.5825e-03],
        [ 3.8757e-03,  9.3384e-03, -1.7166e-03,  ...,  2.4414e-03,
         -1.0681e-02, -1.4801e-03],
        ...,
        [-7.6294e-03,  1.0010e-02,  1.0193e-02,  ..., -4.3945e-03,
         -1.1749e-03,  7.9346e-03],
        [ 1.4832e-02,  8.5449e-03, -1.3367e-02,  ...,  7.0801e-03,
         -1.1780e-02, -2.7161e-03],
        [ 7.5378e-03, -3.5706e-03,  1.3428e-02,  ..., -1.4099e-02,
         -1.0132e-02, -5.6839e-04]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.0.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[204],
            [228],
            [226],
            ...,
            [ 25],
            [ 23],
            [146]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0066, -0.0052,  0.0103,  ...,  0.0054, -0.0072,  0.0027],
        [ 0.0055,  0.0144,  0.0076,  ..., -0.0069, -0.0131, -0.0060],
        [-0.0007,  0.0018,  0.0135,  ..., -0.0047,  0.0132,  0.0147],
        ...,
        [-0.0051, -0.0056, -0.0137,  ..., -0.0068, -0.0095,  0.0143],
        [-0.0074,  0.0120, -0.0148,  ..., -0.0084, -0.0002,  0.0040],
        [ 0.0070,  0.0024, -0.0142,  ...,  0.0132,  0.0049,  0.0002]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.0.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[132],
            [149],
            [204],
            ...,
            [ 73],
            [103],
            [ 98]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0024,  0.0050, -0.0063,  ...,  0.0093,  0.0084, -0.0011],
        [ 0.0029, -0.0017, -0.0051,  ...,  0.0078,  0.0006, -0.0064],
        [-0.0033,  0.0071,  0.0066,  ..., -0.0052,  0.0073, -0.0004],
        ...,
        [-0.0037, -0.0050,  0.0054,  ...,  0.0009, -0.0056, -0.0052],
        [ 0.0075,  0.0085,  0.0044,  ...,  0.0050, -0.0038,  0.0082],
        [ 0.0034,  0.0009, -0.0037,  ..., -0.0011, -0.0030, -0.0004]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.0.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[113],
            [180],
            [ 20],
            ...,
            [ 57],
            [154],
            [ 48]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0134,  0.0128, -0.0116,  ...,  0.0078, -0.0133, -0.0120],
        [ 0.0149,  0.0094,  0.0048,  ...,  0.0079, -0.0073, -0.0036],
        [ 0.0068, -0.0086, -0.0065,  ..., -0.0117, -0.0135, -0.0109],
        ...,
        [ 0.0088, -0.0081, -0.0124,  ..., -0.0090,  0.0084,  0.0075],
        [ 0.0144,  0.0103,  0.0152,  ...,  0.0088, -0.0035,  0.0008],
        [ 0.0038,  0.0087, -0.0115,  ..., -0.0116, -0.0040, -0.0135]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.0.input_layernorm.weight Parameter containing:
tensor([0.0297, 0.0136, 0.0020,  ..., 0.0103, 0.0110, 0.0061], device='cuda:0')
base_model.model.model.layers.0.post_attention_layernorm.weight Parameter containing:
tensor([0.0503, 0.0525, 0.0500,  ..., 0.0525, 0.0535, 0.0491], device='cuda:0')
base_model.model.model.layers.1.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 88],
            [ 40],
            [155],
            ...,
            [ 71],
            [148],
            [114]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0131, -0.0004, -0.0131,  ...,  0.0137, -0.0007,  0.0069],
        [ 0.0009, -0.0143,  0.0109,  ..., -0.0125,  0.0125,  0.0057],
        [ 0.0056, -0.0148, -0.0005,  ..., -0.0002, -0.0056,  0.0016],
        ...,
        [-0.0035,  0.0050, -0.0114,  ...,  0.0092, -0.0096, -0.0078],
        [ 0.0128, -0.0014,  0.0012,  ..., -0.0140,  0.0024,  0.0045],
        [-0.0005, -0.0020, -0.0081,  ..., -0.0019,  0.0036,  0.0092]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.1.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 38],
            [228],
            [ 71],
            ...,
            [212],
            [ 42],
            [107]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0024, -0.0099, -0.0115,  ...,  0.0041,  0.0111, -0.0154],
        [-0.0027,  0.0155,  0.0050,  ...,  0.0013, -0.0011,  0.0060],
        [-0.0112,  0.0063,  0.0006,  ...,  0.0005,  0.0118, -0.0007],
        ...,
        [-0.0011, -0.0071,  0.0074,  ...,  0.0109,  0.0079, -0.0140],
        [ 0.0126,  0.0058,  0.0032,  ..., -0.0022,  0.0137,  0.0080],
        [ 0.0041, -0.0117, -0.0087,  ..., -0.0022, -0.0030,  0.0118]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.1.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[133],
            [104],
            [154],
            ...,
            [228],
            [ 89],
            [ 61]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0092, -0.0074,  0.0145,  ..., -0.0077,  0.0073, -0.0137],
        [-0.0048, -0.0079, -0.0045,  ..., -0.0148, -0.0008, -0.0065],
        [-0.0054, -0.0140,  0.0079,  ..., -0.0020, -0.0092,  0.0109],
        ...,
        [-0.0026, -0.0015, -0.0131,  ..., -0.0101, -0.0007,  0.0102],
        [-0.0079,  0.0060,  0.0138,  ..., -0.0078,  0.0059,  0.0056],
        [-0.0064, -0.0008,  0.0018,  ...,  0.0147, -0.0021,  0.0112]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.1.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[145],
            [213],
            [198],
            ...,
            [ 71],
            [217],
            [ 42]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0107, -0.0118, -0.0118,  ...,  0.0031, -0.0051,  0.0126],
        [ 0.0044, -0.0043,  0.0093,  ..., -0.0075, -0.0008, -0.0125],
        [ 0.0067,  0.0076,  0.0071,  ...,  0.0004,  0.0033,  0.0036],
        ...,
        [ 0.0054,  0.0150, -0.0147,  ...,  0.0150, -0.0049, -0.0024],
        [-0.0154, -0.0065,  0.0143,  ..., -0.0081,  0.0018, -0.0040],
        [ 0.0069,  0.0110,  0.0081,  ...,  0.0107, -0.0065, -0.0093]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.1.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[229],
            [246],
            [163],
            ...,
            [152],
            [ 78],
            [161]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0048,  0.0090,  0.0098,  ...,  0.0064,  0.0016, -0.0082],
        [-0.0109,  0.0118, -0.0011,  ...,  0.0009, -0.0095,  0.0131],
        [ 0.0117,  0.0106, -0.0089,  ..., -0.0017, -0.0125,  0.0086],
        ...,
        [-0.0011,  0.0076,  0.0031,  ...,  0.0020, -0.0016,  0.0066],
        [ 0.0106,  0.0119, -0.0014,  ...,  0.0073, -0.0110,  0.0046],
        [-0.0060, -0.0142, -0.0008,  ..., -0.0047, -0.0072,  0.0139]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.1.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[153],
            [248],
            [186],
            ...,
            [ 70],
            [131],
            [ 62]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0081,  0.0041, -0.0084,  ..., -0.0031, -0.0017,  0.0059],
        [ 0.0024, -0.0031, -0.0045,  ...,  0.0068,  0.0012,  0.0068],
        [-0.0015, -0.0002, -0.0054,  ...,  0.0027, -0.0038, -0.0038],
        ...,
        [-0.0025, -0.0028,  0.0085,  ...,  0.0065,  0.0082, -0.0069],
        [-0.0061, -0.0004,  0.0089,  ...,  0.0041, -0.0083,  0.0092],
        [ 0.0008, -0.0042, -0.0010,  ...,  0.0034, -0.0001,  0.0068]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.1.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[143],
            [ 61],
            [149],
            ...,
            [106],
            [ 18],
            [ 92]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0053, -0.0020,  0.0153,  ..., -0.0117,  0.0079,  0.0067],
        [-0.0045,  0.0128,  0.0148,  ...,  0.0123, -0.0093, -0.0124],
        [-0.0029,  0.0139,  0.0078,  ...,  0.0051,  0.0130, -0.0085],
        ...,
        [ 0.0151,  0.0013,  0.0080,  ...,  0.0154,  0.0092, -0.0068],
        [ 0.0125, -0.0039, -0.0140,  ..., -0.0034, -0.0140, -0.0151],
        [ 0.0059, -0.0029,  0.0062,  ..., -0.0045,  0.0020,  0.0031]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.1.input_layernorm.weight Parameter containing:
tensor([0.1138, 0.1099, 0.1006,  ..., 0.0630, 0.0942, 0.0742], device='cuda:0')
base_model.model.model.layers.1.post_attention_layernorm.weight Parameter containing:
tensor([0.0996, 0.1006, 0.0962,  ..., 0.1074, 0.0996, 0.1016], device='cuda:0')
base_model.model.model.layers.2.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 20],
            [172],
            [210],
            ...,
            [199],
            [ 62],
            [ 23]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0132,  0.0107,  0.0081,  ...,  0.0148, -0.0115, -0.0147],
        [-0.0097, -0.0095,  0.0018,  ..., -0.0026,  0.0128, -0.0041],
        [ 0.0039,  0.0018,  0.0063,  ..., -0.0129,  0.0150,  0.0017],
        ...,
        [-0.0002,  0.0137,  0.0073,  ..., -0.0081, -0.0119,  0.0103],
        [-0.0056,  0.0129,  0.0046,  ..., -0.0087, -0.0050,  0.0149],
        [ 0.0126,  0.0078,  0.0151,  ..., -0.0082,  0.0038, -0.0101]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.2.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[107],
            [ 77],
            [ 91],
            ...,
            [125],
            [192],
            [ 79]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0114,  0.0037, -0.0052,  ...,  0.0083, -0.0021, -0.0009],
        [ 0.0138, -0.0149,  0.0114,  ..., -0.0077, -0.0026,  0.0043],
        [-0.0055,  0.0156,  0.0066,  ...,  0.0026, -0.0033,  0.0141],
        ...,
        [ 0.0002,  0.0071,  0.0038,  ..., -0.0126,  0.0031, -0.0093],
        [ 0.0036,  0.0142,  0.0122,  ...,  0.0117, -0.0140,  0.0094],
        [-0.0112, -0.0128, -0.0058,  ..., -0.0106, -0.0147, -0.0100]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.2.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[103],
            [215],
            [133],
            ...,
            [152],
            [165],
            [197]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0096, -0.0024,  0.0056,  ..., -0.0085,  0.0074,  0.0153],
        [-0.0099,  0.0067,  0.0084,  ...,  0.0127, -0.0022, -0.0084],
        [ 0.0058, -0.0053,  0.0128,  ...,  0.0117, -0.0069,  0.0100],
        ...,
        [-0.0020, -0.0066, -0.0018,  ...,  0.0079,  0.0130,  0.0057],
        [ 0.0132, -0.0030, -0.0019,  ..., -0.0057,  0.0039,  0.0090],
        [ 0.0015, -0.0009, -0.0029,  ...,  0.0086,  0.0127,  0.0098]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.2.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[123],
            [155],
            [ 38],
            ...,
            [120],
            [100],
            [179]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0060, -0.0109, -0.0154,  ...,  0.0070, -0.0015, -0.0123],
        [ 0.0079, -0.0121, -0.0047,  ...,  0.0018, -0.0119, -0.0058],
        [-0.0045, -0.0153,  0.0093,  ...,  0.0121,  0.0066, -0.0012],
        ...,
        [-0.0066,  0.0084, -0.0139,  ...,  0.0077,  0.0039, -0.0049],
        [-0.0049,  0.0033,  0.0033,  ..., -0.0004,  0.0013, -0.0121],
        [ 0.0093,  0.0110, -0.0064,  ...,  0.0016, -0.0012,  0.0143]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.2.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[139],
            [ 88],
            [105],
            ...,
            [123],
            [244],
            [ 25]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0058, -0.0106,  0.0055,  ...,  0.0124,  0.0106, -0.0088],
        [-0.0012, -0.0008, -0.0087,  ...,  0.0118,  0.0070,  0.0059],
        [ 0.0068,  0.0089,  0.0046,  ..., -0.0148,  0.0077,  0.0132],
        ...,
        [-0.0067, -0.0089, -0.0118,  ..., -0.0060,  0.0039, -0.0146],
        [ 0.0021,  0.0078,  0.0036,  ..., -0.0079, -0.0075,  0.0112],
        [ 0.0119,  0.0042, -0.0022,  ...,  0.0146,  0.0032, -0.0110]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.2.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[154],
            [212],
            [109],
            ...,
            [208],
            [109],
            [  5]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 6.4697e-03,  8.9111e-03, -7.6904e-03,  ..., -5.8594e-03,
          5.8413e-05, -9.0332e-03],
        [-1.0452e-03, -6.0425e-03, -5.9814e-03,  ..., -8.8501e-03,
          3.5706e-03, -8.7891e-03],
        [ 8.8882e-04, -6.9275e-03, -9.4604e-03,  ..., -2.7924e-03,
         -3.7842e-03, -9.2163e-03],
        ...,
        [-8.7280e-03,  8.5449e-03, -6.2866e-03,  ..., -1.2970e-03,
          7.4463e-03,  7.8201e-04],
        [ 2.5482e-03, -6.5308e-03,  4.4861e-03,  ...,  1.8463e-03,
         -3.1433e-03,  9.0942e-03],
        [-3.7994e-03,  4.9210e-04, -6.4697e-03,  ...,  8.5449e-03,
         -8.7280e-03,  1.0986e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.2.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[119],
            [153],
            [139],
            ...,
            [168],
            [ 69],
            [117]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0037,  0.0039, -0.0049,  ..., -0.0134, -0.0120,  0.0046],
        [-0.0109, -0.0143, -0.0015,  ..., -0.0101, -0.0087,  0.0089],
        [ 0.0110, -0.0107,  0.0134,  ..., -0.0082,  0.0123,  0.0148],
        ...,
        [ 0.0150, -0.0071, -0.0107,  ...,  0.0141,  0.0143, -0.0151],
        [-0.0014,  0.0005,  0.0142,  ..., -0.0129,  0.0051, -0.0121],
        [ 0.0049, -0.0042, -0.0045,  ...,  0.0048,  0.0035, -0.0143]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.2.input_layernorm.weight Parameter containing:
tensor([0.1738, 0.1777, 0.1738,  ..., 0.1768, 0.1709, 0.1748], device='cuda:0')
base_model.model.model.layers.2.post_attention_layernorm.weight Parameter containing:
tensor([0.1338, 0.1367, 0.1357,  ..., 0.1357, 0.1387, 0.1357], device='cuda:0')
base_model.model.model.layers.3.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[172],
            [115],
            [246],
            ...,
            [ 89],
            [181],
            [102]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0074,  0.0008, -0.0069,  ..., -0.0038, -0.0049,  0.0118],
        [-0.0143,  0.0124, -0.0034,  ...,  0.0118, -0.0115, -0.0139],
        [-0.0034, -0.0054, -0.0007,  ...,  0.0002,  0.0026,  0.0130],
        ...,
        [-0.0022, -0.0032,  0.0108,  ..., -0.0142, -0.0126,  0.0064],
        [ 0.0021,  0.0073, -0.0022,  ..., -0.0136, -0.0139,  0.0132],
        [ 0.0081, -0.0050, -0.0070,  ...,  0.0014,  0.0016,  0.0117]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.3.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[149],
            [ 81],
            [173],
            ...,
            [ 88],
            [150],
            [134]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 8.8501e-03,  1.2268e-02, -5.8594e-03,  ..., -3.9062e-03,
         -4.1199e-04, -2.7313e-03],
        [-8.6060e-03, -7.9346e-03,  9.0942e-03,  ..., -3.3569e-03,
          1.0315e-02,  8.3160e-04],
        [-1.2390e-02,  2.8534e-03,  1.2817e-02,  ..., -1.0193e-02,
         -1.3794e-02, -7.5684e-03],
        ...,
        [-1.1658e-02,  6.3477e-03,  1.5503e-02,  ..., -1.4648e-02,
         -8.7891e-03, -1.3428e-02],
        [-4.3640e-03, -1.4099e-02, -5.3101e-03,  ..., -4.2725e-03,
          1.0986e-02, -1.6117e-04],
        [-1.2939e-02,  6.4373e-05,  4.2419e-03,  ..., -1.0986e-02,
          1.1230e-02,  1.3611e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.3.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[108],
            [ 88],
            [ 77],
            ...,
            [105],
            [ 55],
            [142]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 1.3000e-02, -1.0757e-03, -1.2695e-02,  ..., -1.2939e-02,
          6.4087e-03, -2.1219e-05],
        [-1.3611e-02,  7.4158e-03,  1.1902e-03,  ...,  8.0566e-03,
         -8.1787e-03,  3.2806e-03],
        [-4.1809e-03, -5.7678e-03, -1.2756e-02,  ..., -1.1292e-02,
         -1.4282e-02,  2.9755e-03],
        ...,
        [ 2.6093e-03,  4.7607e-03, -8.9722e-03,  ..., -1.3367e-02,
         -8.4229e-03,  1.4893e-02],
        [ 1.6174e-03, -1.2390e-02,  1.0071e-02,  ..., -5.7983e-03,
          2.5635e-03,  1.0498e-02],
        [-2.5635e-03,  2.6855e-03,  8.9111e-03,  ...,  9.5825e-03,
          1.0803e-02, -1.5137e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.3.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[ 25],
            [ 38],
            [ 85],
            ...,
            [115],
            [201],
            [156]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[ 2.6245e-03,  7.6599e-03, -1.0986e-03,  ..., -6.6833e-03,
         -8.1787e-03, -6.8054e-03],
        [-1.4160e-02,  5.2185e-03, -4.7913e-03,  ...,  4.3335e-03,
          4.8828e-03,  1.8311e-03],
        [-1.5259e-03,  4.4250e-03, -3.7432e-05,  ..., -1.5137e-02,
         -5.4626e-03, -4.4250e-03],
        ...,
        [ 3.0365e-03,  1.2512e-02, -1.4954e-02,  ..., -1.1230e-02,
         -3.8300e-03,  1.1414e-02],
        [ 1.2756e-02,  1.1841e-02,  1.8234e-03,  ..., -9.8877e-03,
         -1.4343e-02,  5.1575e-03],
        [-3.1128e-03, -4.4250e-03, -6.6223e-03,  ...,  3.9482e-04,
          1.1780e-02, -4.5166e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.3.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[101],
            [ 68],
            [139],
            ...,
            [238],
            [173],
            [ 26]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0146, -0.0010, -0.0064,  ..., -0.0146, -0.0056,  0.0015],
        [-0.0151, -0.0049, -0.0025,  ..., -0.0052, -0.0095,  0.0148],
        [ 0.0123, -0.0005,  0.0100,  ..., -0.0075,  0.0035,  0.0022],
        ...,
        [ 0.0148, -0.0093,  0.0015,  ..., -0.0143,  0.0115, -0.0072],
        [-0.0146, -0.0006, -0.0145,  ..., -0.0088, -0.0017,  0.0045],
        [-0.0067, -0.0146, -0.0059,  ...,  0.0120,  0.0139,  0.0037]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.3.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[117],
            [150],
            [ 43],
            ...,
            [ 88],
            [167],
            [153]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0017,  0.0031, -0.0036,  ..., -0.0020,  0.0064, -0.0047],
        [ 0.0018, -0.0063,  0.0084,  ...,  0.0004,  0.0094, -0.0034],
        [ 0.0067, -0.0084,  0.0086,  ...,  0.0089,  0.0006,  0.0009],
        ...,
        [ 0.0008,  0.0063,  0.0062,  ..., -0.0048,  0.0001, -0.0005],
        [ 0.0029, -0.0008,  0.0069,  ..., -0.0065, -0.0089, -0.0038],
        [-0.0065,  0.0033, -0.0084,  ..., -0.0082, -0.0032, -0.0083]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.3.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 75],
            [158],
            [108],
            ...,
            [ 59],
            [158],
            [152]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0067,  0.0089, -0.0151,  ..., -0.0156, -0.0096,  0.0083],
        [ 0.0081,  0.0048, -0.0034,  ...,  0.0106,  0.0151, -0.0134],
        [ 0.0139,  0.0125, -0.0144,  ...,  0.0039, -0.0143, -0.0058],
        ...,
        [-0.0111, -0.0151, -0.0054,  ..., -0.0153, -0.0033, -0.0018],
        [-0.0050,  0.0093, -0.0047,  ...,  0.0026, -0.0117, -0.0101],
        [-0.0118, -0.0056, -0.0002,  ...,  0.0017,  0.0114,  0.0030]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.3.input_layernorm.weight Parameter containing:
tensor([0.2832, 0.2832, 0.2812,  ..., 0.2793, 0.2891, 0.2910], device='cuda:0')
base_model.model.model.layers.3.post_attention_layernorm.weight Parameter containing:
tensor([0.1748, 0.1748, 0.1699,  ..., 0.1738, 0.1709, 0.1748], device='cuda:0')
base_model.model.model.layers.4.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 41],
            [101],
            [ 85],
            ...,
            [179],
            [158],
            [210]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0112, -0.0099, -0.0150,  ...,  0.0128,  0.0067,  0.0125],
        [ 0.0011, -0.0099, -0.0142,  ..., -0.0141,  0.0145, -0.0127],
        [ 0.0012, -0.0045,  0.0018,  ...,  0.0104,  0.0090, -0.0023],
        ...,
        [-0.0029, -0.0058, -0.0031,  ..., -0.0017, -0.0134,  0.0025],
        [-0.0079, -0.0153,  0.0072,  ..., -0.0072,  0.0084,  0.0024],
        [-0.0092, -0.0020, -0.0010,  ...,  0.0112,  0.0044, -0.0151]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.4.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 91],
            [ 73],
            [170],
            ...,
            [135],
            [151],
            [182]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0114,  0.0098, -0.0045,  ...,  0.0082, -0.0114,  0.0005],
        [-0.0084,  0.0121,  0.0079,  ...,  0.0021, -0.0074, -0.0026],
        [ 0.0095, -0.0005,  0.0019,  ..., -0.0052, -0.0096, -0.0138],
        ...,
        [ 0.0017,  0.0089,  0.0118,  ..., -0.0103,  0.0081, -0.0109],
        [ 0.0074, -0.0108,  0.0106,  ...,  0.0114,  0.0127,  0.0032],
        [-0.0007,  0.0065, -0.0116,  ..., -0.0058,  0.0104,  0.0027]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.4.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[118],
            [ 73],
            [148],
            ...,
            [ 83],
            [ 57],
            [107]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0113, -0.0015, -0.0104,  ...,  0.0075,  0.0017,  0.0027],
        [-0.0037,  0.0039, -0.0148,  ..., -0.0066, -0.0085, -0.0002],
        [-0.0045,  0.0137,  0.0120,  ..., -0.0148, -0.0101,  0.0089],
        ...,
        [ 0.0092, -0.0141,  0.0109,  ...,  0.0040, -0.0117, -0.0029],
        [ 0.0065, -0.0051,  0.0140,  ...,  0.0153,  0.0079, -0.0078],
        [-0.0093, -0.0081,  0.0140,  ...,  0.0056, -0.0059,  0.0121]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.4.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[250],
            [105],
            [ 51],
            ...,
            [182],
            [118],
            [199]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0132, -0.0098,  0.0025,  ...,  0.0069,  0.0029, -0.0156],
        [-0.0109,  0.0132,  0.0120,  ...,  0.0049, -0.0082, -0.0121],
        [ 0.0093,  0.0145, -0.0027,  ..., -0.0117, -0.0120, -0.0022],
        ...,
        [-0.0132, -0.0099,  0.0011,  ..., -0.0086,  0.0102, -0.0032],
        [ 0.0092,  0.0020, -0.0145,  ...,  0.0077,  0.0084,  0.0011],
        [-0.0011,  0.0058, -0.0135,  ...,  0.0074, -0.0151,  0.0060]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.4.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[105],
            [ 89],
            [123],
            ...,
            [ 67],
            [ 69],
            [187]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-1.0925e-02, -1.3428e-02,  1.0498e-02,  ...,  5.8413e-06,
         -1.4267e-03, -9.7752e-05],
        [-1.5503e-02, -7.9346e-03,  4.6692e-03,  ...,  5.7983e-03,
          1.3367e-02, -8.6670e-03],
        [ 9.3994e-03,  7.2937e-03,  9.5825e-03,  ...,  1.1963e-02,
          4.5166e-03,  3.8300e-03],
        ...,
        [-1.2085e-02,  1.2329e-02, -1.4038e-02,  ...,  6.1951e-03,
          2.4414e-03, -1.4526e-02],
        [-3.9101e-04, -1.4038e-02, -1.3733e-02,  ...,  6.5918e-03,
         -6.0120e-03,  1.5564e-02],
        [-9.3384e-03, -2.2984e-04,  4.0588e-03,  ...,  1.1841e-02,
         -7.3853e-03,  1.4526e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.4.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[214],
            [226],
            [228],
            ...,
            [187],
            [230],
            [  2]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0067,  0.0076, -0.0004,  ...,  0.0058, -0.0013,  0.0038],
        [-0.0094,  0.0029,  0.0085,  ...,  0.0021,  0.0020,  0.0066],
        [-0.0013, -0.0060,  0.0003,  ..., -0.0077, -0.0026, -0.0089],
        ...,
        [-0.0006, -0.0091,  0.0033,  ..., -0.0082, -0.0051,  0.0049],
        [ 0.0037,  0.0060, -0.0045,  ..., -0.0092,  0.0035,  0.0039],
        [-0.0011, -0.0074,  0.0004,  ...,  0.0005,  0.0032,  0.0022]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.4.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 69],
            [  5],
            [ 92],
            ...,
            [ 55],
            [183],
            [118]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0047,  0.0154, -0.0114,  ..., -0.0012, -0.0066, -0.0070],
        [ 0.0069, -0.0107, -0.0114,  ...,  0.0115, -0.0038,  0.0060],
        [ 0.0010,  0.0032, -0.0080,  ...,  0.0024,  0.0071,  0.0026],
        ...,
        [-0.0019, -0.0093,  0.0053,  ..., -0.0131,  0.0121, -0.0093],
        [-0.0114,  0.0072,  0.0023,  ...,  0.0054, -0.0044,  0.0055],
        [-0.0088, -0.0011, -0.0130,  ...,  0.0154,  0.0033,  0.0008]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.4.input_layernorm.weight Parameter containing:
tensor([0.2617, 0.2578, 0.2598,  ..., 0.2559, 0.2637, 0.2715], device='cuda:0')
base_model.model.model.layers.4.post_attention_layernorm.weight Parameter containing:
tensor([0.1885, 0.1865, 0.1816,  ..., 0.1885, 0.1865, 0.1855], device='cuda:0')
base_model.model.model.layers.5.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 85],
            [ 57],
            [165],
            ...,
            [187],
            [195],
            [ 87]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 1.0742e-02, -7.5989e-03,  1.1108e-02,  ...,  1.0437e-02,
          9.8877e-03, -1.5015e-02],
        [-6.6376e-04, -1.3794e-02,  3.2806e-03,  ..., -4.4556e-03,
          3.1586e-03, -4.7607e-03],
        [ 7.9346e-03,  1.5137e-02, -1.4038e-02,  ...,  6.3896e-05,
         -3.5858e-03,  2.3804e-03],
        ...,
        [ 1.8768e-03,  1.3504e-03, -8.4229e-03,  ...,  8.2397e-03,
          3.7384e-03,  1.3977e-02],
        [ 1.5106e-03, -1.3123e-02, -8.1787e-03,  ...,  1.3550e-02,
          9.5215e-03,  1.0300e-03],
        [ 9.8267e-03, -9.5825e-03, -8.2397e-03,  ..., -3.4637e-03,
         -1.1719e-02, -7.9346e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.5.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 63],
            [156],
            [ 92],
            ...,
            [178],
            [ 85],
            [ 56]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0027, -0.0087,  0.0068,  ..., -0.0146, -0.0092, -0.0017],
        [ 0.0151, -0.0050, -0.0058,  ...,  0.0080, -0.0099,  0.0117],
        [-0.0052, -0.0007,  0.0094,  ..., -0.0024,  0.0004,  0.0025],
        ...,
        [-0.0114,  0.0090, -0.0123,  ..., -0.0119, -0.0015, -0.0114],
        [-0.0127,  0.0022,  0.0120,  ...,  0.0105,  0.0079, -0.0076],
        [-0.0126, -0.0099,  0.0080,  ..., -0.0039, -0.0015, -0.0050]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.5.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[  9],
            [198],
            [ 72],
            ...,
            [215],
            [212],
            [ 81]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0013, -0.0002,  0.0154,  ..., -0.0087, -0.0154,  0.0026],
        [ 0.0069, -0.0004,  0.0051,  ...,  0.0059,  0.0135, -0.0008],
        [ 0.0052,  0.0050,  0.0102,  ..., -0.0146, -0.0075,  0.0146],
        ...,
        [ 0.0045,  0.0124, -0.0057,  ..., -0.0062,  0.0131,  0.0044],
        [-0.0106, -0.0075, -0.0126,  ..., -0.0035,  0.0153, -0.0036],
        [-0.0015, -0.0040,  0.0111,  ..., -0.0152, -0.0033,  0.0093]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.5.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[ 37],
            [151],
            [100],
            ...,
            [100],
            [105],
            [ 82]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[ 3.6469e-03,  1.1353e-02,  6.9885e-03,  ..., -6.7444e-03,
         -2.1973e-03, -7.8964e-04],
        [ 3.3417e-03,  7.9956e-03, -1.5198e-02,  ...,  1.5442e-02,
          1.0864e-02, -1.4038e-02],
        [ 1.4893e-02, -7.1049e-05, -1.0925e-02,  ..., -1.1902e-02,
          2.1667e-03, -5.3406e-04],
        ...,
        [ 6.8359e-03,  1.2207e-02, -9.5825e-03,  ..., -1.2207e-02,
          8.5449e-03, -1.4099e-02],
        [-7.8125e-03,  1.3367e-02,  8.6060e-03,  ...,  3.4943e-03,
         -7.2479e-04,  9.7656e-03],
        [ 2.1515e-03,  1.4725e-03, -1.5137e-02,  ...,  9.7656e-03,
         -3.7537e-03, -1.1475e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.5.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[119],
            [ 44],
            [ 65],
            ...,
            [ 26],
            [ 86],
            [113]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0049, -0.0124, -0.0079,  ..., -0.0120, -0.0014,  0.0023],
        [-0.0089,  0.0134, -0.0125,  ..., -0.0114, -0.0076,  0.0070],
        [ 0.0015, -0.0056, -0.0120,  ..., -0.0103,  0.0012, -0.0012],
        ...,
        [-0.0057,  0.0135,  0.0138,  ..., -0.0148, -0.0131, -0.0102],
        [ 0.0064, -0.0093, -0.0124,  ..., -0.0092, -0.0033, -0.0135],
        [ 0.0074, -0.0003,  0.0017,  ..., -0.0009,  0.0129, -0.0130]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.5.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[ 97],
            [106],
            [120],
            ...,
            [ 54],
            [126],
            [148]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-8.3008e-03, -6.4697e-03,  4.3640e-03,  ..., -7.0190e-03,
          6.9275e-03,  1.5488e-03],
        [-5.2795e-03, -5.1270e-03, -7.7515e-03,  ...,  3.2043e-04,
         -3.0756e-05,  7.6904e-03],
        [-3.3875e-03,  3.6621e-03,  3.3264e-03,  ...,  8.9722e-03,
          9.0332e-03,  7.4463e-03],
        ...,
        [-2.7084e-04,  6.9809e-04,  3.1891e-03,  ..., -9.3384e-03,
          6.4087e-03, -8.3618e-03],
        [ 5.8365e-04, -3.2349e-03,  2.1744e-04,  ..., -2.4128e-04,
          8.1787e-03,  6.4697e-03],
        [-7.7820e-03, -6.0120e-03, -5.3101e-03,  ...,  1.2360e-03,
         -8.9645e-04,  4.1504e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.5.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[100],
            [122],
            [170],
            ...,
            [ 33],
            [ 20],
            [ 98]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 3.9673e-03,  9.7656e-03,  7.9346e-03,  ...,  1.4526e-02,
         -6.5994e-04, -1.1658e-02],
        [ 9.5844e-05,  7.7820e-03, -1.5564e-02,  ..., -1.2329e-02,
         -4.1809e-03, -1.3733e-02],
        [ 5.2490e-03, -1.2817e-02, -2.4567e-03,  ...,  1.2939e-02,
         -4.5776e-03,  8.7280e-03],
        ...,
        [-5.5237e-03, -1.2695e-02,  1.5137e-02,  ..., -1.8921e-03,
          8.7280e-03,  4.8523e-03],
        [-1.1719e-02,  9.6436e-03, -1.2589e-03,  ..., -1.1536e-02,
         -1.4771e-02,  1.5320e-02],
        [ 3.7689e-03, -1.2634e-02,  1.9226e-03,  ..., -5.2185e-03,
         -1.5869e-03, -1.5564e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.5.input_layernorm.weight Parameter containing:
tensor([0.2637, 0.2637, 0.2598,  ..., 0.2520, 0.2676, 0.2695], device='cuda:0')
base_model.model.model.layers.5.post_attention_layernorm.weight Parameter containing:
tensor([0.2041, 0.1934, 0.1895,  ..., 0.2051, 0.1992, 0.2031], device='cuda:0')
base_model.model.model.layers.6.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[102],
            [142],
            [ 81],
            ...,
            [ 56],
            [121],
            [120]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0053, -0.0054,  0.0130,  ..., -0.0056,  0.0036, -0.0132],
        [ 0.0070,  0.0058, -0.0065,  ..., -0.0044,  0.0023, -0.0107],
        [-0.0120, -0.0097,  0.0141,  ...,  0.0148,  0.0078, -0.0054],
        ...,
        [ 0.0040,  0.0052,  0.0153,  ...,  0.0107,  0.0106, -0.0102],
        [ 0.0002,  0.0152,  0.0047,  ...,  0.0034,  0.0006,  0.0060],
        [-0.0102,  0.0142,  0.0089,  ..., -0.0129, -0.0107, -0.0090]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.6.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[165],
            [162],
            [151],
            ...,
            [ 39],
            [ 74],
            [ 35]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0085,  0.0123,  0.0102,  ...,  0.0004,  0.0060,  0.0106],
        [-0.0074,  0.0009, -0.0045,  ...,  0.0012,  0.0104, -0.0033],
        [ 0.0132, -0.0002,  0.0052,  ..., -0.0092,  0.0011, -0.0003],
        ...,
        [-0.0148, -0.0155, -0.0045,  ...,  0.0085, -0.0025, -0.0129],
        [-0.0089,  0.0082, -0.0056,  ...,  0.0052,  0.0020, -0.0125],
        [ 0.0067, -0.0021,  0.0109,  ...,  0.0110, -0.0092,  0.0128]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.6.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[191],
            [154],
            [191],
            ...,
            [183],
            [242],
            [118]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0129, -0.0019,  0.0019,  ...,  0.0126, -0.0092, -0.0050],
        [-0.0069, -0.0042,  0.0151,  ..., -0.0027, -0.0117,  0.0109],
        [-0.0052, -0.0017,  0.0128,  ...,  0.0055,  0.0056,  0.0103],
        ...,
        [ 0.0018, -0.0024,  0.0129,  ..., -0.0077, -0.0010, -0.0156],
        [ 0.0109,  0.0076, -0.0020,  ...,  0.0104,  0.0094,  0.0131],
        [-0.0125, -0.0022,  0.0076,  ..., -0.0148, -0.0054, -0.0025]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.6.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[179],
            [  9],
            [103],
            ...,
            [ 19],
            [195],
            [154]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0035, -0.0155,  0.0057,  ..., -0.0058, -0.0042,  0.0139],
        [-0.0038, -0.0140, -0.0052,  ..., -0.0046, -0.0048, -0.0009],
        [ 0.0046, -0.0050, -0.0048,  ..., -0.0094,  0.0059,  0.0094],
        ...,
        [ 0.0079, -0.0128,  0.0001,  ...,  0.0063, -0.0014,  0.0137],
        [-0.0133, -0.0083, -0.0030,  ...,  0.0001,  0.0119, -0.0078],
        [ 0.0035,  0.0029,  0.0063,  ..., -0.0009,  0.0095, -0.0083]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.6.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[221],
            [ 55],
            [231],
            ...,
            [161],
            [209],
            [123]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 1.1780e-02,  6.5002e-03,  3.5248e-03,  ...,  1.5198e-02,
          7.8735e-03, -2.2430e-03],
        [ 5.4321e-03,  3.7689e-03,  4.1723e-05,  ...,  1.9531e-03,
         -1.1475e-02, -5.5542e-03],
        [-1.4282e-02, -2.8687e-03,  9.2773e-03,  ...,  7.1716e-03,
          4.9438e-03,  5.8594e-03],
        ...,
        [-1.4771e-02,  4.9210e-04,  9.3994e-03,  ...,  9.2163e-03,
          1.3809e-03,  4.0588e-03],
        [ 1.4343e-02, -1.2878e-02,  9.9487e-03,  ..., -8.3008e-03,
          5.1880e-03, -4.4250e-04],
        [ 1.4893e-02, -7.5684e-03,  4.2419e-03,  ...,  7.7820e-03,
          2.0294e-03,  2.7466e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.6.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[ 92],
            [105],
            [176],
            ...,
            [226],
            [175],
            [ 89]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0075,  0.0035, -0.0065,  ...,  0.0082,  0.0086, -0.0072],
        [-0.0043,  0.0089,  0.0032,  ..., -0.0017,  0.0092, -0.0054],
        [-0.0038,  0.0027, -0.0045,  ...,  0.0056, -0.0031, -0.0011],
        ...,
        [ 0.0085,  0.0046, -0.0045,  ..., -0.0043, -0.0008, -0.0006],
        [-0.0062, -0.0036, -0.0053,  ...,  0.0047,  0.0088,  0.0026],
        [ 0.0055,  0.0042, -0.0040,  ...,  0.0077, -0.0070, -0.0052]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.6.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 85],
            [ 73],
            [ 88],
            ...,
            [250],
            [185],
            [ 48]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0081, -0.0140, -0.0022,  ...,  0.0089, -0.0056,  0.0099],
        [ 0.0125,  0.0060,  0.0140,  ..., -0.0117, -0.0009,  0.0047],
        [ 0.0152, -0.0134, -0.0032,  ..., -0.0051, -0.0142, -0.0110],
        ...,
        [-0.0065,  0.0103,  0.0001,  ..., -0.0115,  0.0087,  0.0048],
        [-0.0025,  0.0035, -0.0010,  ...,  0.0053,  0.0046,  0.0060],
        [-0.0065, -0.0044, -0.0010,  ..., -0.0143, -0.0082,  0.0008]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.6.input_layernorm.weight Parameter containing:
tensor([0.3184, 0.3555, 0.3281,  ..., 0.3164, 0.3359, 0.3203], device='cuda:0')
base_model.model.model.layers.6.post_attention_layernorm.weight Parameter containing:
tensor([0.2168, 0.2061, 0.2041,  ..., 0.2178, 0.2109, 0.2129], device='cuda:0')
base_model.model.model.layers.7.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[168],
            [131],
            [ 33],
            ...,
            [231],
            [158],
            [182]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0013, -0.0054,  0.0148,  ..., -0.0077,  0.0007,  0.0154],
        [ 0.0101,  0.0036,  0.0065,  ...,  0.0052, -0.0140,  0.0116],
        [-0.0066, -0.0057,  0.0131,  ...,  0.0146, -0.0113,  0.0085],
        ...,
        [ 0.0084,  0.0112, -0.0147,  ..., -0.0037, -0.0051,  0.0058],
        [ 0.0015, -0.0048,  0.0138,  ..., -0.0069,  0.0010,  0.0032],
        [ 0.0147,  0.0151, -0.0010,  ..., -0.0078, -0.0031,  0.0140]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.7.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[101],
            [ 52],
            [102],
            ...,
            [198],
            [184],
            [132]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 1.1169e-02, -1.2512e-02, -9.8267e-03,  ...,  1.4099e-02,
         -2.6398e-03,  8.8501e-03],
        [-1.1826e-03, -1.0681e-02, -7.2327e-03,  ...,  7.9346e-03,
         -1.1414e-02,  1.4771e-02],
        [-1.0803e-02, -9.7752e-05,  1.1597e-02,  ...,  1.2268e-02,
          7.4768e-03,  1.2207e-02],
        ...,
        [-1.2756e-02,  4.3030e-03, -4.0588e-03,  ..., -4.1504e-03,
          6.5918e-03, -1.1047e-02],
        [ 6.2561e-03,  1.1658e-02,  3.6774e-03,  ...,  3.9101e-04,
          9.1553e-03, -1.2085e-02],
        [ 8.8882e-04, -3.7537e-03, -9.4604e-03,  ..., -1.2085e-02,
         -3.0670e-03, -1.4771e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.7.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[ 56],
            [201],
            [ 74],
            ...,
            [108],
            [ 46],
            [171]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0071,  0.0060, -0.0156,  ..., -0.0052,  0.0036,  0.0003],
        [ 0.0122,  0.0046, -0.0069,  ..., -0.0015,  0.0079, -0.0087],
        [-0.0134, -0.0033, -0.0066,  ...,  0.0124,  0.0059,  0.0131],
        ...,
        [ 0.0055,  0.0010,  0.0055,  ..., -0.0107,  0.0042, -0.0125],
        [ 0.0098,  0.0036,  0.0010,  ..., -0.0053,  0.0040,  0.0090],
        [ 0.0009, -0.0152,  0.0031,  ...,  0.0109,  0.0008, -0.0142]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.7.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[179],
            [101],
            [133],
            ...,
            [ 21],
            [ 74],
            [ 41]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0042, -0.0102,  0.0099,  ...,  0.0010, -0.0129, -0.0020],
        [-0.0035,  0.0091,  0.0018,  ..., -0.0001, -0.0037,  0.0101],
        [ 0.0031,  0.0003, -0.0069,  ..., -0.0086, -0.0049,  0.0070],
        ...,
        [ 0.0057,  0.0104, -0.0045,  ..., -0.0153, -0.0013, -0.0143],
        [-0.0141,  0.0049, -0.0069,  ..., -0.0087, -0.0019,  0.0097],
        [-0.0150,  0.0126,  0.0009,  ...,  0.0108,  0.0125, -0.0066]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.7.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[151],
            [ 57],
            [178],
            ...,
            [133],
            [ 75],
            [199]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-4.7493e-04,  1.2512e-02, -8.3618e-03,  ..., -3.0670e-03,
          1.1780e-02, -4.8828e-03],
        [ 3.1281e-03,  5.7983e-03, -1.1414e-02,  ..., -3.9673e-03,
          1.5646e-06, -3.6774e-03],
        [ 7.9956e-03,  2.7008e-03,  4.1723e-05,  ...,  8.5449e-03,
          1.5015e-02,  1.2146e-02],
        ...,
        [ 3.7384e-03, -3.3875e-03,  1.1292e-02,  ..., -5.2185e-03,
          1.5320e-02, -6.3477e-03],
        [-9.4604e-03, -1.2390e-02,  1.2665e-03,  ..., -9.3384e-03,
          9.9487e-03,  8.4839e-03],
        [-3.6011e-03,  3.5858e-03,  1.8158e-03,  ...,  1.0315e-02,
         -9.7656e-03, -1.0681e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.7.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[ 84],
            [124],
            [ 88],
            ...,
            [126],
            [120],
            [ 55]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0067, -0.0084,  0.0003,  ...,  0.0090,  0.0048,  0.0027],
        [ 0.0083,  0.0072,  0.0042,  ...,  0.0078, -0.0036, -0.0029],
        [ 0.0067,  0.0095,  0.0004,  ..., -0.0048, -0.0077, -0.0080],
        ...,
        [-0.0091,  0.0069, -0.0081,  ..., -0.0009,  0.0064, -0.0001],
        [ 0.0012,  0.0056, -0.0059,  ..., -0.0023, -0.0006,  0.0006],
        [ 0.0088,  0.0033,  0.0093,  ..., -0.0011,  0.0005, -0.0028]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.7.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[215],
            [ 97],
            [ 50],
            ...,
            [250],
            [172],
            [168]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-6.8665e-03, -3.6469e-03, -5.2490e-03,  ...,  1.3367e-02,
          8.0872e-04, -1.1536e-02],
        [-8.4305e-04,  7.7820e-03,  6.7749e-03,  ...,  1.0681e-02,
          1.0254e-02,  6.5918e-03],
        [-8.1177e-03,  6.5613e-03, -8.4877e-05,  ...,  7.4158e-03,
          4.7607e-03,  1.4771e-02],
        ...,
        [ 8.8501e-03,  1.0742e-02, -6.2866e-03,  ...,  4.3945e-03,
         -1.5442e-02, -2.5368e-04],
        [ 2.3193e-03, -1.2207e-03,  4.2114e-03,  ..., -6.9275e-03,
         -6.9885e-03,  8.9111e-03],
        [ 2.7161e-03,  1.3733e-02,  6.0120e-03,  ...,  1.2684e-04,
         -8.7891e-03,  1.3916e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.7.input_layernorm.weight Parameter containing:
tensor([0.3223, 0.3652, 0.3379,  ..., 0.3242, 0.3574, 0.3301], device='cuda:0')
base_model.model.model.layers.7.post_attention_layernorm.weight Parameter containing:
tensor([0.2314, 0.2158, 0.2178,  ..., 0.2256, 0.2246, 0.2236], device='cuda:0')
base_model.model.model.layers.8.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[148],
            [ 18],
            [169],
            ...,
            [ 53],
            [130],
            [181]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0139, -0.0044,  0.0017,  ..., -0.0140,  0.0091, -0.0005],
        [ 0.0094, -0.0102, -0.0036,  ...,  0.0043, -0.0013, -0.0090],
        [-0.0114, -0.0082, -0.0089,  ...,  0.0032,  0.0034,  0.0083],
        ...,
        [ 0.0094, -0.0104,  0.0049,  ..., -0.0146, -0.0137,  0.0099],
        [-0.0076,  0.0129,  0.0059,  ..., -0.0046,  0.0063,  0.0079],
        [ 0.0027,  0.0056,  0.0082,  ...,  0.0049,  0.0015,  0.0047]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.8.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 89],
            [ 93],
            [198],
            ...,
            [ 36],
            [ 26],
            [149]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0114,  0.0109, -0.0039,  ..., -0.0139, -0.0120,  0.0023],
        [-0.0002,  0.0136, -0.0051,  ..., -0.0123, -0.0143, -0.0052],
        [ 0.0068,  0.0055,  0.0031,  ...,  0.0031, -0.0077,  0.0012],
        ...,
        [ 0.0106, -0.0153, -0.0136,  ...,  0.0028, -0.0156, -0.0043],
        [-0.0091,  0.0080,  0.0134,  ...,  0.0052, -0.0043,  0.0134],
        [ 0.0081,  0.0074, -0.0148,  ..., -0.0101,  0.0146,  0.0149]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.8.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[ 19],
            [133],
            [114],
            ...,
            [114],
            [ 38],
            [148]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0031,  0.0143,  0.0084,  ..., -0.0035, -0.0049, -0.0019],
        [ 0.0100,  0.0053,  0.0096,  ...,  0.0013, -0.0086, -0.0114],
        [ 0.0137,  0.0010,  0.0118,  ..., -0.0135,  0.0084, -0.0090],
        ...,
        [-0.0092, -0.0090,  0.0156,  ..., -0.0057,  0.0147, -0.0092],
        [-0.0131, -0.0046,  0.0099,  ..., -0.0067,  0.0094,  0.0136],
        [ 0.0010,  0.0003,  0.0008,  ...,  0.0151,  0.0124, -0.0152]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.8.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[204],
            [ 45],
            [178],
            ...,
            [120],
            [127],
            [ 54]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0010, -0.0121,  0.0062,  ...,  0.0012,  0.0095, -0.0063],
        [ 0.0035, -0.0046,  0.0038,  ..., -0.0088, -0.0138,  0.0136],
        [ 0.0036, -0.0045, -0.0062,  ...,  0.0135,  0.0084, -0.0142],
        ...,
        [ 0.0118,  0.0082, -0.0070,  ..., -0.0133, -0.0090, -0.0123],
        [-0.0023,  0.0046,  0.0014,  ..., -0.0003, -0.0023,  0.0127],
        [-0.0109, -0.0087,  0.0121,  ...,  0.0051,  0.0039, -0.0125]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.8.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[152],
            [ 44],
            [153],
            ...,
            [ 52],
            [ 83],
            [183]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0114,  0.0003, -0.0148,  ...,  0.0015,  0.0019,  0.0118],
        [ 0.0090, -0.0107,  0.0143,  ...,  0.0025, -0.0099,  0.0014],
        [ 0.0089, -0.0003,  0.0148,  ...,  0.0142, -0.0063, -0.0149],
        ...,
        [ 0.0125, -0.0028,  0.0029,  ..., -0.0115,  0.0019,  0.0147],
        [-0.0113,  0.0020, -0.0078,  ...,  0.0129,  0.0008, -0.0148],
        [ 0.0120,  0.0045, -0.0131,  ...,  0.0033, -0.0107, -0.0150]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.8.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[ 86],
            [183],
            [168],
            ...,
            [148],
            [167],
            [135]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0079, -0.0014, -0.0022,  ..., -0.0051, -0.0041,  0.0036],
        [ 0.0061, -0.0029, -0.0047,  ..., -0.0007,  0.0069,  0.0049],
        [-0.0029, -0.0005,  0.0092,  ...,  0.0054, -0.0046,  0.0054],
        ...,
        [-0.0048,  0.0085,  0.0035,  ..., -0.0011, -0.0014,  0.0085],
        [ 0.0017, -0.0015, -0.0089,  ..., -0.0090,  0.0017,  0.0052],
        [ 0.0057, -0.0015,  0.0062,  ..., -0.0071,  0.0078, -0.0043]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.8.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 91],
            [ 70],
            [237],
            ...,
            [ 53],
            [148],
            [120]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0110, -0.0140, -0.0146,  ...,  0.0019, -0.0056,  0.0082],
        [ 0.0035,  0.0137,  0.0108,  ...,  0.0098, -0.0121,  0.0004],
        [ 0.0153,  0.0036,  0.0045,  ..., -0.0045,  0.0154,  0.0127],
        ...,
        [ 0.0068, -0.0027, -0.0042,  ...,  0.0082, -0.0078,  0.0068],
        [ 0.0069,  0.0052, -0.0139,  ...,  0.0153, -0.0048, -0.0118],
        [ 0.0030, -0.0024,  0.0048,  ..., -0.0039,  0.0096,  0.0008]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.8.input_layernorm.weight Parameter containing:
tensor([0.3320, 0.3457, 0.3301,  ..., 0.3203, 0.3438, 0.3223], device='cuda:0')
base_model.model.model.layers.8.post_attention_layernorm.weight Parameter containing:
tensor([0.2383, 0.2236, 0.2178,  ..., 0.2363, 0.2285, 0.2256], device='cuda:0')
base_model.model.model.layers.9.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 56],
            [186],
            [178],
            ...,
            [ 17],
            [146],
            [214]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0082,  0.0039, -0.0143,  ..., -0.0058, -0.0049,  0.0065],
        [-0.0043, -0.0079,  0.0081,  ..., -0.0153, -0.0017,  0.0072],
        [ 0.0110, -0.0029, -0.0155,  ..., -0.0087,  0.0145, -0.0135],
        ...,
        [-0.0067,  0.0150, -0.0110,  ..., -0.0015, -0.0125,  0.0095],
        [ 0.0085,  0.0106, -0.0042,  ..., -0.0102, -0.0014,  0.0007],
        [-0.0013, -0.0040, -0.0150,  ...,  0.0132, -0.0067, -0.0143]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.9.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 10],
            [ 58],
            [ 98],
            ...,
            [ 58],
            [ 41],
            [180]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0003,  0.0103, -0.0098,  ..., -0.0063,  0.0060,  0.0110],
        [-0.0121, -0.0142, -0.0087,  ..., -0.0020, -0.0083, -0.0124],
        [-0.0103, -0.0137, -0.0046,  ...,  0.0122,  0.0010, -0.0079],
        ...,
        [-0.0137, -0.0060, -0.0061,  ..., -0.0006, -0.0102, -0.0019],
        [-0.0043, -0.0074,  0.0007,  ...,  0.0031,  0.0107, -0.0077],
        [-0.0110, -0.0020, -0.0049,  ...,  0.0064,  0.0022,  0.0010]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.9.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[106],
            [120],
            [ 53],
            ...,
            [168],
            [131],
            [ 86]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 1.2695e-02, -4.6692e-03, -8.7280e-03,  ...,  1.4832e-02,
         -1.2329e-02, -6.3477e-03],
        [-3.9673e-03, -7.4387e-04, -1.1597e-02,  ...,  8.9722e-03,
         -2.2430e-03,  6.3782e-03],
        [-1.5015e-02,  8.6308e-05,  7.2327e-03,  ...,  7.2327e-03,
          5.1880e-03,  1.3306e-02],
        ...,
        [-1.0620e-02,  7.2632e-03,  1.4404e-02,  ..., -4.2114e-03,
          1.3306e-02,  9.2163e-03],
        [-3.8910e-03,  3.2501e-03,  1.1414e-02,  ...,  2.6703e-03,
          5.6763e-03, -3.3264e-03],
        [-7.8735e-03, -1.3916e-02, -9.9487e-03,  ...,  1.1597e-02,
          1.4160e-02, -1.1841e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.9.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[170],
            [145],
            [173],
            ...,
            [215],
            [229],
            [170]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-7.2632e-03, -1.3550e-02,  1.3489e-02,  ..., -6.4087e-03,
         -1.4526e-02, -5.2490e-03],
        [-4.1199e-03, -5.5847e-03,  6.7444e-03,  ..., -6.0425e-03,
          2.2125e-04,  8.3618e-03],
        [-1.4709e-02,  9.0332e-03,  4.4823e-04,  ...,  4.9973e-04,
          2.4567e-03, -1.0132e-02],
        ...,
        [ 1.7395e-03,  1.5320e-02, -7.8125e-03,  ..., -7.6294e-03,
          8.4229e-03,  3.7842e-03],
        [ 9.8419e-04, -8.1787e-03,  1.1963e-02,  ...,  1.5503e-02,
         -3.8300e-03, -1.3000e-02],
        [ 1.0498e-02, -6.5918e-03,  1.1027e-05,  ..., -1.0315e-02,
          1.0529e-03,  4.6997e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.9.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[ 93],
            [ 89],
            [245],
            ...,
            [ 85],
            [ 58],
            [ 73]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0109, -0.0056, -0.0043,  ...,  0.0134, -0.0128,  0.0019],
        [ 0.0066,  0.0131, -0.0112,  ...,  0.0077, -0.0132,  0.0084],
        [ 0.0108,  0.0076, -0.0134,  ..., -0.0111,  0.0003,  0.0071],
        ...,
        [ 0.0089,  0.0068,  0.0106,  ..., -0.0086,  0.0029, -0.0071],
        [-0.0156,  0.0106, -0.0103,  ...,  0.0006, -0.0003, -0.0020],
        [ 0.0108, -0.0079,  0.0013,  ...,  0.0032, -0.0154,  0.0012]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.9.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[109],
            [  4],
            [231],
            ...,
            [166],
            [174],
            [187]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-1.2817e-03,  8.6670e-03, -1.1861e-05,  ..., -7.7820e-03,
         -7.8735e-03, -2.5330e-03],
        [ 1.3275e-03,  8.4229e-03, -8.7280e-03,  ..., -1.9531e-03,
         -5.4932e-03,  6.5002e-03],
        [ 8.9722e-03, -8.1787e-03,  1.4782e-04,  ..., -7.9346e-03,
         -8.8501e-03, -2.2125e-03],
        ...,
        [ 3.7842e-03, -7.8583e-04,  4.1504e-03,  ...,  2.9755e-04,
          3.7231e-03, -8.6670e-03],
        [ 4.4861e-03,  8.4229e-03,  3.9978e-03,  ...,  2.4719e-03,
         -9.5215e-03, -3.3875e-03],
        [ 8.3618e-03, -1.8845e-03,  7.6294e-04,  ..., -4.7607e-03,
         -1.6327e-03,  6.5994e-04]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.9.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 90],
            [168],
            [117],
            ...,
            [ 67],
            [179],
            [231]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0132,  0.0086, -0.0053,  ...,  0.0025,  0.0041, -0.0017],
        [ 0.0048, -0.0126, -0.0008,  ..., -0.0013, -0.0111,  0.0028],
        [-0.0097,  0.0125,  0.0128,  ...,  0.0085, -0.0141, -0.0039],
        ...,
        [-0.0028,  0.0116, -0.0120,  ..., -0.0005, -0.0148, -0.0065],
        [ 0.0003, -0.0146,  0.0080,  ..., -0.0005, -0.0077,  0.0039],
        [ 0.0112,  0.0128, -0.0148,  ...,  0.0025,  0.0002, -0.0145]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.9.input_layernorm.weight Parameter containing:
tensor([0.3516, 0.3594, 0.3223,  ..., 0.3496, 0.3457, 0.3398], device='cuda:0')
base_model.model.model.layers.9.post_attention_layernorm.weight Parameter containing:
tensor([0.2422, 0.2305, 0.2217,  ..., 0.2373, 0.2363, 0.2324], device='cuda:0')
base_model.model.model.layers.10.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[105],
            [102],
            [233],
            ...,
            [197],
            [166],
            [ 52]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0106,  0.0121,  0.0036,  ..., -0.0123, -0.0150, -0.0057],
        [-0.0109, -0.0032, -0.0123,  ..., -0.0107,  0.0033,  0.0123],
        [-0.0031,  0.0120,  0.0108,  ..., -0.0012, -0.0019,  0.0033],
        ...,
        [-0.0155, -0.0084, -0.0074,  ...,  0.0038, -0.0026,  0.0075],
        [-0.0107,  0.0103, -0.0118,  ...,  0.0090,  0.0149, -0.0133],
        [-0.0072,  0.0063,  0.0045,  ..., -0.0108, -0.0021,  0.0146]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.10.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 11],
            [ 90],
            [106],
            ...,
            [ 20],
            [ 42],
            [229]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0034,  0.0106, -0.0129,  ..., -0.0015,  0.0090, -0.0077],
        [-0.0075,  0.0032,  0.0014,  ..., -0.0043, -0.0074, -0.0084],
        [-0.0003, -0.0114, -0.0138,  ...,  0.0056, -0.0022,  0.0005],
        ...,
        [ 0.0066,  0.0141, -0.0070,  ..., -0.0087,  0.0029,  0.0128],
        [ 0.0063,  0.0077, -0.0060,  ..., -0.0145, -0.0064, -0.0151],
        [-0.0068, -0.0104, -0.0013,  ...,  0.0156, -0.0004, -0.0050]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.10.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[ 24],
            [107],
            [216],
            ...,
            [ 57],
            [198],
            [ 78]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0068, -0.0101, -0.0065,  ..., -0.0059, -0.0053, -0.0132],
        [-0.0139,  0.0020, -0.0131,  ...,  0.0130,  0.0139,  0.0112],
        [ 0.0137,  0.0043, -0.0011,  ..., -0.0140,  0.0145,  0.0143],
        ...,
        [ 0.0107, -0.0008,  0.0018,  ..., -0.0082, -0.0150,  0.0062],
        [-0.0046,  0.0032, -0.0055,  ...,  0.0072,  0.0011,  0.0110],
        [ 0.0141,  0.0014, -0.0053,  ..., -0.0038,  0.0098,  0.0057]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.10.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[103],
            [ 30],
            [ 72],
            ...,
            [176],
            [154],
            [120]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0017, -0.0009,  0.0065,  ...,  0.0008, -0.0063,  0.0078],
        [ 0.0093, -0.0108,  0.0096,  ...,  0.0092, -0.0016,  0.0066],
        [-0.0022,  0.0126, -0.0037,  ..., -0.0153, -0.0021, -0.0075],
        ...,
        [-0.0077, -0.0049,  0.0109,  ...,  0.0052, -0.0099,  0.0018],
        [-0.0009, -0.0090, -0.0057,  ..., -0.0134, -0.0116, -0.0106],
        [-0.0012,  0.0070, -0.0020,  ..., -0.0015,  0.0146, -0.0008]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.10.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[ 16],
            [ 41],
            [ 93],
            ...,
            [147],
            [181],
            [132]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0115, -0.0103, -0.0126,  ..., -0.0020, -0.0063,  0.0110],
        [ 0.0077,  0.0104, -0.0151,  ..., -0.0106, -0.0001,  0.0018],
        [ 0.0015, -0.0039,  0.0115,  ..., -0.0086, -0.0114, -0.0071],
        ...,
        [-0.0090, -0.0019, -0.0144,  ..., -0.0151, -0.0026,  0.0103],
        [ 0.0013, -0.0070, -0.0103,  ...,  0.0037,  0.0085, -0.0026],
        [ 0.0076,  0.0095, -0.0012,  ...,  0.0134, -0.0085,  0.0025]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.10.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[ 82],
            [109],
            [150],
            ...,
            [ 59],
            [180],
            [ 73]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 2.4567e-03, -5.5237e-03,  4.6692e-03,  ...,  8.9111e-03,
          8.4229e-03, -6.0797e-05],
        [ 2.8801e-04,  3.2043e-03,  5.3101e-03,  ...,  4.3030e-03,
          6.0120e-03,  5.7983e-03],
        [ 4.3030e-03, -1.3199e-03, -7.5989e-03,  ..., -3.9368e-03,
         -1.9264e-04,  5.0354e-03],
        ...,
        [ 9.5215e-03, -5.3406e-03,  1.4877e-03,  ..., -1.0192e-05,
         -7.4768e-03,  1.6403e-03],
        [ 3.9368e-03,  9.4986e-04,  8.3618e-03,  ...,  7.0190e-03,
          7.4768e-03, -6.0425e-03],
        [-8.6060e-03, -7.7438e-04, -6.8970e-03,  ..., -7.0572e-04,
          3.1128e-03,  8.8501e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.10.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 22],
            [ 21],
            [ 87],
            ...,
            [  4],
            [215],
            [155]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0085, -0.0126,  0.0056,  ...,  0.0103,  0.0128,  0.0009],
        [-0.0138, -0.0114, -0.0018,  ...,  0.0073,  0.0125, -0.0067],
        [-0.0111, -0.0014,  0.0098,  ..., -0.0049,  0.0018, -0.0008],
        ...,
        [-0.0028, -0.0037, -0.0023,  ...,  0.0041, -0.0028,  0.0151],
        [ 0.0053,  0.0081,  0.0118,  ...,  0.0076, -0.0116, -0.0151],
        [-0.0035, -0.0048,  0.0082,  ..., -0.0121, -0.0137,  0.0120]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.10.input_layernorm.weight Parameter containing:
tensor([0.3633, 0.3594, 0.3203,  ..., 0.3398, 0.3477, 0.3359], device='cuda:0')
base_model.model.model.layers.10.post_attention_layernorm.weight Parameter containing:
tensor([0.2461, 0.2324, 0.2236,  ..., 0.2402, 0.2373, 0.2373], device='cuda:0')
base_model.model.model.layers.11.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[205],
            [ 82],
            [ 34],
            ...,
            [156],
            [244],
            [222]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0107,  0.0015,  0.0016,  ...,  0.0040,  0.0063, -0.0123],
        [-0.0019,  0.0064, -0.0016,  ...,  0.0007,  0.0090,  0.0115],
        [-0.0030, -0.0029,  0.0005,  ...,  0.0139, -0.0148,  0.0036],
        ...,
        [ 0.0012, -0.0036,  0.0096,  ..., -0.0052, -0.0079,  0.0004],
        [-0.0086, -0.0095, -0.0036,  ..., -0.0099, -0.0092,  0.0092],
        [-0.0084, -0.0099, -0.0092,  ...,  0.0082, -0.0021, -0.0156]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.11.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[140],
            [117],
            [139],
            ...,
            [187],
            [150],
            [251]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-9.4414e-05,  1.5320e-02,  7.5684e-03,  ...,  5.9204e-03,
          1.0529e-03,  1.2207e-02],
        [ 7.3242e-04, -8.0566e-03, -3.4790e-03,  ..., -1.2939e-02,
          8.0566e-03,  1.1841e-02],
        [ 1.0147e-03,  1.1292e-02, -7.4768e-03,  ..., -2.1210e-03,
         -1.2939e-02, -1.1047e-02],
        ...,
        [-3.0975e-03, -1.4587e-02,  1.3916e-02,  ..., -6.0425e-03,
         -8.7280e-03, -6.8970e-03],
        [-3.7994e-03,  1.4709e-02, -5.7068e-03,  ..., -8.5068e-04,
          4.3106e-04,  5.0049e-03],
        [-6.8970e-03, -2.8381e-03,  2.5635e-03,  ...,  5.8594e-03,
          4.9133e-03, -2.1820e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.11.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[136],
            [ 84],
            [149],
            ...,
            [103],
            [128],
            [100]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0068, -0.0048,  0.0121,  ...,  0.0082, -0.0021,  0.0072],
        [-0.0061, -0.0105,  0.0118,  ...,  0.0084, -0.0101, -0.0103],
        [ 0.0114,  0.0039, -0.0155,  ..., -0.0072,  0.0084, -0.0147],
        ...,
        [ 0.0065,  0.0115, -0.0073,  ...,  0.0082, -0.0044, -0.0085],
        [ 0.0140, -0.0052,  0.0118,  ...,  0.0151,  0.0047,  0.0076],
        [-0.0040, -0.0121,  0.0092,  ..., -0.0043, -0.0140, -0.0068]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.11.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[ 23],
            [160],
            [  1],
            ...,
            [193],
            [ 85],
            [116]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0037,  0.0029, -0.0096,  ...,  0.0114,  0.0003, -0.0049],
        [-0.0100, -0.0143,  0.0125,  ..., -0.0123, -0.0107, -0.0081],
        [ 0.0094, -0.0093, -0.0025,  ...,  0.0146, -0.0079, -0.0050],
        ...,
        [-0.0036, -0.0098,  0.0106,  ...,  0.0082, -0.0121, -0.0138],
        [ 0.0036, -0.0023,  0.0092,  ..., -0.0059,  0.0090, -0.0098],
        [-0.0028, -0.0132, -0.0020,  ...,  0.0015,  0.0093, -0.0118]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.11.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[  5],
            [119],
            [169],
            ...,
            [ 88],
            [141],
            [100]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0121, -0.0129,  0.0069,  ..., -0.0141, -0.0095, -0.0026],
        [ 0.0004,  0.0156,  0.0049,  ..., -0.0131, -0.0073, -0.0138],
        [ 0.0018,  0.0049, -0.0076,  ...,  0.0115,  0.0133, -0.0058],
        ...,
        [ 0.0019,  0.0092, -0.0043,  ..., -0.0128, -0.0139,  0.0057],
        [-0.0138, -0.0156, -0.0080,  ...,  0.0078, -0.0014,  0.0004],
        [ 0.0023,  0.0031,  0.0023,  ...,  0.0129,  0.0104, -0.0014]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.11.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[ 63],
            [183],
            [166],
            ...,
            [198],
            [  8],
            [ 79]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0076, -0.0023,  0.0056,  ...,  0.0055,  0.0005,  0.0026],
        [ 0.0018, -0.0022, -0.0038,  ...,  0.0073,  0.0013, -0.0055],
        [-0.0049,  0.0022,  0.0060,  ..., -0.0048, -0.0094,  0.0076],
        ...,
        [ 0.0065, -0.0023, -0.0049,  ...,  0.0005, -0.0063,  0.0055],
        [ 0.0001, -0.0068,  0.0070,  ..., -0.0078, -0.0048,  0.0010],
        [ 0.0037, -0.0056, -0.0027,  ...,  0.0016,  0.0070,  0.0064]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.11.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 87],
            [ 45],
            [ 57],
            ...,
            [187],
            [ 68],
            [157]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0136,  0.0116, -0.0100,  ..., -0.0155, -0.0043,  0.0024],
        [-0.0002,  0.0140, -0.0040,  ..., -0.0103,  0.0002, -0.0073],
        [ 0.0072,  0.0049,  0.0067,  ..., -0.0033,  0.0116, -0.0109],
        ...,
        [-0.0142,  0.0078, -0.0081,  ...,  0.0143,  0.0062, -0.0067],
        [ 0.0072, -0.0090, -0.0135,  ..., -0.0121,  0.0107,  0.0055],
        [-0.0010,  0.0050, -0.0102,  ...,  0.0098,  0.0090,  0.0051]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.11.input_layernorm.weight Parameter containing:
tensor([0.3945, 0.3926, 0.3594,  ..., 0.3828, 0.3770, 0.3672], device='cuda:0')
base_model.model.model.layers.11.post_attention_layernorm.weight Parameter containing:
tensor([0.2539, 0.2363, 0.2334,  ..., 0.2500, 0.2490, 0.2451], device='cuda:0')
base_model.model.model.layers.12.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 81],
            [185],
            [211],
            ...,
            [165],
            [116],
            [188]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0041, -0.0081,  0.0062,  ...,  0.0094,  0.0122, -0.0120],
        [ 0.0106, -0.0120,  0.0118,  ...,  0.0114,  0.0038, -0.0153],
        [ 0.0105,  0.0056,  0.0085,  ...,  0.0111,  0.0067, -0.0015],
        ...,
        [-0.0089, -0.0015, -0.0104,  ..., -0.0027,  0.0025,  0.0084],
        [ 0.0125,  0.0090, -0.0150,  ...,  0.0133,  0.0118,  0.0021],
        [-0.0120, -0.0031,  0.0142,  ...,  0.0024, -0.0034, -0.0043]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.12.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[181],
            [103],
            [ 64],
            ...,
            [169],
            [105],
            [ 35]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-1.4038e-02, -9.3994e-03, -7.3547e-03,  ...,  5.4626e-03,
          1.4709e-02,  2.2430e-03],
        [-1.3245e-02,  1.4465e-02, -1.2573e-02,  ...,  9.7046e-03,
          8.2016e-05, -1.2451e-02],
        [-1.1047e-02, -7.0496e-03,  3.1586e-03,  ...,  8.5831e-04,
          2.2888e-03, -1.3794e-02],
        ...,
        [-5.8594e-03,  1.1536e-02,  9.0942e-03,  ..., -1.3885e-03,
          4.3030e-03, -5.3101e-03],
        [ 6.3171e-03, -1.3123e-02,  9.3994e-03,  ..., -1.5137e-02,
          1.1492e-04,  4.9133e-03],
        [-4.3030e-03,  6.8359e-03,  1.1063e-03,  ...,  8.3618e-03,
         -2.0599e-03,  8.6060e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.12.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[153],
            [137],
            [101],
            ...,
            [202],
            [ 77],
            [148]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0126, -0.0029,  0.0087,  ..., -0.0048, -0.0052,  0.0135],
        [-0.0071, -0.0020,  0.0031,  ..., -0.0106,  0.0034,  0.0089],
        [-0.0154, -0.0032, -0.0083,  ...,  0.0121, -0.0099, -0.0125],
        ...,
        [-0.0035, -0.0028, -0.0124,  ...,  0.0031,  0.0129,  0.0134],
        [ 0.0131,  0.0018, -0.0084,  ...,  0.0005,  0.0010, -0.0089],
        [ 0.0057, -0.0023,  0.0129,  ...,  0.0153,  0.0131,  0.0091]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.12.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[233],
            [116],
            [218],
            ...,
            [197],
            [222],
            [ 50]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0024,  0.0146, -0.0117,  ...,  0.0112, -0.0137, -0.0090],
        [ 0.0156,  0.0003,  0.0142,  ..., -0.0121,  0.0064,  0.0070],
        [-0.0116,  0.0078, -0.0079,  ...,  0.0118, -0.0090, -0.0076],
        ...,
        [-0.0020,  0.0014,  0.0114,  ...,  0.0005, -0.0102, -0.0129],
        [-0.0135, -0.0085,  0.0022,  ..., -0.0051, -0.0117,  0.0143],
        [-0.0077,  0.0020,  0.0108,  ...,  0.0008, -0.0051, -0.0095]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.12.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[147],
            [147],
            [179],
            ...,
            [100],
            [198],
            [ 83]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0121, -0.0106,  0.0153,  ...,  0.0104,  0.0073,  0.0112],
        [-0.0004,  0.0058,  0.0080,  ...,  0.0038, -0.0034, -0.0155],
        [ 0.0027,  0.0014, -0.0095,  ...,  0.0004,  0.0101, -0.0156],
        ...,
        [ 0.0155, -0.0064, -0.0146,  ..., -0.0059, -0.0018,  0.0043],
        [ 0.0142, -0.0148,  0.0083,  ..., -0.0154, -0.0031,  0.0070],
        [ 0.0135,  0.0099,  0.0030,  ...,  0.0120, -0.0063, -0.0010]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.12.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[133],
            [ 26],
            [139],
            ...,
            [218],
            [ 64],
            [198]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0079,  0.0091,  0.0095,  ...,  0.0032,  0.0067,  0.0033],
        [-0.0074, -0.0067,  0.0080,  ...,  0.0080, -0.0039,  0.0014],
        [-0.0067,  0.0070,  0.0004,  ..., -0.0031,  0.0054,  0.0009],
        ...,
        [ 0.0028, -0.0009,  0.0019,  ...,  0.0065,  0.0029, -0.0032],
        [-0.0008, -0.0058,  0.0052,  ..., -0.0003,  0.0025, -0.0062],
        [ 0.0032,  0.0029,  0.0088,  ...,  0.0079, -0.0017, -0.0042]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.12.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[139],
            [138],
            [137],
            ...,
            [ 91],
            [ 94],
            [ 97]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-2.8534e-03,  1.3489e-02, -1.4160e-02,  ..., -1.0437e-02,
         -1.3611e-02,  8.6060e-03],
        [-4.4107e-06, -9.1553e-03,  2.6703e-03,  ..., -3.2654e-03,
         -3.5667e-04, -3.3112e-03],
        [-1.2512e-02, -1.0864e-02, -7.5378e-03,  ..., -8.5449e-03,
          2.2736e-03, -4.3945e-03],
        ...,
        [ 1.0681e-02, -1.0559e-02,  6.9885e-03,  ...,  1.4221e-02,
         -2.5177e-03, -5.2185e-03],
        [ 9.2773e-03,  2.9602e-03,  8.5449e-03,  ..., -1.1292e-02,
         -1.2573e-02, -5.8899e-03],
        [ 2.7657e-05,  1.0681e-02,  8.9722e-03,  ..., -2.3193e-03,
         -7.0190e-04, -1.5137e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.12.input_layernorm.weight Parameter containing:
tensor([0.4023, 0.3984, 0.3652,  ..., 0.3789, 0.3828, 0.3848], device='cuda:0')
base_model.model.model.layers.12.post_attention_layernorm.weight Parameter containing:
tensor([0.2578, 0.2441, 0.2373,  ..., 0.2559, 0.2539, 0.2539], device='cuda:0')
base_model.model.model.layers.13.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[101],
            [ 79],
            [176],
            ...,
            [166],
            [133],
            [182]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0042, -0.0055, -0.0064,  ..., -0.0105, -0.0151,  0.0022],
        [ 0.0003,  0.0019, -0.0003,  ..., -0.0099,  0.0117,  0.0154],
        [-0.0096, -0.0132, -0.0023,  ...,  0.0045,  0.0098,  0.0066],
        ...,
        [-0.0151,  0.0107,  0.0132,  ...,  0.0062,  0.0154, -0.0139],
        [-0.0114,  0.0082,  0.0139,  ..., -0.0131, -0.0027, -0.0071],
        [-0.0055, -0.0139,  0.0154,  ..., -0.0116, -0.0025, -0.0039]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.13.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[185],
            [107],
            [ 90],
            ...,
            [136],
            [ 84],
            [ 88]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0031, -0.0104,  0.0076,  ...,  0.0141,  0.0139, -0.0017],
        [-0.0009,  0.0118,  0.0022,  ..., -0.0091, -0.0150,  0.0084],
        [-0.0067, -0.0114, -0.0054,  ..., -0.0024,  0.0049,  0.0082],
        ...,
        [-0.0005, -0.0100,  0.0072,  ...,  0.0154, -0.0025,  0.0146],
        [ 0.0156,  0.0065,  0.0146,  ..., -0.0096, -0.0115,  0.0047],
        [-0.0041, -0.0028,  0.0057,  ...,  0.0045, -0.0075,  0.0034]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.13.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[200],
            [ 61],
            [ 86],
            ...,
            [ 23],
            [ 75],
            [126]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0139, -0.0009, -0.0067,  ...,  0.0032,  0.0143, -0.0106],
        [-0.0008,  0.0011, -0.0059,  ...,  0.0151, -0.0138,  0.0083],
        [-0.0083, -0.0017, -0.0084,  ...,  0.0081,  0.0069,  0.0052],
        ...,
        [ 0.0053, -0.0098,  0.0137,  ...,  0.0084, -0.0139, -0.0006],
        [-0.0036, -0.0150, -0.0031,  ...,  0.0142,  0.0081, -0.0050],
        [ 0.0127, -0.0150, -0.0112,  ..., -0.0061,  0.0138, -0.0016]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.13.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[103],
            [ 83],
            [ 70],
            ...,
            [ 73],
            [108],
            [ 65]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-1.0803e-02,  3.2196e-03,  9.7046e-03,  ..., -5.0659e-03,
          1.5259e-02,  8.2397e-03],
        [ 1.1841e-02, -8.8501e-03,  3.5095e-04,  ..., -3.0212e-03,
          3.5858e-03, -6.7749e-03],
        [ 1.5564e-02,  2.8229e-03,  2.1338e-05,  ..., -9.3384e-03,
          5.1880e-03,  8.6670e-03],
        ...,
        [-4.6997e-03, -4.3030e-03, -4.4556e-03,  ..., -7.7209e-03,
          8.2970e-05, -1.3428e-03],
        [ 6.5994e-04,  1.5381e-02,  8.9722e-03,  ..., -6.8970e-03,
         -1.3245e-02, -6.7139e-03],
        [-1.2695e-02,  2.2278e-03,  1.0986e-02,  ...,  5.1880e-04,
         -3.7994e-03,  1.8387e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.13.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[218],
            [ 84],
            [ 57],
            ...,
            [182],
            [ 22],
            [222]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0135, -0.0027,  0.0077,  ...,  0.0128,  0.0043,  0.0128],
        [ 0.0134,  0.0109, -0.0101,  ..., -0.0098, -0.0100,  0.0036],
        [-0.0104, -0.0069, -0.0084,  ..., -0.0094, -0.0024, -0.0084],
        ...,
        [-0.0071, -0.0146,  0.0045,  ..., -0.0068, -0.0079, -0.0052],
        [ 0.0096, -0.0009, -0.0017,  ...,  0.0024, -0.0048, -0.0055],
        [ 0.0115, -0.0086,  0.0060,  ..., -0.0006,  0.0091,  0.0151]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.13.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[ 86],
            [104],
            [158],
            ...,
            [ 85],
            [136],
            [142]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0085, -0.0018,  0.0038,  ...,  0.0074,  0.0079,  0.0016],
        [-0.0010, -0.0009,  0.0009,  ...,  0.0056, -0.0031,  0.0041],
        [ 0.0084,  0.0075, -0.0083,  ...,  0.0051,  0.0078, -0.0085],
        ...,
        [-0.0040, -0.0033, -0.0056,  ...,  0.0036,  0.0084, -0.0012],
        [-0.0086,  0.0001,  0.0087,  ..., -0.0004,  0.0090, -0.0061],
        [-0.0070,  0.0028, -0.0044,  ...,  0.0053,  0.0069, -0.0022]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.13.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 28],
            [ 85],
            [ 39],
            ...,
            [  3],
            [115],
            [124]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0014, -0.0027,  0.0085,  ..., -0.0029,  0.0153, -0.0086],
        [-0.0070, -0.0004,  0.0141,  ..., -0.0121,  0.0018, -0.0106],
        [-0.0018,  0.0132, -0.0070,  ..., -0.0082,  0.0022, -0.0134],
        ...,
        [ 0.0074, -0.0146, -0.0042,  ..., -0.0117,  0.0147, -0.0097],
        [-0.0044,  0.0068,  0.0054,  ..., -0.0034,  0.0076, -0.0130],
        [ 0.0011, -0.0125, -0.0095,  ...,  0.0117, -0.0109, -0.0084]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.13.input_layernorm.weight Parameter containing:
tensor([0.4141, 0.4043, 0.3711,  ..., 0.3867, 0.3809, 0.3906], device='cuda:0')
base_model.model.model.layers.13.post_attention_layernorm.weight Parameter containing:
tensor([0.2637, 0.2520, 0.2451,  ..., 0.2637, 0.2656, 0.2598], device='cuda:0')
base_model.model.model.layers.14.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 87],
            [198],
            [ 82],
            ...,
            [125],
            [201],
            [145]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 1.1597e-02, -6.3477e-03,  1.1353e-02,  ...,  1.4771e-02,
          1.4526e-02, -5.0049e-03],
        [-4.7874e-04, -4.3335e-03,  1.0986e-03,  ...,  5.4932e-03,
         -4.4861e-03, -5.7678e-03],
        [-1.0193e-02, -1.3916e-02,  5.1880e-03,  ...,  1.4282e-02,
         -7.1716e-04, -1.3123e-02],
        ...,
        [ 1.2207e-02,  5.9204e-03,  1.2329e-02,  ...,  1.5020e-05,
          5.1117e-04,  5.0049e-03],
        [-1.1658e-02, -1.2146e-02,  1.1536e-02,  ...,  1.2390e-02,
          5.6152e-03,  2.1973e-03],
        [ 7.0801e-03, -6.7749e-03, -5.3883e-05,  ...,  1.1414e-02,
          8.9722e-03, -1.3123e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.14.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 70],
            [151],
            [ 74],
            ...,
            [ 87],
            [175],
            [147]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 1.4648e-02,  8.4839e-03, -1.1841e-02,  ...,  2.9907e-03,
          5.5237e-03, -1.2817e-02],
        [-8.8501e-03, -1.0803e-02, -6.7139e-03,  ...,  2.8687e-03,
          1.1673e-03, -4.5776e-03],
        [-3.2349e-03,  1.1963e-02,  1.1230e-02,  ...,  5.9843e-05,
          7.2021e-03, -5.1575e-03],
        ...,
        [ 9.6436e-03, -1.2207e-02, -1.1536e-02,  ...,  1.2268e-02,
          1.0803e-02, -5.4932e-03],
        [ 6.2561e-03, -9.1553e-04,  1.5640e-03,  ..., -7.4768e-03,
         -2.7466e-03, -6.0425e-03],
        [ 1.5625e-02, -3.3188e-04, -6.2866e-03,  ..., -1.2451e-02,
         -4.9438e-03, -6.7749e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.14.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[169],
            [  5],
            [103],
            ...,
            [166],
            [117],
            [ 92]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0107,  0.0050, -0.0033,  ...,  0.0005,  0.0073,  0.0134],
        [-0.0098, -0.0083, -0.0147,  ..., -0.0121,  0.0009,  0.0036],
        [ 0.0123,  0.0003,  0.0067,  ...,  0.0149, -0.0016,  0.0008],
        ...,
        [ 0.0074, -0.0081, -0.0009,  ...,  0.0021,  0.0065, -0.0131],
        [ 0.0113,  0.0035, -0.0069,  ..., -0.0037, -0.0065,  0.0041],
        [-0.0126,  0.0060,  0.0137,  ..., -0.0059,  0.0034, -0.0119]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.14.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[ 57],
            [ 20],
            [140],
            ...,
            [ 45],
            [197],
            [152]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0123,  0.0094,  0.0051,  ..., -0.0109, -0.0090, -0.0066],
        [ 0.0088, -0.0100,  0.0058,  ..., -0.0103, -0.0050,  0.0101],
        [ 0.0128, -0.0066, -0.0039,  ..., -0.0081, -0.0146,  0.0044],
        ...,
        [-0.0120,  0.0084,  0.0008,  ..., -0.0031, -0.0102,  0.0045],
        [-0.0148, -0.0046, -0.0086,  ...,  0.0083, -0.0156, -0.0120],
        [-0.0149,  0.0105,  0.0104,  ..., -0.0060,  0.0029,  0.0002]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.14.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[58],
            [72],
            [19],
            ...,
            [69],
            [37],
            [29]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0125,  0.0047,  0.0099,  ..., -0.0035,  0.0086, -0.0154],
        [ 0.0024,  0.0115,  0.0135,  ...,  0.0148,  0.0057, -0.0043],
        [-0.0035,  0.0118, -0.0071,  ..., -0.0025,  0.0045, -0.0112],
        ...,
        [-0.0004, -0.0145, -0.0121,  ...,  0.0008, -0.0115,  0.0081],
        [ 0.0083,  0.0026, -0.0068,  ...,  0.0120, -0.0018, -0.0082],
        [ 0.0133,  0.0072, -0.0115,  ..., -0.0142, -0.0028,  0.0014]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.14.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[155],
            [134],
            [201],
            ...,
            [  2],
            [ 81],
            [172]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0049,  0.0008, -0.0014,  ...,  0.0028, -0.0057, -0.0062],
        [-0.0010, -0.0085, -0.0034,  ..., -0.0019, -0.0003,  0.0057],
        [-0.0028,  0.0077, -0.0050,  ...,  0.0017, -0.0033, -0.0084],
        ...,
        [-0.0043, -0.0009,  0.0047,  ...,  0.0062, -0.0054, -0.0008],
        [-0.0047,  0.0047,  0.0050,  ...,  0.0077, -0.0031,  0.0046],
        [-0.0025, -0.0048, -0.0022,  ..., -0.0033,  0.0095, -0.0035]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.14.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[121],
            [168],
            [103],
            ...,
            [ 35],
            [ 91],
            [ 58]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0037,  0.0021, -0.0080,  ..., -0.0069,  0.0128, -0.0024],
        [ 0.0131, -0.0002, -0.0045,  ..., -0.0095,  0.0069,  0.0096],
        [-0.0060,  0.0127,  0.0037,  ..., -0.0070,  0.0085,  0.0103],
        ...,
        [ 0.0099, -0.0012, -0.0027,  ...,  0.0123, -0.0064, -0.0023],
        [-0.0031,  0.0063,  0.0110,  ...,  0.0114, -0.0099, -0.0120],
        [-0.0139, -0.0010,  0.0078,  ...,  0.0148, -0.0060, -0.0098]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.14.input_layernorm.weight Parameter containing:
tensor([0.4160, 0.4258, 0.3750,  ..., 0.4062, 0.3984, 0.3887], device='cuda:0')
base_model.model.model.layers.14.post_attention_layernorm.weight Parameter containing:
tensor([0.2734, 0.2617, 0.2598,  ..., 0.2773, 0.2734, 0.2695], device='cuda:0')
base_model.model.model.layers.15.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 82],
            [ 88],
            [226],
            ...,
            [ 93],
            [167],
            [ 69]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0110,  0.0051, -0.0070,  ..., -0.0069,  0.0153,  0.0041],
        [ 0.0018, -0.0088, -0.0131,  ..., -0.0103,  0.0077,  0.0154],
        [-0.0015,  0.0044, -0.0008,  ..., -0.0130,  0.0029, -0.0140],
        ...,
        [ 0.0025,  0.0070,  0.0146,  ..., -0.0145,  0.0005, -0.0130],
        [-0.0006, -0.0084, -0.0041,  ...,  0.0002, -0.0003, -0.0064],
        [-0.0093, -0.0064,  0.0058,  ...,  0.0057,  0.0136,  0.0135]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.15.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[213],
            [ 79],
            [244],
            ...,
            [121],
            [ 55],
            [ 87]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0049,  0.0128, -0.0064,  ...,  0.0130,  0.0139,  0.0117],
        [-0.0045, -0.0051,  0.0053,  ...,  0.0087,  0.0087,  0.0109],
        [-0.0093,  0.0033, -0.0145,  ..., -0.0096,  0.0144, -0.0050],
        ...,
        [ 0.0019, -0.0051, -0.0126,  ..., -0.0098,  0.0109, -0.0078],
        [ 0.0022,  0.0150, -0.0132,  ...,  0.0053, -0.0063,  0.0122],
        [-0.0082, -0.0068, -0.0087,  ..., -0.0051,  0.0092,  0.0152]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.15.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[151],
            [134],
            [158],
            ...,
            [199],
            [178],
            [137]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0119, -0.0023, -0.0006,  ..., -0.0010,  0.0065, -0.0107],
        [-0.0097, -0.0033,  0.0117,  ..., -0.0123, -0.0119,  0.0074],
        [-0.0051,  0.0044,  0.0009,  ...,  0.0055,  0.0093,  0.0032],
        ...,
        [ 0.0041,  0.0067, -0.0093,  ...,  0.0081,  0.0082,  0.0067],
        [ 0.0114,  0.0028,  0.0059,  ...,  0.0095,  0.0118,  0.0031],
        [-0.0079,  0.0107,  0.0123,  ...,  0.0075, -0.0131,  0.0017]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.15.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[138],
            [ 48],
            [ 29],
            ...,
            [135],
            [150],
            [ 17]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0024, -0.0003,  0.0143,  ..., -0.0039,  0.0135, -0.0030],
        [-0.0015,  0.0020, -0.0092,  ..., -0.0021,  0.0080, -0.0042],
        [ 0.0137, -0.0103, -0.0023,  ...,  0.0144, -0.0094, -0.0070],
        ...,
        [ 0.0111,  0.0031,  0.0015,  ..., -0.0007, -0.0117,  0.0105],
        [-0.0123,  0.0148, -0.0081,  ...,  0.0091, -0.0137, -0.0039],
        [-0.0134,  0.0080, -0.0122,  ..., -0.0074,  0.0102,  0.0003]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.15.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[ 94],
            [ 52],
            [106],
            ...,
            [  7],
            [103],
            [121]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0094, -0.0146, -0.0103,  ..., -0.0108,  0.0127, -0.0127],
        [ 0.0117,  0.0084,  0.0131,  ..., -0.0134,  0.0031,  0.0080],
        [-0.0066, -0.0003, -0.0131,  ...,  0.0016,  0.0043,  0.0016],
        ...,
        [-0.0012,  0.0039, -0.0095,  ..., -0.0001, -0.0082, -0.0061],
        [-0.0093,  0.0015, -0.0054,  ..., -0.0025, -0.0049,  0.0046],
        [-0.0053,  0.0034,  0.0055,  ...,  0.0013,  0.0052, -0.0118]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.15.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[ 83],
            [ 21],
            [105],
            ...,
            [196],
            [132],
            [ 34]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0028,  0.0015,  0.0081,  ...,  0.0026,  0.0070, -0.0008],
        [ 0.0006, -0.0012, -0.0067,  ...,  0.0040,  0.0038, -0.0081],
        [-0.0059, -0.0004, -0.0042,  ..., -0.0020,  0.0009, -0.0054],
        ...,
        [-0.0013,  0.0043, -0.0033,  ...,  0.0034, -0.0010,  0.0064],
        [ 0.0085, -0.0050,  0.0065,  ...,  0.0001, -0.0090,  0.0025],
        [ 0.0036, -0.0094,  0.0032,  ...,  0.0086, -0.0038,  0.0095]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.15.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 85],
            [199],
            [ 34],
            ...,
            [ 53],
            [216],
            [ 55]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0033,  0.0085, -0.0040,  ..., -0.0093,  0.0128, -0.0128],
        [ 0.0079,  0.0029,  0.0036,  ...,  0.0067,  0.0079, -0.0032],
        [ 0.0142, -0.0146, -0.0013,  ..., -0.0150,  0.0019, -0.0001],
        ...,
        [ 0.0110, -0.0136,  0.0096,  ...,  0.0123, -0.0044,  0.0029],
        [-0.0029, -0.0133, -0.0028,  ...,  0.0125,  0.0153, -0.0042],
        [ 0.0042, -0.0062, -0.0067,  ...,  0.0061, -0.0095,  0.0069]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.15.input_layernorm.weight Parameter containing:
tensor([0.4062, 0.4004, 0.3770,  ..., 0.3848, 0.3848, 0.3887], device='cuda:0')
base_model.model.model.layers.15.post_attention_layernorm.weight Parameter containing:
tensor([0.2852, 0.2715, 0.2715,  ..., 0.2852, 0.2812, 0.2812], device='cuda:0')
base_model.model.model.layers.16.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[169],
            [ 34],
            [208],
            ...,
            [123],
            [ 54],
            [ 53]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 1.0498e-02,  2.7008e-03, -1.4404e-02,  ...,  5.5237e-03,
         -8.2397e-04,  2.3842e-04],
        [-4.5166e-03, -1.9360e-04,  7.1716e-03,  ..., -9.3384e-03,
          6.8359e-03,  1.3062e-02],
        [-1.1597e-02, -3.0365e-03,  6.8054e-03,  ..., -7.5073e-03,
          8.5449e-03, -7.1228e-06],
        ...,
        [-5.1880e-03,  7.9956e-03,  5.5237e-03,  ...,  1.9836e-04,
         -2.1667e-03, -6.3171e-03],
        [ 4.9744e-03, -2.5177e-03, -1.4221e-02,  ..., -7.1411e-03,
         -7.6599e-03, -4.7607e-03],
        [ 1.1780e-02,  1.3580e-03,  7.8125e-03,  ...,  4.2419e-03,
         -1.2684e-04, -5.2490e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.16.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 60],
            [151],
            [125],
            ...,
            [ 71],
            [119],
            [ 92]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 8.0566e-03, -6.3171e-03,  7.2937e-03,  ..., -5.4626e-03,
         -1.5442e-02, -1.1292e-03],
        [ 1.4648e-02, -1.3000e-02,  4.5166e-03,  ..., -1.2878e-02,
         -1.2024e-02,  7.8735e-03],
        [-5.4321e-03,  8.9407e-06, -1.3794e-02,  ...,  2.5024e-03,
         -4.5776e-03, -9.3384e-03],
        ...,
        [-4.6082e-03,  1.5076e-02, -1.0681e-02,  ...,  1.1841e-02,
         -8.8501e-03, -2.2125e-03],
        [ 9.4604e-03, -1.4099e-02,  2.8992e-03,  ...,  2.6550e-03,
         -3.3112e-03, -4.9133e-03],
        [ 9.9487e-03, -9.9487e-03,  9.1553e-03,  ...,  1.0742e-02,
          5.4932e-03, -1.4343e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.16.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[ 88],
            [ 99],
            [ 76],
            ...,
            [126],
            [180],
            [113]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0038,  0.0073,  0.0045,  ..., -0.0049, -0.0137,  0.0099],
        [ 0.0060, -0.0151, -0.0118,  ..., -0.0092, -0.0057, -0.0104],
        [-0.0148,  0.0067,  0.0013,  ..., -0.0024,  0.0131, -0.0102],
        ...,
        [ 0.0123,  0.0005, -0.0051,  ...,  0.0067, -0.0109, -0.0001],
        [ 0.0155,  0.0067,  0.0083,  ...,  0.0137, -0.0151,  0.0006],
        [ 0.0119,  0.0122, -0.0145,  ..., -0.0050,  0.0145, -0.0098]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.16.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[231],
            [ 70],
            [ 57],
            ...,
            [ 87],
            [215],
            [103]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0027,  0.0075, -0.0046,  ...,  0.0128,  0.0154, -0.0043],
        [-0.0029, -0.0154,  0.0114,  ...,  0.0099,  0.0145,  0.0057],
        [-0.0098,  0.0113, -0.0084,  ...,  0.0006,  0.0147,  0.0107],
        ...,
        [-0.0096, -0.0130, -0.0125,  ..., -0.0125, -0.0149,  0.0153],
        [-0.0128,  0.0145, -0.0098,  ...,  0.0054,  0.0076,  0.0107],
        [-0.0121, -0.0004, -0.0038,  ...,  0.0121, -0.0074,  0.0066]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.16.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[ 51],
            [ 45],
            [ 62],
            ...,
            [121],
            [113],
            [197]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0064, -0.0038,  0.0046,  ...,  0.0029, -0.0051,  0.0004],
        [-0.0062, -0.0131,  0.0123,  ...,  0.0118, -0.0047,  0.0125],
        [-0.0129,  0.0055, -0.0058,  ..., -0.0028, -0.0032,  0.0116],
        ...,
        [-0.0037,  0.0145,  0.0074,  ..., -0.0025, -0.0051, -0.0087],
        [-0.0082, -0.0148,  0.0093,  ...,  0.0120, -0.0046, -0.0023],
        [-0.0114,  0.0031,  0.0156,  ..., -0.0025, -0.0039, -0.0110]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.16.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[ 55],
            [188],
            [ 53],
            ...,
            [135],
            [125],
            [132]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 6.5308e-03,  7.5378e-03, -2.6398e-03,  ...,  1.0986e-03,
          6.8665e-03, -2.8839e-03],
        [ 9.3079e-04, -8.9722e-03, -4.4250e-03,  ..., -8.9645e-04,
          9.2163e-03,  1.6861e-03],
        [ 2.4719e-03, -7.5684e-03, -8.9722e-03,  ...,  3.1662e-04,
          3.6926e-03,  5.0659e-03],
        ...,
        [ 2.8687e-03, -4.0588e-03,  6.0730e-03,  ..., -2.7618e-03,
         -5.7983e-03,  6.4697e-03],
        [ 6.7749e-03, -8.3618e-03, -7.2021e-03,  ...,  3.3875e-03,
          4.6539e-04, -7.5989e-03],
        [-2.7466e-03,  4.0894e-03, -1.4484e-05,  ...,  4.4250e-03,
          5.3711e-03, -4.8523e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.16.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 26],
            [141],
            [111],
            ...,
            [125],
            [183],
            [ 51]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0066, -0.0131, -0.0054,  ..., -0.0005,  0.0039,  0.0080],
        [-0.0143,  0.0099, -0.0038,  ...,  0.0046,  0.0073, -0.0111],
        [ 0.0013, -0.0059, -0.0140,  ...,  0.0122,  0.0093,  0.0019],
        ...,
        [-0.0093,  0.0137, -0.0062,  ...,  0.0096,  0.0027, -0.0077],
        [-0.0123,  0.0034,  0.0134,  ..., -0.0068, -0.0125,  0.0114],
        [ 0.0072, -0.0100,  0.0010,  ...,  0.0135,  0.0153,  0.0083]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.16.input_layernorm.weight Parameter containing:
tensor([0.4102, 0.4160, 0.3867,  ..., 0.3867, 0.4023, 0.4004], device='cuda:0')
base_model.model.model.layers.16.post_attention_layernorm.weight Parameter containing:
tensor([0.3027, 0.2891, 0.2910,  ..., 0.3027, 0.3066, 0.2969], device='cuda:0')
base_model.model.model.layers.17.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 34],
            [215],
            [150],
            ...,
            [216],
            [219],
            [126]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0005, -0.0131, -0.0041,  ..., -0.0095, -0.0153, -0.0040],
        [-0.0082, -0.0154,  0.0134,  ..., -0.0050, -0.0019,  0.0080],
        [-0.0106,  0.0087,  0.0033,  ..., -0.0045,  0.0055, -0.0048],
        ...,
        [-0.0006,  0.0049, -0.0114,  ..., -0.0054,  0.0089, -0.0004],
        [ 0.0081,  0.0076,  0.0098,  ..., -0.0053, -0.0140,  0.0065],
        [-0.0004,  0.0049,  0.0118,  ..., -0.0110,  0.0059, -0.0056]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.17.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 41],
            [153],
            [194],
            ...,
            [182],
            [ 58],
            [150]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0129, -0.0115, -0.0146,  ..., -0.0056, -0.0126,  0.0085],
        [-0.0037,  0.0076,  0.0081,  ...,  0.0130,  0.0103, -0.0154],
        [ 0.0139,  0.0118, -0.0089,  ...,  0.0063, -0.0008,  0.0085],
        ...,
        [ 0.0084, -0.0125,  0.0058,  ...,  0.0068,  0.0114,  0.0143],
        [ 0.0084,  0.0065, -0.0099,  ..., -0.0048,  0.0113,  0.0056],
        [ 0.0118, -0.0156,  0.0126,  ...,  0.0084,  0.0103, -0.0044]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.17.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[ 73],
            [156],
            [ 86],
            ...,
            [157],
            [186],
            [169]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0100, -0.0134, -0.0099,  ...,  0.0093, -0.0006,  0.0114],
        [ 0.0097, -0.0123,  0.0087,  ..., -0.0002,  0.0064, -0.0076],
        [ 0.0063, -0.0091,  0.0095,  ...,  0.0105,  0.0130, -0.0044],
        ...,
        [-0.0060, -0.0061,  0.0005,  ..., -0.0025,  0.0156,  0.0073],
        [-0.0149,  0.0025,  0.0078,  ...,  0.0003, -0.0010,  0.0081],
        [ 0.0126, -0.0008, -0.0092,  ..., -0.0041,  0.0084, -0.0048]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.17.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[101],
            [164],
            [ 87],
            ...,
            [ 39],
            [ 47],
            [ 44]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0037,  0.0139,  0.0081,  ..., -0.0109, -0.0053,  0.0060],
        [ 0.0027,  0.0032, -0.0067,  ...,  0.0117, -0.0066, -0.0114],
        [-0.0062, -0.0108, -0.0003,  ..., -0.0117,  0.0139,  0.0012],
        ...,
        [-0.0051, -0.0002, -0.0055,  ...,  0.0084, -0.0101,  0.0029],
        [ 0.0132,  0.0064, -0.0028,  ..., -0.0095,  0.0093,  0.0057],
        [-0.0107, -0.0058,  0.0091,  ...,  0.0089,  0.0140,  0.0133]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.17.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[ 44],
            [171],
            [169],
            ...,
            [103],
            [ 68],
            [150]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0016, -0.0067, -0.0154,  ...,  0.0151,  0.0114, -0.0002],
        [ 0.0044, -0.0117, -0.0061,  ..., -0.0098,  0.0070, -0.0132],
        [ 0.0083, -0.0070, -0.0087,  ...,  0.0052, -0.0013, -0.0118],
        ...,
        [-0.0085, -0.0058, -0.0014,  ..., -0.0070,  0.0145, -0.0029],
        [ 0.0004, -0.0045, -0.0144,  ...,  0.0087, -0.0042,  0.0066],
        [-0.0002, -0.0120, -0.0028,  ...,  0.0044,  0.0072,  0.0100]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.17.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[195],
            [156],
            [245],
            ...,
            [104],
            [ 66],
            [ 88]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0070, -0.0085,  0.0042,  ...,  0.0012,  0.0026,  0.0036],
        [-0.0015,  0.0013,  0.0036,  ...,  0.0078, -0.0075,  0.0089],
        [ 0.0050,  0.0045, -0.0019,  ...,  0.0062, -0.0006,  0.0068],
        ...,
        [-0.0021,  0.0070,  0.0045,  ...,  0.0013, -0.0080, -0.0002],
        [ 0.0089,  0.0089, -0.0049,  ...,  0.0053, -0.0057, -0.0049],
        [ 0.0024,  0.0059,  0.0009,  ...,  0.0071, -0.0061, -0.0031]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.17.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 82],
            [ 83],
            [ 55],
            ...,
            [ 81],
            [184],
            [  3]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0114,  0.0148,  0.0052,  ..., -0.0040, -0.0013, -0.0106],
        [-0.0134,  0.0010, -0.0037,  ..., -0.0143, -0.0143,  0.0071],
        [-0.0042,  0.0114, -0.0088,  ..., -0.0026,  0.0079, -0.0012],
        ...,
        [ 0.0033,  0.0054, -0.0011,  ..., -0.0045,  0.0120,  0.0110],
        [ 0.0150, -0.0027, -0.0030,  ..., -0.0085, -0.0117,  0.0010],
        [ 0.0018, -0.0134,  0.0031,  ..., -0.0092, -0.0137,  0.0092]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.17.input_layernorm.weight Parameter containing:
tensor([0.4238, 0.4277, 0.4004,  ..., 0.4199, 0.4219, 0.4043], device='cuda:0')
base_model.model.model.layers.17.post_attention_layernorm.weight Parameter containing:
tensor([0.3203, 0.3125, 0.3105,  ..., 0.3223, 0.3242, 0.3145], device='cuda:0')
base_model.model.model.layers.18.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[152],
            [141],
            [  8],
            ...,
            [ 67],
            [202],
            [209]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 1.1902e-02,  9.7656e-03,  1.4709e-02,  ..., -4.9438e-03,
          1.2939e-02,  9.7046e-03],
        [-9.9487e-03, -2.0447e-03,  1.2085e-02,  ...,  1.3916e-02,
         -9.7656e-03,  1.0620e-02],
        [-1.2939e-02,  1.1108e-02, -9.0942e-03,  ...,  8.1539e-05,
          9.7656e-03,  1.4099e-02],
        ...,
        [ 7.2937e-03,  5.3711e-03, -7.3242e-03,  ...,  3.6774e-03,
          1.2894e-03, -1.0132e-02],
        [ 7.7515e-03, -1.2878e-02,  1.3428e-02,  ...,  5.8289e-03,
          1.2329e-02, -1.6098e-03],
        [-1.4221e-02,  3.9368e-03,  1.0986e-02,  ...,  1.4420e-03,
          9.3384e-03, -4.1199e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.18.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 62],
            [ 66],
            [ 54],
            ...,
            [145],
            [156],
            [ 83]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0026,  0.0154,  0.0144,  ..., -0.0020, -0.0130,  0.0059],
        [ 0.0156, -0.0122, -0.0046,  ...,  0.0009,  0.0065, -0.0012],
        [ 0.0143,  0.0065,  0.0103,  ...,  0.0118, -0.0026,  0.0035],
        ...,
        [ 0.0151,  0.0027,  0.0067,  ..., -0.0146, -0.0156, -0.0064],
        [ 0.0149, -0.0133, -0.0063,  ..., -0.0009, -0.0069,  0.0038],
        [-0.0128, -0.0032, -0.0050,  ..., -0.0042, -0.0051,  0.0053]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.18.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[ 43],
            [ 53],
            [119],
            ...,
            [ 44],
            [149],
            [ 71]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0131,  0.0122, -0.0009,  ...,  0.0117,  0.0137,  0.0053],
        [ 0.0007,  0.0067,  0.0101,  ..., -0.0132, -0.0125,  0.0099],
        [ 0.0062, -0.0096, -0.0117,  ..., -0.0027, -0.0137,  0.0066],
        ...,
        [-0.0024,  0.0095,  0.0151,  ...,  0.0040,  0.0128,  0.0048],
        [ 0.0119, -0.0071,  0.0057,  ..., -0.0031,  0.0036, -0.0002],
        [-0.0147,  0.0013,  0.0047,  ...,  0.0103,  0.0020,  0.0070]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.18.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[ 58],
            [ 67],
            [206],
            ...,
            [ 64],
            [211],
            [178]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0051,  0.0144, -0.0043,  ...,  0.0148,  0.0076,  0.0117],
        [ 0.0012,  0.0108, -0.0078,  ..., -0.0080,  0.0102, -0.0124],
        [ 0.0059,  0.0049, -0.0096,  ..., -0.0002,  0.0027,  0.0051],
        ...,
        [-0.0055,  0.0129,  0.0054,  ..., -0.0106, -0.0104, -0.0145],
        [ 0.0121,  0.0017,  0.0148,  ...,  0.0146, -0.0034,  0.0022],
        [-0.0106, -0.0119, -0.0018,  ...,  0.0026, -0.0016,  0.0040]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.18.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[166],
            [167],
            [115],
            ...,
            [ 23],
            [105],
            [103]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0131, -0.0101, -0.0126,  ...,  0.0014,  0.0013,  0.0126],
        [ 0.0106,  0.0110,  0.0096,  ..., -0.0019, -0.0129, -0.0106],
        [-0.0036, -0.0150,  0.0014,  ...,  0.0084,  0.0013, -0.0151],
        ...,
        [ 0.0068,  0.0036,  0.0039,  ...,  0.0048, -0.0092, -0.0150],
        [ 0.0044,  0.0122,  0.0020,  ...,  0.0028, -0.0087, -0.0133],
        [ 0.0140, -0.0136, -0.0051,  ..., -0.0102, -0.0030,  0.0061]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.18.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[218],
            [ 31],
            [127],
            ...,
            [106],
            [214],
            [186]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0060,  0.0010, -0.0050,  ...,  0.0010,  0.0051,  0.0020],
        [ 0.0050, -0.0039,  0.0084,  ...,  0.0091, -0.0072, -0.0031],
        [-0.0085,  0.0070, -0.0047,  ..., -0.0017,  0.0065,  0.0068],
        ...,
        [-0.0076,  0.0087,  0.0023,  ..., -0.0043,  0.0072, -0.0047],
        [ 0.0046, -0.0021, -0.0034,  ...,  0.0090,  0.0039,  0.0002],
        [-0.0092,  0.0071, -0.0045,  ..., -0.0090, -0.0005,  0.0031]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.18.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[170],
            [220],
            [178],
            ...,
            [ 98],
            [164],
            [187]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 1.2390e-02,  3.5400e-03,  9.0942e-03,  ..., -8.3618e-03,
          1.1063e-03,  1.3062e-02],
        [ 1.2817e-02, -1.4526e-02,  1.0437e-02,  ...,  1.1597e-02,
          1.2878e-02,  9.9487e-03],
        [-9.8419e-04,  1.2817e-02, -5.5542e-03,  ..., -7.9956e-03,
         -3.0212e-03, -2.3041e-03],
        ...,
        [ 5.8289e-03,  7.5989e-03,  1.0925e-02,  ..., -3.8910e-03,
          1.1475e-02,  7.9155e-05],
        [ 1.0132e-02,  6.1646e-03,  9.3384e-03,  ...,  2.3041e-03,
         -1.5137e-02,  1.0925e-02],
        [-1.0803e-02, -1.2024e-02, -4.5471e-03,  ...,  8.1177e-03,
         -8.5831e-04, -7.8735e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.18.input_layernorm.weight Parameter containing:
tensor([0.4473, 0.4473, 0.4277,  ..., 0.4297, 0.4414, 0.4258], device='cuda:0')
base_model.model.model.layers.18.post_attention_layernorm.weight Parameter containing:
tensor([0.3398, 0.3301, 0.3281,  ..., 0.3359, 0.3398, 0.3340], device='cuda:0')
base_model.model.model.layers.19.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[137],
            [ 82],
            [154],
            ...,
            [200],
            [215],
            [ 37]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0058, -0.0076, -0.0110,  ..., -0.0118,  0.0064, -0.0123],
        [ 0.0089,  0.0009,  0.0156,  ..., -0.0048,  0.0108,  0.0128],
        [-0.0066, -0.0145, -0.0022,  ...,  0.0150,  0.0033, -0.0027],
        ...,
        [-0.0065,  0.0023, -0.0075,  ...,  0.0093, -0.0110, -0.0120],
        [ 0.0120, -0.0089, -0.0063,  ..., -0.0050, -0.0008,  0.0011],
        [ 0.0151,  0.0026,  0.0021,  ..., -0.0116,  0.0071,  0.0147]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.19.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[219],
            [133],
            [ 69],
            ...,
            [200],
            [ 84],
            [155]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0108, -0.0114, -0.0152,  ..., -0.0020,  0.0030,  0.0068],
        [-0.0096, -0.0053,  0.0081,  ...,  0.0124, -0.0031, -0.0001],
        [-0.0053, -0.0052, -0.0113,  ..., -0.0073,  0.0010,  0.0010],
        ...,
        [ 0.0032,  0.0112,  0.0090,  ...,  0.0137,  0.0142, -0.0079],
        [ 0.0075,  0.0015, -0.0003,  ..., -0.0051,  0.0046, -0.0114],
        [-0.0131,  0.0067, -0.0156,  ...,  0.0011, -0.0098, -0.0045]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.19.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[134],
            [102],
            [153],
            ...,
            [105],
            [130],
            [ 75]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0049,  0.0060,  0.0125,  ...,  0.0102, -0.0006,  0.0121],
        [-0.0005,  0.0111,  0.0069,  ..., -0.0103,  0.0083,  0.0078],
        [ 0.0121,  0.0143, -0.0113,  ...,  0.0014, -0.0030,  0.0090],
        ...,
        [-0.0046, -0.0016, -0.0041,  ..., -0.0142,  0.0052, -0.0002],
        [ 0.0038,  0.0087,  0.0085,  ...,  0.0059, -0.0056, -0.0101],
        [-0.0139,  0.0051, -0.0055,  ..., -0.0132, -0.0073, -0.0036]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.19.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[192],
            [195],
            [ 93],
            ...,
            [213],
            [195],
            [ 75]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-2.6398e-03,  2.2888e-03,  2.6398e-03,  ..., -1.5015e-02,
         -9.3994e-03,  1.2817e-02],
        [-8.3618e-03,  1.7700e-03,  1.0925e-02,  ..., -4.4441e-04,
         -8.5449e-04, -1.2268e-02],
        [-6.0730e-03, -2.9449e-03,  5.1498e-05,  ..., -5.0049e-03,
         -6.8970e-03, -1.4648e-02],
        ...,
        [-1.1108e-02, -4.6387e-03,  2.4872e-03,  ...,  9.0942e-03,
         -4.3640e-03, -9.0027e-04],
        [ 9.9945e-04, -5.8899e-03,  6.6223e-03,  ...,  1.2878e-02,
          5.6152e-03,  1.3672e-02],
        [ 4.4250e-03, -6.8665e-03,  1.4221e-02,  ...,  9.1553e-03,
         -2.9755e-03, -2.4109e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.19.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[ 72],
            [152],
            [163],
            ...,
            [151],
            [ 87],
            [151]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0027, -0.0088, -0.0113,  ...,  0.0032, -0.0024,  0.0093],
        [ 0.0053,  0.0069, -0.0061,  ...,  0.0140,  0.0071,  0.0156],
        [-0.0054,  0.0135,  0.0024,  ..., -0.0145, -0.0154, -0.0013],
        ...,
        [-0.0135,  0.0047,  0.0006,  ..., -0.0135,  0.0147,  0.0099],
        [ 0.0112,  0.0057,  0.0052,  ...,  0.0120,  0.0104,  0.0043],
        [ 0.0029,  0.0093,  0.0077,  ...,  0.0109,  0.0137,  0.0107]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.19.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[ 98],
            [150],
            [118],
            ...,
            [236],
            [103],
            [153]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-6.0730e-03,  3.9368e-03, -7.0572e-04,  ...,  2.7313e-03,
          7.2937e-03,  9.3994e-03],
        [-1.2436e-03, -5.3406e-03,  4.1199e-03,  ..., -6.9885e-03,
         -4.4556e-03, -4.4250e-03],
        [-8.6670e-03,  1.3809e-03, -2.6398e-03,  ...,  6.8054e-03,
         -7.8125e-03,  3.3264e-03],
        ...,
        [ 3.6469e-03, -2.0752e-03,  9.0942e-03,  ...,  8.2016e-04,
         -3.6011e-03, -6.1512e-05],
        [-4.1504e-03,  9.3994e-03, -7.9956e-03,  ...,  2.4261e-03,
          4.7607e-03, -6.5613e-03],
        [-1.1978e-03,  1.2741e-03, -2.8992e-04,  ..., -6.5308e-03,
          3.5858e-03, -2.8229e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.19.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[114],
            [184],
            [ 58],
            ...,
            [  6],
            [102],
            [121]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0052,  0.0150, -0.0074,  ...,  0.0112,  0.0037,  0.0006],
        [-0.0146, -0.0004, -0.0117,  ...,  0.0153, -0.0057,  0.0118],
        [-0.0060,  0.0056,  0.0149,  ...,  0.0150,  0.0075, -0.0110],
        ...,
        [-0.0059,  0.0090, -0.0066,  ...,  0.0065, -0.0010, -0.0112],
        [-0.0046, -0.0018,  0.0047,  ...,  0.0099,  0.0126, -0.0107],
        [ 0.0099, -0.0087, -0.0010,  ...,  0.0137,  0.0146,  0.0120]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.19.input_layernorm.weight Parameter containing:
tensor([0.4512, 0.4570, 0.4375,  ..., 0.4258, 0.4336, 0.4375], device='cuda:0')
base_model.model.model.layers.19.post_attention_layernorm.weight Parameter containing:
tensor([0.3535, 0.3398, 0.3418,  ..., 0.3496, 0.3496, 0.3477], device='cuda:0')
base_model.model.model.layers.20.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[133],
            [226],
            [214],
            ...,
            [159],
            [181],
            [ 87]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0003, -0.0150,  0.0127,  ...,  0.0012, -0.0128, -0.0093],
        [-0.0034, -0.0027, -0.0045,  ...,  0.0109,  0.0154, -0.0128],
        [-0.0125,  0.0074,  0.0126,  ..., -0.0082, -0.0022,  0.0074],
        ...,
        [ 0.0043, -0.0006, -0.0020,  ..., -0.0107, -0.0025,  0.0045],
        [ 0.0109,  0.0129,  0.0101,  ..., -0.0135,  0.0114,  0.0029],
        [ 0.0078, -0.0119, -0.0026,  ..., -0.0017,  0.0153, -0.0152]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.20.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 90],
            [194],
            [101],
            ...,
            [135],
            [ 27],
            [ 61]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0041, -0.0016, -0.0057,  ...,  0.0137,  0.0063, -0.0092],
        [-0.0103,  0.0031,  0.0153,  ...,  0.0080,  0.0107,  0.0049],
        [ 0.0082,  0.0029,  0.0061,  ..., -0.0012,  0.0154,  0.0003],
        ...,
        [-0.0040,  0.0093,  0.0131,  ...,  0.0066,  0.0049, -0.0087],
        [-0.0049,  0.0005,  0.0034,  ..., -0.0037, -0.0108, -0.0075],
        [-0.0048, -0.0101,  0.0024,  ...,  0.0143, -0.0106,  0.0107]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.20.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[100],
            [184],
            [ 39],
            ...,
            [172],
            [ 85],
            [ 64]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0014, -0.0059, -0.0092,  ...,  0.0093, -0.0139, -0.0106],
        [ 0.0090, -0.0063, -0.0003,  ...,  0.0086, -0.0145,  0.0027],
        [ 0.0018,  0.0093, -0.0052,  ..., -0.0022,  0.0120, -0.0074],
        ...,
        [-0.0015,  0.0013,  0.0092,  ..., -0.0126, -0.0153,  0.0137],
        [ 0.0034, -0.0125,  0.0062,  ..., -0.0101, -0.0142, -0.0031],
        [ 0.0107,  0.0053,  0.0067,  ...,  0.0137, -0.0074,  0.0066]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.20.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[152],
            [214],
            [169],
            ...,
            [117],
            [ 43],
            [ 42]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0012,  0.0079,  0.0043,  ...,  0.0112,  0.0002, -0.0025],
        [ 0.0149,  0.0081,  0.0087,  ...,  0.0014,  0.0090,  0.0079],
        [-0.0009,  0.0094,  0.0154,  ..., -0.0042,  0.0056, -0.0106],
        ...,
        [ 0.0114,  0.0095, -0.0001,  ...,  0.0072,  0.0090,  0.0125],
        [-0.0122, -0.0059, -0.0059,  ...,  0.0086, -0.0039, -0.0069],
        [ 0.0085, -0.0074,  0.0047,  ..., -0.0106, -0.0106,  0.0050]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.20.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[118],
            [ 57],
            [107],
            ...,
            [ 17],
            [119],
            [ 38]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0093, -0.0025, -0.0039,  ...,  0.0012,  0.0012, -0.0104],
        [ 0.0072, -0.0063,  0.0074,  ...,  0.0055, -0.0043,  0.0071],
        [-0.0070,  0.0117, -0.0097,  ..., -0.0044,  0.0134,  0.0090],
        ...,
        [-0.0092,  0.0086,  0.0033,  ..., -0.0132,  0.0092,  0.0018],
        [-0.0112,  0.0031, -0.0059,  ...,  0.0097,  0.0012, -0.0089],
        [ 0.0031,  0.0077, -0.0109,  ..., -0.0095, -0.0104,  0.0124]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.20.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[109],
            [101],
            [150],
            ...,
            [149],
            [247],
            [ 86]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0056, -0.0017,  0.0055,  ..., -0.0033,  0.0090,  0.0040],
        [-0.0089, -0.0027,  0.0079,  ...,  0.0035, -0.0007, -0.0049],
        [ 0.0089, -0.0069, -0.0051,  ..., -0.0077,  0.0034,  0.0024],
        ...,
        [-0.0090,  0.0036,  0.0053,  ..., -0.0020, -0.0081, -0.0030],
        [ 0.0042, -0.0012,  0.0007,  ...,  0.0019,  0.0075,  0.0014],
        [ 0.0047, -0.0070, -0.0088,  ...,  0.0045, -0.0093,  0.0039]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.20.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[  7],
            [120],
            [102],
            ...,
            [ 66],
            [245],
            [ 53]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-2.0294e-03,  1.4465e-02,  9.5844e-05,  ...,  1.5564e-02,
          1.4893e-02,  1.0071e-02],
        [ 1.3855e-02, -1.5076e-02, -1.8921e-03,  ...,  9.7656e-03,
          1.5320e-02,  1.5381e-02],
        [-1.1841e-02, -1.3611e-02, -9.2773e-03,  ...,  7.6904e-03,
         -2.1515e-03, -1.3184e-02],
        ...,
        [-8.6060e-03, -5.6763e-03,  1.0010e-02,  ...,  1.1963e-02,
          1.0681e-02, -1.1414e-02],
        [-1.8768e-03,  9.2773e-03, -6.1340e-03,  ..., -5.5847e-03,
         -1.4893e-02, -3.0823e-03],
        [ 6.8054e-03,  1.1475e-02,  2.1210e-03,  ..., -9.4604e-03,
          1.0864e-02, -1.0681e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.20.input_layernorm.weight Parameter containing:
tensor([0.4531, 0.4668, 0.4375,  ..., 0.4336, 0.4336, 0.4473], device='cuda:0')
base_model.model.model.layers.20.post_attention_layernorm.weight Parameter containing:
tensor([0.3633, 0.3574, 0.3516,  ..., 0.3633, 0.3613, 0.3574], device='cuda:0')
base_model.model.model.layers.21.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 37],
            [219],
            [ 81],
            ...,
            [ 74],
            [ 70],
            [162]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 3.6163e-03,  8.3923e-04, -7.2632e-03,  ..., -9.4604e-03,
          5.4321e-03,  7.2327e-03],
        [-1.4648e-02,  1.9670e-05,  1.2878e-02,  ...,  1.4465e-02,
         -8.7891e-03, -1.2634e-02],
        [ 1.1749e-03, -7.3242e-03,  6.5918e-03,  ..., -1.3489e-02,
          3.3875e-03, -1.1047e-02],
        ...,
        [ 7.1716e-03,  8.8501e-03, -7.8735e-03,  ..., -3.2806e-04,
          1.0864e-02, -6.9580e-03],
        [-1.3855e-02, -6.6223e-03,  9.3384e-03,  ..., -3.3875e-03,
          4.0770e-05,  4.3945e-03],
        [-1.4282e-02,  2.4605e-04,  1.0986e-02,  ...,  1.5717e-03,
         -1.4587e-02, -5.0049e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.21.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[156],
            [157],
            [231],
            ...,
            [149],
            [215],
            [ 79]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-3.7384e-03, -1.0803e-02, -6.3171e-03,  ..., -6.1951e-03,
         -2.9602e-03,  7.8125e-03],
        [-7.3242e-03,  3.8910e-03,  6.5613e-03,  ...,  2.5177e-03,
          1.3428e-02,  6.1951e-03],
        [-6.4697e-03,  1.4893e-02,  9.2163e-03,  ...,  4.3030e-03,
         -5.8899e-03, -1.7776e-03],
        ...,
        [-5.8746e-04, -1.5616e-05,  1.0742e-02,  ...,  1.1292e-02,
         -1.1841e-02,  5.0354e-04],
        [-8.6060e-03,  7.1411e-03,  3.4943e-03,  ...,  1.3245e-02,
          8.9111e-03, -1.8978e-04],
        [ 1.0620e-02,  1.4526e-02, -7.3242e-03,  ...,  1.3916e-02,
         -1.3916e-02, -1.1536e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.21.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[121],
            [101],
            [ 83],
            ...,
            [ 36],
            [152],
            [ 71]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0152,  0.0042, -0.0049,  ..., -0.0069, -0.0146,  0.0132],
        [-0.0041,  0.0137, -0.0125,  ...,  0.0033, -0.0019,  0.0075],
        [ 0.0146,  0.0154,  0.0076,  ...,  0.0033,  0.0088, -0.0099],
        ...,
        [ 0.0025, -0.0047,  0.0014,  ..., -0.0110, -0.0097,  0.0004],
        [ 0.0037,  0.0139, -0.0076,  ...,  0.0047, -0.0057,  0.0123],
        [-0.0052, -0.0139, -0.0148,  ...,  0.0089,  0.0088, -0.0064]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.21.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[ 71],
            [166],
            [124],
            ...,
            [ 36],
            [174],
            [ 56]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0067, -0.0039, -0.0003,  ...,  0.0125, -0.0138,  0.0015],
        [-0.0023,  0.0067, -0.0029,  ...,  0.0119,  0.0090, -0.0039],
        [-0.0036, -0.0146, -0.0080,  ..., -0.0008, -0.0112,  0.0005],
        ...,
        [ 0.0041,  0.0141,  0.0039,  ...,  0.0065, -0.0089,  0.0022],
        [-0.0018, -0.0113, -0.0063,  ..., -0.0065, -0.0006,  0.0059],
        [-0.0030, -0.0029,  0.0004,  ...,  0.0105, -0.0123,  0.0102]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.21.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[ 98],
            [162],
            [165],
            ...,
            [  7],
            [ 35],
            [121]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0058, -0.0026, -0.0150,  ...,  0.0030,  0.0023,  0.0024],
        [ 0.0045, -0.0135,  0.0061,  ...,  0.0115, -0.0018,  0.0106],
        [ 0.0002, -0.0004, -0.0083,  ...,  0.0121, -0.0151,  0.0046],
        ...,
        [-0.0016, -0.0090,  0.0077,  ..., -0.0143,  0.0098, -0.0010],
        [ 0.0123, -0.0071,  0.0070,  ..., -0.0074,  0.0105,  0.0101],
        [ 0.0046, -0.0107,  0.0075,  ...,  0.0122, -0.0134,  0.0035]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.21.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[171],
            [104],
            [141],
            ...,
            [ 59],
            [116],
            [179]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-1.1978e-03, -8.7280e-03, -5.9509e-03,  ...,  2.1515e-03,
         -4.3335e-03, -3.8910e-03],
        [ 2.4261e-03, -1.2970e-04,  3.3722e-03,  ...,  8.6060e-03,
          9.8419e-04,  1.8921e-03],
        [ 6.0425e-03, -1.3580e-03, -9.2773e-03,  ...,  1.9379e-03,
         -2.4796e-05,  1.9455e-03],
        ...,
        [-9.5215e-03,  7.1106e-03,  7.3853e-03,  ..., -4.9744e-03,
         -1.2741e-03, -3.8452e-03],
        [-3.8757e-03,  5.2185e-03, -5.7678e-03,  ...,  7.9346e-03,
         -6.8054e-03,  5.0049e-03],
        [-8.5449e-03,  8.0566e-03,  7.2021e-03,  ...,  8.3008e-03,
          5.8289e-03,  7.1716e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.21.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[209],
            [ 55],
            [217],
            ...,
            [ 22],
            [133],
            [ 97]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0055,  0.0148, -0.0102,  ..., -0.0125,  0.0084,  0.0081],
        [-0.0052,  0.0095,  0.0048,  ...,  0.0002,  0.0045,  0.0143],
        [ 0.0097, -0.0111,  0.0071,  ..., -0.0018,  0.0001, -0.0086],
        ...,
        [-0.0126, -0.0029,  0.0059,  ..., -0.0032,  0.0010, -0.0139],
        [-0.0021,  0.0036,  0.0104,  ...,  0.0092,  0.0003,  0.0075],
        [ 0.0091, -0.0115, -0.0070,  ..., -0.0002, -0.0132,  0.0085]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.21.input_layernorm.weight Parameter containing:
tensor([0.4785, 0.4863, 0.4688,  ..., 0.4551, 0.4707, 0.4766], device='cuda:0')
base_model.model.model.layers.21.post_attention_layernorm.weight Parameter containing:
tensor([0.3730, 0.3691, 0.3633,  ..., 0.3789, 0.3711, 0.3730], device='cuda:0')
base_model.model.model.layers.22.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 52],
            [ 84],
            [ 26],
            ...,
            [152],
            [146],
            [154]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0033,  0.0027, -0.0108,  ..., -0.0049,  0.0127, -0.0086],
        [ 0.0048,  0.0024,  0.0021,  ..., -0.0116,  0.0106,  0.0095],
        [-0.0148,  0.0087,  0.0047,  ..., -0.0051, -0.0086,  0.0145],
        ...,
        [-0.0025, -0.0079,  0.0084,  ..., -0.0047, -0.0120,  0.0007],
        [ 0.0079, -0.0052,  0.0058,  ..., -0.0020,  0.0029, -0.0095],
        [-0.0081, -0.0007,  0.0115,  ..., -0.0129, -0.0033,  0.0139]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.22.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 55],
            [102],
            [ 87],
            ...,
            [241],
            [107],
            [ 57]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-1.0437e-02, -9.4604e-03, -5.7678e-03,  ..., -2.9907e-03,
         -7.5073e-03,  7.4463e-03],
        [-1.4603e-05, -1.3611e-02, -4.1809e-03,  ...,  8.6670e-03,
         -5.8899e-03,  1.5625e-02],
        [ 6.5918e-03, -7.2327e-03, -4.2419e-03,  ...,  1.4267e-03,
          3.9978e-03, -1.5259e-04],
        ...,
        [ 1.5442e-02,  1.0681e-02,  1.2512e-02,  ...,  5.9204e-03,
          4.5776e-03, -1.4160e-02],
        [ 6.5613e-03,  1.1719e-02,  1.3245e-02,  ..., -9.3384e-03,
         -7.2632e-03,  1.2939e-02],
        [-8.8501e-03, -1.1047e-02, -7.6294e-03,  ...,  1.1658e-02,
         -3.2043e-03, -1.3672e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.22.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[ 56],
            [106],
            [251],
            ...,
            [137],
            [153],
            [125]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0116, -0.0009,  0.0095,  ..., -0.0058,  0.0046, -0.0051],
        [-0.0023,  0.0115, -0.0016,  ..., -0.0086, -0.0026, -0.0005],
        [-0.0077, -0.0093,  0.0058,  ..., -0.0023,  0.0151, -0.0119],
        ...,
        [-0.0090, -0.0063,  0.0110,  ...,  0.0022, -0.0136,  0.0095],
        [-0.0031, -0.0015,  0.0120,  ...,  0.0004,  0.0010,  0.0049],
        [ 0.0063,  0.0005,  0.0104,  ..., -0.0147,  0.0017,  0.0115]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.22.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[202],
            [ 54],
            [185],
            ...,
            [152],
            [169],
            [170]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0014,  0.0033, -0.0086,  ..., -0.0019,  0.0049,  0.0100],
        [ 0.0125,  0.0074,  0.0002,  ...,  0.0086, -0.0149,  0.0112],
        [ 0.0044,  0.0044,  0.0127,  ..., -0.0052, -0.0073, -0.0046],
        ...,
        [ 0.0100,  0.0052,  0.0020,  ...,  0.0011, -0.0077,  0.0032],
        [ 0.0022, -0.0131, -0.0148,  ..., -0.0042, -0.0143,  0.0112],
        [-0.0043,  0.0127, -0.0099,  ..., -0.0030,  0.0134, -0.0048]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.22.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[ 68],
            [195],
            [ 68],
            ...,
            [102],
            [117],
            [187]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0096,  0.0011, -0.0066,  ...,  0.0053, -0.0022, -0.0143],
        [-0.0099,  0.0045, -0.0095,  ...,  0.0117, -0.0129, -0.0041],
        [ 0.0144,  0.0071,  0.0085,  ..., -0.0136, -0.0006,  0.0082],
        ...,
        [ 0.0013, -0.0040,  0.0062,  ...,  0.0089,  0.0049, -0.0124],
        [ 0.0087,  0.0012,  0.0136,  ..., -0.0135, -0.0001, -0.0050],
        [ 0.0061, -0.0123, -0.0057,  ...,  0.0138,  0.0103,  0.0058]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.22.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[165],
            [169],
            [ 60],
            ...,
            [217],
            [ 54],
            [ 41]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0003,  0.0063,  0.0019,  ...,  0.0023,  0.0025, -0.0056],
        [ 0.0050,  0.0085,  0.0050,  ...,  0.0075,  0.0005,  0.0008],
        [-0.0013,  0.0092,  0.0068,  ..., -0.0065,  0.0023,  0.0093],
        ...,
        [-0.0020,  0.0085, -0.0094,  ..., -0.0090, -0.0018,  0.0024],
        [-0.0032,  0.0076,  0.0076,  ..., -0.0038,  0.0072,  0.0087],
        [-0.0092,  0.0063, -0.0003,  ..., -0.0084,  0.0015,  0.0035]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.22.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[237],
            [132],
            [222],
            ...,
            [ 83],
            [ 49],
            [ 10]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0025, -0.0143, -0.0004,  ...,  0.0035, -0.0021,  0.0014],
        [ 0.0045, -0.0154,  0.0114,  ..., -0.0095, -0.0015,  0.0092],
        [ 0.0115, -0.0037,  0.0135,  ..., -0.0125,  0.0123, -0.0004],
        ...,
        [-0.0065, -0.0019,  0.0067,  ..., -0.0012, -0.0081, -0.0070],
        [-0.0093, -0.0002,  0.0052,  ..., -0.0092, -0.0060, -0.0002],
        [-0.0140, -0.0044,  0.0046,  ..., -0.0112, -0.0153, -0.0021]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.22.input_layernorm.weight Parameter containing:
tensor([0.4863, 0.4863, 0.4746,  ..., 0.4688, 0.4863, 0.4863], device='cuda:0')
base_model.model.model.layers.22.post_attention_layernorm.weight Parameter containing:
tensor([0.3848, 0.3809, 0.3828,  ..., 0.3926, 0.3867, 0.3887], device='cuda:0')
base_model.model.model.layers.23.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[131],
            [126],
            [172],
            ...,
            [165],
            [ 89],
            [214]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0076,  0.0096, -0.0082,  ..., -0.0139,  0.0108, -0.0139],
        [ 0.0119, -0.0014, -0.0013,  ..., -0.0048,  0.0077,  0.0092],
        [-0.0131,  0.0049,  0.0018,  ..., -0.0029,  0.0040, -0.0069],
        ...,
        [ 0.0134,  0.0077,  0.0037,  ...,  0.0075,  0.0025,  0.0145],
        [-0.0115,  0.0143, -0.0110,  ..., -0.0146,  0.0087, -0.0056],
        [-0.0145,  0.0076, -0.0045,  ..., -0.0031,  0.0109, -0.0072]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.23.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[106],
            [154],
            [157],
            ...,
            [192],
            [232],
            [117]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0079,  0.0078,  0.0145,  ..., -0.0056, -0.0142,  0.0102],
        [ 0.0116, -0.0023, -0.0011,  ...,  0.0092, -0.0061,  0.0068],
        [-0.0126,  0.0041, -0.0135,  ...,  0.0129, -0.0013,  0.0092],
        ...,
        [-0.0018,  0.0097,  0.0005,  ...,  0.0052,  0.0037, -0.0072],
        [ 0.0087,  0.0126,  0.0021,  ..., -0.0023,  0.0023,  0.0024],
        [ 0.0026, -0.0052, -0.0070,  ...,  0.0056, -0.0134, -0.0015]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.23.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[141],
            [103],
            [138],
            ...,
            [150],
            [ 26],
            [145]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0054,  0.0048,  0.0110,  ...,  0.0001, -0.0120,  0.0053],
        [-0.0144,  0.0117, -0.0050,  ..., -0.0060,  0.0151, -0.0006],
        [ 0.0143,  0.0013,  0.0053,  ..., -0.0123,  0.0044, -0.0104],
        ...,
        [ 0.0056, -0.0145,  0.0121,  ..., -0.0151,  0.0154,  0.0049],
        [-0.0031, -0.0084,  0.0146,  ...,  0.0128,  0.0137, -0.0093],
        [ 0.0135, -0.0111,  0.0071,  ..., -0.0122, -0.0061,  0.0118]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.23.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[102],
            [ 73],
            [144],
            ...,
            [148],
            [134],
            [115]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0120,  0.0069,  0.0121,  ..., -0.0008, -0.0030,  0.0035],
        [-0.0040,  0.0027,  0.0011,  ..., -0.0026, -0.0140,  0.0084],
        [-0.0055, -0.0048,  0.0029,  ...,  0.0049, -0.0125, -0.0156],
        ...,
        [-0.0098, -0.0007,  0.0123,  ...,  0.0032, -0.0117,  0.0124],
        [-0.0046,  0.0002,  0.0128,  ..., -0.0090, -0.0047, -0.0099],
        [ 0.0046,  0.0035,  0.0031,  ...,  0.0044, -0.0137, -0.0095]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.23.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[ 93],
            [116],
            [120],
            ...,
            [106],
            [ 86],
            [134]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0110,  0.0113,  0.0074,  ..., -0.0017, -0.0154, -0.0152],
        [-0.0038,  0.0004, -0.0011,  ...,  0.0044, -0.0026, -0.0041],
        [-0.0053,  0.0089,  0.0140,  ...,  0.0057,  0.0147,  0.0099],
        ...,
        [-0.0073,  0.0128,  0.0058,  ..., -0.0095, -0.0109, -0.0109],
        [ 0.0117, -0.0152, -0.0047,  ..., -0.0007, -0.0153, -0.0139],
        [-0.0072, -0.0038,  0.0102,  ...,  0.0139, -0.0034,  0.0155]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.23.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[122],
            [ 45],
            [ 89],
            ...,
            [102],
            [ 37],
            [181]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0077, -0.0052, -0.0007,  ..., -0.0060, -0.0089, -0.0045],
        [ 0.0042, -0.0092,  0.0075,  ..., -0.0052, -0.0050, -0.0010],
        [ 0.0077, -0.0047,  0.0089,  ...,  0.0077, -0.0066,  0.0090],
        ...,
        [ 0.0028, -0.0092, -0.0009,  ...,  0.0043,  0.0018, -0.0070],
        [ 0.0066, -0.0002, -0.0045,  ..., -0.0003, -0.0075, -0.0061],
        [ 0.0010,  0.0021,  0.0052,  ...,  0.0071, -0.0089,  0.0043]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.23.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[218],
            [211],
            [101],
            ...,
            [214],
            [ 23],
            [ 60]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0005, -0.0079,  0.0004,  ..., -0.0114,  0.0085, -0.0105],
        [-0.0085, -0.0093,  0.0053,  ...,  0.0057, -0.0042, -0.0015],
        [-0.0061,  0.0049,  0.0092,  ...,  0.0055, -0.0146, -0.0079],
        ...,
        [ 0.0024, -0.0017, -0.0056,  ..., -0.0082,  0.0094,  0.0053],
        [-0.0034,  0.0143, -0.0009,  ...,  0.0020,  0.0018,  0.0154],
        [ 0.0123,  0.0029,  0.0105,  ...,  0.0102,  0.0100,  0.0071]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.23.input_layernorm.weight Parameter containing:
tensor([0.5117, 0.5234, 0.5078,  ..., 0.5039, 0.5195, 0.5234], device='cuda:0')
base_model.model.model.layers.23.post_attention_layernorm.weight Parameter containing:
tensor([0.4004, 0.3926, 0.3945,  ..., 0.3984, 0.4004, 0.4023], device='cuda:0')
base_model.model.model.layers.24.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[180],
            [153],
            [ 37],
            ...,
            [154],
            [101],
            [207]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 2.1057e-03, -3.7994e-03, -2.1820e-03,  ...,  9.7656e-04,
         -1.5320e-02,  8.3008e-03],
        [ 1.0681e-02,  5.2795e-03, -1.0620e-02,  ...,  1.1536e-02,
         -1.3275e-03, -7.3853e-03],
        [ 1.2451e-02, -2.5635e-03,  2.9297e-03,  ..., -5.5542e-03,
          1.1841e-02,  1.1475e-02],
        ...,
        [-5.8594e-03,  4.5776e-04,  8.1177e-03,  ...,  5.8899e-03,
          1.1597e-02,  1.4587e-02],
        [-5.2185e-03,  1.1169e-02,  1.0010e-02,  ..., -9.3994e-03,
          1.2512e-02,  6.6280e-05],
        [-1.0010e-02, -3.5248e-03, -1.6022e-04,  ..., -1.1719e-02,
         -2.8419e-04, -1.1658e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.24.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[199],
            [ 88],
            [ 52],
            ...,
            [180],
            [184],
            [111]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0056, -0.0094, -0.0128,  ...,  0.0105, -0.0020,  0.0091],
        [ 0.0052, -0.0119, -0.0026,  ...,  0.0154, -0.0081,  0.0087],
        [ 0.0046,  0.0132, -0.0014,  ...,  0.0057,  0.0093,  0.0032],
        ...,
        [-0.0097, -0.0138, -0.0135,  ..., -0.0085, -0.0053,  0.0063],
        [-0.0151,  0.0094,  0.0151,  ..., -0.0103,  0.0089, -0.0089],
        [ 0.0043,  0.0045, -0.0012,  ...,  0.0117, -0.0094, -0.0049]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.24.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[211],
            [103],
            [ 34],
            ...,
            [231],
            [123],
            [ 69]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0007, -0.0077, -0.0154,  ..., -0.0061, -0.0069, -0.0045],
        [ 0.0082, -0.0154, -0.0103,  ..., -0.0095,  0.0033, -0.0001],
        [-0.0148, -0.0140, -0.0054,  ...,  0.0075,  0.0060,  0.0060],
        ...,
        [ 0.0030,  0.0115,  0.0109,  ...,  0.0108,  0.0135, -0.0033],
        [ 0.0010,  0.0067, -0.0115,  ..., -0.0060, -0.0109, -0.0009],
        [-0.0081,  0.0096,  0.0131,  ..., -0.0044,  0.0136,  0.0017]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.24.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[ 91],
            [209],
            [174],
            ...,
            [136],
            [120],
            [ 58]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0038,  0.0150,  0.0073,  ...,  0.0121, -0.0152,  0.0048],
        [-0.0048,  0.0076, -0.0135,  ..., -0.0105,  0.0129, -0.0150],
        [-0.0043,  0.0087,  0.0121,  ...,  0.0079, -0.0049,  0.0154],
        ...,
        [-0.0139, -0.0048, -0.0072,  ...,  0.0089, -0.0022,  0.0022],
        [-0.0109, -0.0053, -0.0042,  ...,  0.0128,  0.0024,  0.0085],
        [ 0.0029,  0.0149, -0.0104,  ...,  0.0104,  0.0132,  0.0036]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.24.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[ 36],
            [165],
            [119],
            ...,
            [ 75],
            [174],
            [ 64]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0120, -0.0155, -0.0075,  ...,  0.0072,  0.0013,  0.0016],
        [-0.0009, -0.0020, -0.0041,  ...,  0.0048, -0.0109, -0.0025],
        [-0.0010, -0.0155,  0.0067,  ...,  0.0110,  0.0034,  0.0038],
        ...,
        [-0.0056, -0.0088,  0.0137,  ...,  0.0116, -0.0031, -0.0148],
        [-0.0008,  0.0047, -0.0017,  ...,  0.0035,  0.0014,  0.0046],
        [ 0.0066,  0.0082, -0.0046,  ..., -0.0039, -0.0112, -0.0006]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.24.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[101],
            [ 44],
            [124],
            ...,
            [143],
            [106],
            [ 81]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0046, -0.0027, -0.0062,  ...,  0.0026, -0.0092, -0.0089],
        [ 0.0072, -0.0070,  0.0012,  ...,  0.0051, -0.0087,  0.0046],
        [ 0.0090, -0.0090,  0.0009,  ..., -0.0033, -0.0059, -0.0063],
        ...,
        [ 0.0052, -0.0038,  0.0035,  ..., -0.0027,  0.0034,  0.0010],
        [-0.0036,  0.0092, -0.0030,  ..., -0.0080, -0.0055,  0.0033],
        [ 0.0085,  0.0020, -0.0047,  ..., -0.0047, -0.0074, -0.0090]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.24.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[135],
            [199],
            [ 94],
            ...,
            [ 51],
            [140],
            [219]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0008, -0.0151,  0.0155,  ..., -0.0074,  0.0054,  0.0120],
        [ 0.0109,  0.0126, -0.0112,  ..., -0.0004, -0.0102, -0.0009],
        [ 0.0031, -0.0042,  0.0029,  ..., -0.0123,  0.0068, -0.0053],
        ...,
        [ 0.0067, -0.0098, -0.0002,  ..., -0.0015,  0.0056, -0.0111],
        [-0.0073,  0.0037, -0.0020,  ..., -0.0070,  0.0068,  0.0045],
        [ 0.0041,  0.0068,  0.0040,  ..., -0.0007,  0.0084,  0.0128]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.24.input_layernorm.weight Parameter containing:
tensor([0.4941, 0.5195, 0.5117,  ..., 0.4863, 0.5156, 0.5078], device='cuda:0')
base_model.model.model.layers.24.post_attention_layernorm.weight Parameter containing:
tensor([0.4102, 0.4062, 0.4082,  ..., 0.4121, 0.4160, 0.4102], device='cuda:0')
base_model.model.model.layers.25.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[146],
            [ 20],
            [210],
            ...,
            [179],
            [135],
            [163]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0120,  0.0027, -0.0131,  ...,  0.0106, -0.0107,  0.0058],
        [ 0.0073,  0.0068,  0.0041,  ..., -0.0126, -0.0009,  0.0063],
        [ 0.0135,  0.0074, -0.0121,  ..., -0.0072,  0.0058, -0.0014],
        ...,
        [-0.0110, -0.0073, -0.0125,  ...,  0.0067,  0.0072,  0.0120],
        [-0.0053, -0.0065, -0.0127,  ..., -0.0120, -0.0034, -0.0037],
        [ 0.0003,  0.0032,  0.0128,  ..., -0.0050, -0.0070,  0.0018]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.25.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 67],
            [ 85],
            [171],
            ...,
            [176],
            [ 76],
            [132]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0148, -0.0020,  0.0064,  ..., -0.0075, -0.0070,  0.0043],
        [ 0.0076, -0.0039,  0.0115,  ..., -0.0011,  0.0106, -0.0038],
        [ 0.0086, -0.0011,  0.0094,  ..., -0.0088,  0.0019,  0.0032],
        ...,
        [ 0.0041, -0.0056, -0.0101,  ...,  0.0101, -0.0113,  0.0068],
        [ 0.0143,  0.0010,  0.0065,  ..., -0.0081, -0.0089,  0.0118],
        [-0.0132,  0.0044, -0.0096,  ...,  0.0024,  0.0123,  0.0119]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.25.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[154],
            [150],
            [117],
            ...,
            [200],
            [ 23],
            [188]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0103,  0.0099, -0.0014,  ...,  0.0062, -0.0095,  0.0143],
        [-0.0125,  0.0063, -0.0121,  ...,  0.0011,  0.0130,  0.0121],
        [-0.0088,  0.0069,  0.0034,  ...,  0.0097,  0.0071, -0.0025],
        ...,
        [ 0.0046,  0.0020,  0.0009,  ...,  0.0037, -0.0151,  0.0108],
        [ 0.0036, -0.0066, -0.0015,  ..., -0.0093,  0.0129, -0.0010],
        [ 0.0055, -0.0063,  0.0079,  ...,  0.0020, -0.0042,  0.0074]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.25.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[170],
            [117],
            [104],
            ...,
            [ 29],
            [225],
            [ 52]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0015, -0.0156,  0.0128,  ...,  0.0061,  0.0040,  0.0038],
        [ 0.0146, -0.0129, -0.0006,  ...,  0.0029, -0.0137, -0.0115],
        [-0.0025, -0.0082, -0.0028,  ..., -0.0032, -0.0117,  0.0034],
        ...,
        [-0.0012,  0.0009, -0.0134,  ...,  0.0040,  0.0139,  0.0156],
        [-0.0143, -0.0107,  0.0144,  ..., -0.0117,  0.0039, -0.0131],
        [ 0.0142,  0.0104, -0.0138,  ..., -0.0025, -0.0137, -0.0028]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.25.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[112],
            [241],
            [177],
            ...,
            [180],
            [174],
            [134]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0040,  0.0099,  0.0039,  ..., -0.0074, -0.0062, -0.0077],
        [-0.0082, -0.0123, -0.0049,  ..., -0.0067, -0.0099,  0.0110],
        [ 0.0033, -0.0114,  0.0038,  ..., -0.0106,  0.0016,  0.0037],
        ...,
        [ 0.0003,  0.0090,  0.0007,  ...,  0.0045, -0.0153,  0.0115],
        [ 0.0131, -0.0087, -0.0132,  ...,  0.0073, -0.0014, -0.0143],
        [-0.0084,  0.0003,  0.0121,  ..., -0.0028,  0.0052, -0.0109]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.25.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[229],
            [162],
            [138],
            ...,
            [219],
            [ 97],
            [102]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0026,  0.0069, -0.0060,  ..., -0.0032, -0.0018, -0.0081],
        [-0.0060,  0.0024,  0.0061,  ..., -0.0023,  0.0085, -0.0003],
        [-0.0022, -0.0035, -0.0047,  ...,  0.0002,  0.0066, -0.0017],
        ...,
        [ 0.0056,  0.0031,  0.0074,  ..., -0.0002, -0.0021,  0.0063],
        [ 0.0081, -0.0029,  0.0011,  ..., -0.0050,  0.0067, -0.0076],
        [-0.0007,  0.0015, -0.0033,  ..., -0.0018, -0.0038,  0.0081]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.25.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 38],
            [172],
            [176],
            ...,
            [185],
            [125],
            [157]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 7.0801e-03, -1.1658e-02, -6.0730e-03,  ..., -1.0872e-04,
          1.1658e-02, -7.2021e-03],
        [ 9.5825e-03, -1.3977e-02, -1.8463e-03,  ..., -1.0071e-02,
         -8.7891e-03,  1.5259e-02],
        [-3.4943e-03,  3.4943e-03, -8.1539e-05,  ..., -2.1057e-03,
         -1.1475e-02, -1.4648e-02],
        ...,
        [ 1.2268e-02, -1.5320e-02,  5.5847e-03,  ...,  9.0332e-03,
          1.2756e-02, -8.0566e-03],
        [-4.9744e-03, -4.8218e-03,  1.4400e-04,  ...,  1.4587e-02,
          1.3428e-02, -8.8501e-03],
        [ 9.5825e-03,  1.4038e-03, -1.5320e-02,  ..., -1.3123e-02,
          4.0894e-03,  1.4587e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.25.input_layernorm.weight Parameter containing:
tensor([0.5469, 0.5547, 0.5430,  ..., 0.5508, 0.5625, 0.5508], device='cuda:0')
base_model.model.model.layers.25.post_attention_layernorm.weight Parameter containing:
tensor([0.4180, 0.4160, 0.4199,  ..., 0.4277, 0.4238, 0.4238], device='cuda:0')
base_model.model.model.layers.26.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[230],
            [ 84],
            [180],
            ...,
            [167],
            [ 37],
            [ 17]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0096,  0.0073, -0.0089,  ...,  0.0154, -0.0012,  0.0140],
        [-0.0145, -0.0008, -0.0148,  ..., -0.0062,  0.0121,  0.0143],
        [-0.0045, -0.0132,  0.0100,  ...,  0.0014, -0.0046, -0.0130],
        ...,
        [ 0.0039, -0.0105, -0.0049,  ...,  0.0071,  0.0013, -0.0053],
        [ 0.0123,  0.0153, -0.0019,  ...,  0.0137,  0.0049,  0.0027],
        [-0.0025,  0.0143,  0.0008,  ...,  0.0144, -0.0073,  0.0048]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.26.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[203],
            [ 70],
            [164],
            ...,
            [151],
            [ 23],
            [ 85]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0028,  0.0037,  0.0023,  ...,  0.0011, -0.0150,  0.0149],
        [-0.0025,  0.0128, -0.0146,  ...,  0.0098,  0.0033,  0.0057],
        [ 0.0052, -0.0129, -0.0053,  ..., -0.0016,  0.0151, -0.0039],
        ...,
        [ 0.0074,  0.0151,  0.0107,  ..., -0.0043,  0.0024, -0.0107],
        [-0.0057,  0.0144,  0.0010,  ...,  0.0144, -0.0076,  0.0073],
        [ 0.0039, -0.0037, -0.0019,  ..., -0.0038, -0.0133,  0.0090]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.26.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[243],
            [ 54],
            [180],
            ...,
            [ 77],
            [105],
            [124]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0093,  0.0031, -0.0099,  ..., -0.0048, -0.0015,  0.0098],
        [-0.0002,  0.0156,  0.0118,  ...,  0.0156,  0.0076,  0.0050],
        [-0.0079, -0.0127,  0.0062,  ..., -0.0092, -0.0154, -0.0140],
        ...,
        [-0.0097,  0.0076,  0.0027,  ..., -0.0072, -0.0084, -0.0030],
        [ 0.0084, -0.0032,  0.0025,  ...,  0.0109, -0.0036,  0.0032],
        [-0.0074, -0.0094,  0.0138,  ..., -0.0031,  0.0085, -0.0151]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.26.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[168],
            [ 37],
            [ 77],
            ...,
            [ 40],
            [210],
            [195]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-1.1475e-02,  1.1826e-03, -1.0254e-02,  ...,  8.5449e-03,
         -1.3657e-03,  6.0120e-03],
        [ 1.0742e-02, -7.5989e-03, -2.1362e-03,  ..., -8.4839e-03,
          5.6458e-03, -9.1553e-03],
        [ 1.3123e-02,  9.3384e-03,  1.8001e-05,  ...,  1.4465e-02,
          1.3962e-03,  1.1658e-02],
        ...,
        [ 1.4893e-02,  1.3489e-02, -7.3242e-03,  ..., -8.4686e-04,
          1.0071e-02,  1.3885e-03],
        [-1.5320e-02,  2.4261e-03, -5.7373e-03,  ...,  2.7008e-03,
         -1.2390e-02, -8.1177e-03],
        [-6.4087e-03,  7.5989e-03,  1.8539e-03,  ..., -1.4038e-02,
         -4.4861e-03,  4.8637e-04]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.26.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[165],
            [167],
            [ 27],
            ...,
            [ 75],
            [130],
            [102]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0121, -0.0070,  0.0081,  ..., -0.0062, -0.0065, -0.0104],
        [-0.0036,  0.0071, -0.0079,  ...,  0.0036,  0.0098, -0.0001],
        [ 0.0014, -0.0126, -0.0037,  ..., -0.0103,  0.0061, -0.0023],
        ...,
        [-0.0028, -0.0060,  0.0118,  ...,  0.0125, -0.0049,  0.0046],
        [ 0.0142,  0.0039,  0.0056,  ...,  0.0153, -0.0139,  0.0093],
        [ 0.0114, -0.0008, -0.0042,  ...,  0.0142,  0.0143, -0.0041]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.26.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[ 68],
            [ 72],
            [141],
            ...,
            [ 29],
            [219],
            [203]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0011, -0.0013, -0.0002,  ..., -0.0040,  0.0029, -0.0059],
        [-0.0026,  0.0007, -0.0060,  ...,  0.0007,  0.0011,  0.0076],
        [ 0.0010, -0.0013, -0.0077,  ...,  0.0061, -0.0028, -0.0033],
        ...,
        [ 0.0002, -0.0029,  0.0013,  ..., -0.0026, -0.0065,  0.0067],
        [-0.0070,  0.0073,  0.0050,  ..., -0.0018, -0.0087, -0.0038],
        [-0.0070, -0.0084, -0.0001,  ...,  0.0059,  0.0027, -0.0075]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.26.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 78],
            [121],
            [156],
            ...,
            [ 89],
            [ 52],
            [166]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 2.2583e-03, -3.3112e-03,  1.1658e-02,  ...,  6.2256e-03,
          1.2283e-03, -9.0332e-03],
        [-1.1353e-02, -1.6332e-05,  6.8970e-03,  ..., -6.6528e-03,
          1.3184e-02,  9.0942e-03],
        [ 9.5215e-03, -2.1076e-04,  3.5095e-03,  ...,  8.9722e-03,
          4.4556e-03,  7.8735e-03],
        ...,
        [ 9.1553e-03, -1.5869e-03,  7.0190e-03,  ...,  1.0147e-03,
         -5.2490e-03,  1.9264e-04],
        [ 1.2573e-02,  2.2278e-03, -1.0498e-02,  ..., -8.3618e-03,
         -1.7262e-04,  3.8605e-03],
        [ 1.2390e-02,  1.2451e-02, -1.6022e-04,  ...,  1.1215e-03,
         -6.5918e-03, -2.5940e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.26.input_layernorm.weight Parameter containing:
tensor([0.5156, 0.5352, 0.5352,  ..., 0.5195, 0.5430, 0.5312], device='cuda:0')
base_model.model.model.layers.26.post_attention_layernorm.weight Parameter containing:
tensor([0.4355, 0.4316, 0.4336,  ..., 0.4434, 0.4414, 0.4395], device='cuda:0')
base_model.model.model.layers.27.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 43],
            [153],
            [ 19],
            ...,
            [ 23],
            [ 76],
            [181]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0079, -0.0133, -0.0096,  ..., -0.0099, -0.0070,  0.0058],
        [ 0.0084, -0.0149, -0.0017,  ..., -0.0064,  0.0101, -0.0019],
        [ 0.0078,  0.0056, -0.0084,  ...,  0.0040,  0.0050,  0.0150],
        ...,
        [ 0.0128, -0.0078, -0.0111,  ...,  0.0117, -0.0050,  0.0116],
        [-0.0135,  0.0132, -0.0038,  ..., -0.0095,  0.0137, -0.0103],
        [ 0.0103, -0.0112,  0.0044,  ...,  0.0005, -0.0121,  0.0079]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.27.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[102],
            [154],
            [  4],
            ...,
            [220],
            [131],
            [ 41]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0074, -0.0115,  0.0123,  ..., -0.0148,  0.0093,  0.0069],
        [-0.0069, -0.0013,  0.0079,  ...,  0.0061, -0.0085,  0.0123],
        [-0.0030, -0.0114, -0.0044,  ...,  0.0096,  0.0139, -0.0120],
        ...,
        [ 0.0120,  0.0044, -0.0080,  ..., -0.0082,  0.0079, -0.0019],
        [ 0.0154, -0.0070, -0.0106,  ..., -0.0020, -0.0057,  0.0118],
        [ 0.0129, -0.0040,  0.0030,  ...,  0.0040, -0.0025,  0.0120]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.27.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[182],
            [ 22],
            [ 45],
            ...,
            [ 72],
            [ 83],
            [ 59]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0095,  0.0149,  0.0069,  ...,  0.0107, -0.0153, -0.0069],
        [-0.0137,  0.0060, -0.0069,  ...,  0.0120, -0.0046, -0.0055],
        [ 0.0109,  0.0074,  0.0073,  ...,  0.0016,  0.0120, -0.0020],
        ...,
        [ 0.0025,  0.0007, -0.0007,  ...,  0.0061, -0.0130,  0.0028],
        [-0.0041, -0.0121,  0.0126,  ...,  0.0112,  0.0084, -0.0081],
        [ 0.0077, -0.0126,  0.0079,  ...,  0.0092, -0.0100,  0.0098]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.27.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[133],
            [146],
            [ 84],
            ...,
            [137],
            [117],
            [131]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0099, -0.0004,  0.0010,  ...,  0.0103,  0.0126,  0.0115],
        [ 0.0033,  0.0148, -0.0106,  ...,  0.0105,  0.0136, -0.0053],
        [ 0.0132, -0.0156,  0.0032,  ...,  0.0083, -0.0043,  0.0099],
        ...,
        [ 0.0145,  0.0148,  0.0131,  ...,  0.0126, -0.0154, -0.0016],
        [-0.0049, -0.0110, -0.0036,  ...,  0.0048, -0.0124, -0.0085],
        [-0.0056,  0.0147, -0.0042,  ...,  0.0044,  0.0118,  0.0132]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.27.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[235],
            [ 83],
            [140],
            ...,
            [200],
            [165],
            [ 71]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 3.9978e-03, -1.5198e-02,  9.7046e-03,  ...,  6.9580e-03,
          8.7357e-04,  1.1169e-02],
        [-1.4282e-02,  7.7724e-05,  1.3977e-02,  ..., -7.1106e-03,
          6.2256e-03,  4.5300e-05],
        [-3.9062e-03,  1.2329e-02,  2.2278e-03,  ..., -9.0332e-03,
         -5.0964e-03, -9.7752e-05],
        ...,
        [-1.1780e-02, -7.2937e-03,  1.2684e-04,  ...,  1.0071e-03,
         -1.3000e-02,  6.5308e-03],
        [-5.0659e-03, -8.0566e-03, -1.3977e-02,  ..., -4.1723e-05,
         -3.8862e-05, -6.9275e-03],
        [ 1.4526e-02, -1.2146e-02, -5.9204e-03,  ..., -1.3000e-02,
          1.3977e-02,  3.6774e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.27.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[162],
            [224],
            [161],
            ...,
            [147],
            [ 93],
            [ 88]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0089,  0.0025,  0.0059,  ...,  0.0033,  0.0078,  0.0052],
        [-0.0089, -0.0081,  0.0016,  ...,  0.0031, -0.0014, -0.0031],
        [-0.0003,  0.0064, -0.0082,  ...,  0.0041, -0.0091,  0.0089],
        ...,
        [ 0.0029, -0.0080, -0.0014,  ...,  0.0085, -0.0025, -0.0078],
        [ 0.0044,  0.0037,  0.0014,  ...,  0.0083, -0.0010, -0.0076],
        [ 0.0073,  0.0039, -0.0081,  ...,  0.0055, -0.0004,  0.0089]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.27.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[107],
            [214],
            [ 38],
            ...,
            [ 35],
            [148],
            [ 39]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0072,  0.0086,  0.0008,  ...,  0.0069,  0.0073,  0.0040],
        [ 0.0055, -0.0060,  0.0041,  ...,  0.0020,  0.0104,  0.0068],
        [ 0.0086, -0.0008,  0.0017,  ...,  0.0089,  0.0073, -0.0140],
        ...,
        [ 0.0013,  0.0151,  0.0056,  ...,  0.0029,  0.0156,  0.0004],
        [-0.0066, -0.0121,  0.0131,  ...,  0.0002,  0.0087, -0.0120],
        [ 0.0066,  0.0147,  0.0132,  ...,  0.0110,  0.0005, -0.0092]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.27.input_layernorm.weight Parameter containing:
tensor([0.5430, 0.5508, 0.5508,  ..., 0.5508, 0.5508, 0.5547], device='cuda:0')
base_model.model.model.layers.27.post_attention_layernorm.weight Parameter containing:
tensor([0.4551, 0.4453, 0.4434,  ..., 0.4512, 0.4551, 0.4492], device='cuda:0')
base_model.model.model.layers.28.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[114],
            [169],
            [ 52],
            ...,
            [ 39],
            [158],
            [162]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0058, -0.0032, -0.0130,  ..., -0.0054,  0.0026, -0.0098],
        [ 0.0043, -0.0104,  0.0143,  ...,  0.0116, -0.0089,  0.0028],
        [ 0.0134,  0.0071,  0.0066,  ...,  0.0019, -0.0013, -0.0153],
        ...,
        [-0.0071,  0.0114,  0.0085,  ...,  0.0001, -0.0055,  0.0053],
        [ 0.0009,  0.0045, -0.0036,  ..., -0.0115, -0.0020,  0.0102],
        [-0.0081, -0.0117,  0.0095,  ..., -0.0019, -0.0058,  0.0090]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.28.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[119],
            [200],
            [219],
            ...,
            [124],
            [135],
            [ 69]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 1.3977e-02, -4.4556e-03, -1.0193e-02,  ..., -2.3365e-04,
         -1.1108e-02,  7.9956e-03],
        [-9.8877e-03, -8.8882e-04,  6.9580e-03,  ...,  6.2256e-03,
          1.5564e-02, -1.2085e-02],
        [-1.0315e-02,  4.7302e-03, -6.8970e-03,  ...,  1.3489e-02,
         -6.1951e-03,  8.4839e-03],
        ...,
        [-2.1076e-04,  1.4526e-02,  7.6771e-05,  ...,  7.1716e-03,
         -1.8539e-03,  1.4465e-02],
        [ 1.0132e-02, -2.5635e-03, -9.1553e-03,  ..., -6.7139e-03,
         -9.1553e-03, -2.2278e-03],
        [-1.4709e-02, -8.9111e-03, -6.1951e-03,  ...,  1.4038e-02,
          7.8735e-03,  1.0864e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.28.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[ 57],
            [205],
            [113],
            ...,
            [128],
            [ 29],
            [135]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0026, -0.0036,  0.0060,  ..., -0.0098, -0.0056,  0.0153],
        [ 0.0058,  0.0114,  0.0155,  ..., -0.0070,  0.0110, -0.0091],
        [-0.0121,  0.0092, -0.0027,  ...,  0.0031, -0.0066, -0.0122],
        ...,
        [ 0.0095, -0.0146, -0.0071,  ..., -0.0036,  0.0003, -0.0095],
        [ 0.0088,  0.0053,  0.0107,  ..., -0.0100,  0.0054,  0.0107],
        [-0.0071,  0.0101,  0.0074,  ..., -0.0074,  0.0078, -0.0079]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.28.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[155],
            [ 20],
            [ 77],
            ...,
            [102],
            [195],
            [120]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0078,  0.0111,  0.0092,  ...,  0.0063,  0.0090,  0.0099],
        [-0.0081, -0.0145, -0.0125,  ..., -0.0115, -0.0029,  0.0006],
        [ 0.0003, -0.0090, -0.0013,  ...,  0.0002, -0.0045,  0.0149],
        ...,
        [-0.0065,  0.0064, -0.0022,  ...,  0.0039, -0.0094, -0.0025],
        [ 0.0018, -0.0035, -0.0121,  ...,  0.0033,  0.0144, -0.0099],
        [ 0.0101,  0.0098,  0.0060,  ...,  0.0080, -0.0084,  0.0122]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.28.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[120],
            [111],
            [ 98],
            ...,
            [165],
            [149],
            [ 75]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0137,  0.0109,  0.0084,  ..., -0.0039,  0.0148, -0.0016],
        [-0.0020, -0.0089,  0.0108,  ..., -0.0018,  0.0104, -0.0115],
        [ 0.0061, -0.0047, -0.0150,  ...,  0.0017, -0.0009,  0.0132],
        ...,
        [ 0.0041,  0.0145,  0.0148,  ...,  0.0146,  0.0063, -0.0148],
        [-0.0025, -0.0074,  0.0079,  ...,  0.0073, -0.0142, -0.0054],
        [ 0.0050, -0.0128, -0.0019,  ...,  0.0093,  0.0059,  0.0020]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.28.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[158],
            [ 91],
            [136],
            ...,
            [149],
            [108],
            [228]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0018, -0.0060, -0.0075,  ..., -0.0019,  0.0067,  0.0042],
        [ 0.0039, -0.0057, -0.0084,  ...,  0.0015,  0.0093, -0.0016],
        [-0.0044, -0.0069, -0.0011,  ..., -0.0057, -0.0093,  0.0082],
        ...,
        [ 0.0020, -0.0056,  0.0036,  ..., -0.0030, -0.0070, -0.0022],
        [ 0.0052, -0.0060,  0.0020,  ..., -0.0095, -0.0009,  0.0022],
        [ 0.0028,  0.0036, -0.0051,  ...,  0.0073,  0.0003,  0.0059]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.28.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[216],
            [231],
            [ 46],
            ...,
            [130],
            [229],
            [150]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0109, -0.0137,  0.0145,  ...,  0.0037, -0.0005,  0.0037],
        [ 0.0051,  0.0018, -0.0004,  ..., -0.0017, -0.0032, -0.0070],
        [-0.0024,  0.0147, -0.0137,  ...,  0.0084, -0.0020,  0.0038],
        ...,
        [-0.0136, -0.0051, -0.0065,  ...,  0.0108,  0.0089,  0.0119],
        [-0.0104, -0.0148,  0.0003,  ...,  0.0092,  0.0093, -0.0028],
        [ 0.0026, -0.0110, -0.0075,  ..., -0.0091,  0.0048,  0.0037]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.28.input_layernorm.weight Parameter containing:
tensor([0.5625, 0.5664, 0.5586,  ..., 0.5469, 0.5664, 0.5586], device='cuda:0')
base_model.model.model.layers.28.post_attention_layernorm.weight Parameter containing:
tensor([0.4629, 0.4629, 0.4551,  ..., 0.4668, 0.4590, 0.4570], device='cuda:0')
base_model.model.model.layers.29.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[138],
            [ 87],
            [ 40],
            ...,
            [101],
            [202],
            [145]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0121,  0.0131, -0.0018,  ..., -0.0115, -0.0068, -0.0124],
        [ 0.0087,  0.0014,  0.0087,  ...,  0.0087,  0.0107,  0.0142],
        [-0.0074, -0.0077, -0.0144,  ..., -0.0081,  0.0150,  0.0099],
        ...,
        [-0.0090, -0.0077, -0.0095,  ...,  0.0046,  0.0139,  0.0042],
        [-0.0147,  0.0070, -0.0085,  ..., -0.0104, -0.0120, -0.0060],
        [ 0.0123, -0.0026, -0.0099,  ..., -0.0019,  0.0130,  0.0085]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.29.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[244],
            [ 59],
            [156],
            ...,
            [198],
            [ 48],
            [ 56]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 5.3406e-03,  1.3611e-02,  1.4771e-02,  ...,  1.2024e-02,
         -1.1780e-02, -4.0283e-03],
        [ 4.4861e-03,  7.1106e-03,  2.1667e-03,  ...,  1.2695e-02,
         -5.3787e-04,  4.3640e-03],
        [ 2.6703e-03,  1.1963e-02,  1.3794e-02,  ...,  6.8359e-03,
         -1.4160e-02, -7.0190e-03],
        ...,
        [-1.5259e-02, -1.2878e-02,  5.4321e-03,  ..., -5.1880e-03,
         -6.9275e-03, -1.0681e-02],
        [ 8.7891e-03, -1.0925e-02, -6.8054e-03,  ..., -1.2573e-02,
         -1.1902e-02, -8.4839e-03],
        [-4.1199e-03,  1.3733e-03, -1.0986e-02,  ...,  1.0132e-02,
         -4.1809e-03, -2.9206e-05]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.29.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[152],
            [197],
            [  3],
            ...,
            [154],
            [104],
            [ 76]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0069, -0.0119, -0.0084,  ...,  0.0146,  0.0060, -0.0022],
        [-0.0029, -0.0039, -0.0043,  ...,  0.0124,  0.0143,  0.0096],
        [-0.0093,  0.0109, -0.0094,  ...,  0.0088,  0.0060,  0.0076],
        ...,
        [-0.0119,  0.0136, -0.0077,  ..., -0.0123, -0.0097, -0.0096],
        [ 0.0102,  0.0129, -0.0104,  ..., -0.0130,  0.0115,  0.0135],
        [ 0.0050,  0.0127, -0.0014,  ...,  0.0003,  0.0006,  0.0109]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.29.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[110],
            [230],
            [ 91],
            ...,
            [ 86],
            [137],
            [178]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-4.4861e-03, -4.4632e-04,  1.6556e-03,  ..., -1.1368e-03,
          2.0981e-04, -1.0376e-02],
        [ 1.5564e-03,  9.1553e-04, -1.3489e-02,  ..., -7.1049e-05,
          2.8381e-03, -4.8065e-04],
        [ 4.8828e-03,  2.4872e-03,  1.0834e-03,  ...,  8.1787e-03,
          9.5825e-03, -8.7738e-04],
        ...,
        [-1.6861e-03, -1.1597e-02,  1.1780e-02,  ..., -9.5825e-03,
         -6.2561e-03, -1.3657e-03],
        [-8.3008e-03, -1.4404e-02,  3.6163e-03,  ...,  5.1575e-03,
         -9.3384e-03, -7.2327e-03],
        [-1.5259e-02,  1.2756e-02, -8.4229e-03,  ...,  9.0332e-03,
          1.1414e-02,  4.0283e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.29.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[173],
            [ 72],
            [184],
            ...,
            [ 42],
            [ 75],
            [ 68]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0059,  0.0007, -0.0132,  ..., -0.0056, -0.0022, -0.0031],
        [-0.0009,  0.0091,  0.0047,  ..., -0.0052,  0.0073,  0.0060],
        [ 0.0108, -0.0039, -0.0129,  ..., -0.0096,  0.0008, -0.0112],
        ...,
        [-0.0143,  0.0116, -0.0132,  ..., -0.0136, -0.0069, -0.0087],
        [ 0.0035,  0.0031,  0.0113,  ...,  0.0056, -0.0106, -0.0017],
        [-0.0063,  0.0059,  0.0004,  ...,  0.0099, -0.0084,  0.0030]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.29.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[233],
            [115],
            [197],
            ...,
            [199],
            [164],
            [ 87]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0091,  0.0052, -0.0084,  ...,  0.0052,  0.0094,  0.0041],
        [-0.0066, -0.0016, -0.0091,  ...,  0.0049, -0.0083,  0.0092],
        [ 0.0037,  0.0017, -0.0003,  ..., -0.0093, -0.0014,  0.0047],
        ...,
        [ 0.0084,  0.0093, -0.0063,  ...,  0.0019,  0.0036, -0.0095],
        [ 0.0065,  0.0063,  0.0056,  ...,  0.0095, -0.0079,  0.0003],
        [-0.0072,  0.0051, -0.0067,  ...,  0.0090, -0.0035,  0.0009]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.29.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[103],
            [146],
            [102],
            ...,
            [ 23],
            [ 70],
            [184]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 9.2163e-03, -4.1809e-03, -1.0864e-02,  ...,  1.1780e-02,
         -1.9379e-03, -2.8229e-03],
        [-4.2114e-03,  5.9814e-03,  1.2085e-02,  ..., -1.2589e-04,
         -1.4709e-02, -9.3994e-03],
        [ 7.9956e-03,  1.3657e-03,  5.8746e-04,  ...,  9.2163e-03,
          4.4584e-05,  9.1553e-03],
        ...,
        [ 1.1108e-02,  6.7749e-03, -3.1128e-03,  ..., -1.2390e-02,
          1.5015e-02,  7.9956e-03],
        [-3.7537e-03,  6.9580e-03,  1.3245e-02,  ..., -4.8828e-04,
         -2.9755e-03, -3.8147e-03],
        [ 7.5378e-03, -1.3855e-02, -1.4343e-02,  ..., -5.6763e-03,
         -3.8605e-03, -1.0071e-02]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.29.input_layernorm.weight Parameter containing:
tensor([0.5273, 0.5391, 0.5312,  ..., 0.5273, 0.5352, 0.5547], device='cuda:0')
base_model.model.model.layers.29.post_attention_layernorm.weight Parameter containing:
tensor([0.4688, 0.4707, 0.4668,  ..., 0.4727, 0.4746, 0.4727], device='cuda:0')
base_model.model.model.layers.30.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[114],
            [119],
            [176],
            ...,
            [137],
            [ 70],
            [118]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[-1.1353e-02,  8.6670e-03,  9.1553e-03,  ..., -1.5259e-02,
         -1.2024e-02, -1.1826e-03],
        [-6.1646e-03, -1.0620e-02, -1.0254e-02,  ..., -6.9580e-03,
         -1.1902e-02,  5.0049e-03],
        [-1.1475e-02, -4.5300e-05,  1.3367e-02,  ...,  4.3945e-03,
          1.5503e-02,  1.0529e-03],
        ...,
        [ 1.3184e-02,  1.1597e-02,  8.2016e-04,  ..., -1.1902e-02,
          1.1475e-02,  4.9133e-03],
        [ 4.2419e-03,  1.2817e-02, -1.5076e-02,  ..., -2.6321e-04,
          5.3101e-03, -7.2937e-03],
        [ 1.1536e-02, -1.3977e-02,  1.4191e-03,  ..., -9.3994e-03,
          8.9722e-03, -7.4463e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.30.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[211],
            [116],
            [197],
            ...,
            [217],
            [184],
            [179]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0140, -0.0115, -0.0124,  ..., -0.0097,  0.0024, -0.0145],
        [-0.0078, -0.0071,  0.0006,  ...,  0.0087, -0.0086,  0.0120],
        [ 0.0120, -0.0072, -0.0078,  ..., -0.0147, -0.0121, -0.0129],
        ...,
        [ 0.0031, -0.0132, -0.0153,  ...,  0.0013,  0.0034, -0.0126],
        [-0.0030, -0.0040, -0.0009,  ..., -0.0009,  0.0093, -0.0137],
        [-0.0130, -0.0003, -0.0026,  ...,  0.0125, -0.0078, -0.0024]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.30.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[ 97],
            [205],
            [142],
            ...,
            [ 71],
            [ 85],
            [ 20]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0061, -0.0004,  0.0151,  ..., -0.0039,  0.0114,  0.0062],
        [ 0.0125, -0.0092,  0.0007,  ..., -0.0132, -0.0084,  0.0058],
        [ 0.0115, -0.0150,  0.0053,  ...,  0.0027, -0.0014,  0.0139],
        ...,
        [ 0.0092, -0.0152,  0.0069,  ...,  0.0023,  0.0139, -0.0133],
        [-0.0051, -0.0025, -0.0073,  ..., -0.0119,  0.0106,  0.0065],
        [-0.0140, -0.0091,  0.0104,  ..., -0.0110,  0.0004, -0.0037]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.30.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[105],
            [ 84],
            [151],
            ...,
            [213],
            [183],
            [164]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0068, -0.0018, -0.0013,  ...,  0.0012, -0.0029, -0.0103],
        [ 0.0002,  0.0027, -0.0017,  ...,  0.0092, -0.0117, -0.0034],
        [-0.0104,  0.0025, -0.0142,  ..., -0.0071,  0.0114, -0.0024],
        ...,
        [ 0.0116, -0.0053,  0.0029,  ...,  0.0126, -0.0147, -0.0137],
        [ 0.0082, -0.0099,  0.0052,  ..., -0.0153, -0.0151, -0.0024],
        [ 0.0133,  0.0060,  0.0002,  ..., -0.0139, -0.0050, -0.0011]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.30.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[ 17],
            [ 98],
            [105],
            ...,
            [177],
            [119],
            [162]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0146,  0.0081, -0.0120,  ..., -0.0065, -0.0060, -0.0155],
        [ 0.0130,  0.0118,  0.0020,  ..., -0.0098, -0.0065, -0.0007],
        [-0.0050,  0.0090, -0.0144,  ...,  0.0022,  0.0101,  0.0020],
        ...,
        [ 0.0082, -0.0125, -0.0089,  ..., -0.0043, -0.0057,  0.0042],
        [-0.0060, -0.0091,  0.0109,  ..., -0.0008, -0.0058, -0.0001],
        [ 0.0095,  0.0130,  0.0126,  ...,  0.0136,  0.0123,  0.0106]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.30.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[195],
            [ 30],
            [ 89],
            ...,
            [182],
            [153],
            [100]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0093,  0.0070, -0.0054,  ..., -0.0073, -0.0056, -0.0050],
        [ 0.0081, -0.0068,  0.0003,  ...,  0.0089, -0.0061, -0.0005],
        [-0.0013, -0.0064,  0.0048,  ..., -0.0039, -0.0067, -0.0066],
        ...,
        [-0.0062,  0.0009, -0.0091,  ...,  0.0068,  0.0006, -0.0082],
        [-0.0044, -0.0001,  0.0030,  ..., -0.0055,  0.0005, -0.0025],
        [-0.0027,  0.0090, -0.0087,  ...,  0.0032, -0.0040,  0.0011]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.30.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[121],
            [149],
            [201],
            ...,
            [195],
            [132],
            [123]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0104,  0.0072,  0.0111,  ...,  0.0019,  0.0103,  0.0095],
        [ 0.0028, -0.0020, -0.0023,  ...,  0.0042, -0.0141, -0.0077],
        [ 0.0019, -0.0122,  0.0085,  ...,  0.0119, -0.0060, -0.0009],
        ...,
        [ 0.0016, -0.0112,  0.0139,  ...,  0.0110, -0.0074, -0.0107],
        [ 0.0020, -0.0034,  0.0088,  ...,  0.0089, -0.0050, -0.0100],
        [ 0.0004,  0.0109,  0.0056,  ...,  0.0078, -0.0068, -0.0115]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.30.input_layernorm.weight Parameter containing:
tensor([0.5742, 0.5820, 0.5625,  ..., 0.5508, 0.5625, 0.5820], device='cuda:0')
base_model.model.model.layers.30.post_attention_layernorm.weight Parameter containing:
tensor([0.4785, 0.4883, 0.4785,  ..., 0.4805, 0.4824, 0.4785], device='cuda:0')
base_model.model.model.layers.31.self_attn.q_proj.weight Parameter containing:
Parameter(Params4bit([[ 12],
            [222],
            [168],
            ...,
            [180],
            [ 29],
            [156]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0040, -0.0024, -0.0097,  ..., -0.0142, -0.0006,  0.0093],
        [ 0.0023, -0.0155,  0.0020,  ...,  0.0085,  0.0143,  0.0126],
        [-0.0071, -0.0151,  0.0052,  ..., -0.0153, -0.0009, -0.0152],
        ...,
        [-0.0051,  0.0003, -0.0112,  ..., -0.0073, -0.0139, -0.0103],
        [-0.0086, -0.0071, -0.0143,  ..., -0.0004,  0.0023, -0.0138],
        [-0.0041, -0.0114, -0.0124,  ...,  0.0059,  0.0101, -0.0031]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.31.self_attn.k_proj.weight Parameter containing:
Parameter(Params4bit([[ 33],
            [222],
            [182],
            ...,
            [148],
            [166],
            [120]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0078, -0.0072,  0.0139,  ...,  0.0049,  0.0028, -0.0101],
        [ 0.0123, -0.0142,  0.0030,  ..., -0.0007,  0.0037,  0.0060],
        [ 0.0079, -0.0040, -0.0037,  ...,  0.0091, -0.0138,  0.0026],
        ...,
        [-0.0139, -0.0081, -0.0034,  ...,  0.0066, -0.0095,  0.0018],
        [ 0.0053,  0.0088,  0.0101,  ...,  0.0042, -0.0106,  0.0115],
        [-0.0094,  0.0101,  0.0132,  ...,  0.0151, -0.0085,  0.0092]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.31.self_attn.v_proj.weight Parameter containing:
Parameter(Params4bit([[177],
            [ 81],
            [ 83],
            ...,
            [115],
            [252],
            [111]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0060, -0.0061, -0.0055,  ...,  0.0113,  0.0068, -0.0045],
        [-0.0095, -0.0150,  0.0010,  ..., -0.0127, -0.0136,  0.0085],
        [ 0.0137, -0.0027, -0.0004,  ...,  0.0032,  0.0106, -0.0153],
        ...,
        [-0.0060, -0.0046, -0.0024,  ...,  0.0096, -0.0048,  0.0002],
        [ 0.0005, -0.0098, -0.0020,  ..., -0.0078,  0.0071,  0.0049],
        [-0.0063, -0.0112, -0.0051,  ...,  0.0086,  0.0066,  0.0096]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.31.self_attn.o_proj.weight Parameter containing:
Parameter(Params4bit([[157],
            [102],
            [116],
            ...,
            [ 67],
            [189],
            [ 56]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0128,  0.0155, -0.0110,  ...,  0.0024, -0.0066,  0.0134],
        [ 0.0079,  0.0060,  0.0084,  ...,  0.0013,  0.0115, -0.0004],
        [-0.0038,  0.0056,  0.0048,  ...,  0.0096, -0.0145,  0.0007],
        ...,
        [ 0.0004, -0.0142, -0.0003,  ...,  0.0118, -0.0057,  0.0041],
        [ 0.0103,  0.0120,  0.0068,  ...,  0.0029,  0.0066, -0.0131],
        [-0.0006, -0.0121,  0.0060,  ..., -0.0014,  0.0112, -0.0128]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.31.mlp.gate_proj.weight Parameter containing:
Parameter(Params4bit([[132],
            [225],
            [124],
            ...,
            [188],
            [ 68],
            [168]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight Parameter containing:
tensor([[-0.0081, -0.0065,  0.0106,  ...,  0.0037, -0.0114,  0.0110],
        [ 0.0020,  0.0031,  0.0036,  ..., -0.0002,  0.0087, -0.0043],
        [ 0.0060, -0.0030, -0.0075,  ...,  0.0045, -0.0008,  0.0068],
        ...,
        [-0.0075,  0.0014, -0.0026,  ..., -0.0038,  0.0023,  0.0093],
        [-0.0022,  0.0089, -0.0023,  ...,  0.0056,  0.0057,  0.0024],
        [ 0.0109, -0.0052,  0.0072,  ...,  0.0142,  0.0095,  0.0041]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.31.mlp.down_proj.weight Parameter containing:
Parameter(Params4bit([[ 97],
            [ 83],
            [231],
            ...,
            [ 70],
            [ 56],
            [125]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0025,  0.0027,  0.0069,  ..., -0.0029, -0.0069, -0.0040],
        [-0.0002,  0.0027,  0.0007,  ..., -0.0087,  0.0052, -0.0009],
        [-0.0042, -0.0035, -0.0082,  ..., -0.0001,  0.0009, -0.0036],
        ...,
        [ 0.0009,  0.0017,  0.0012,  ..., -0.0049,  0.0066,  0.0017],
        [-0.0010,  0.0044,  0.0081,  ..., -0.0070,  0.0093, -0.0076],
        [-0.0090,  0.0018, -0.0080,  ...,  0.0043, -0.0067, -0.0006]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.31.mlp.up_proj.weight Parameter containing:
Parameter(Params4bit([[ 20],
            [210],
            [149],
            ...,
            [ 86],
            [134],
            [139]], device='cuda:0', dtype=torch.uint8))
base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight Parameter containing:
tensor([[ 0.0150,  0.0156, -0.0129,  ..., -0.0079, -0.0044, -0.0077],
        [-0.0025, -0.0113,  0.0023,  ...,  0.0059,  0.0110, -0.0018],
        [ 0.0019,  0.0135,  0.0031,  ...,  0.0145,  0.0144, -0.0043],
        ...,
        [ 0.0093,  0.0050, -0.0089,  ...,  0.0019,  0.0111,  0.0087],
        [ 0.0031,  0.0131, -0.0132,  ...,  0.0151,  0.0073,  0.0116],
        [-0.0129, -0.0145, -0.0152,  ...,  0.0142, -0.0030,  0.0012]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight Parameter containing:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
base_model.model.model.layers.31.input_layernorm.weight Parameter containing:
tensor([0.4863, 0.4844, 0.4355,  ..., 0.4316, 0.4551, 0.4805], device='cuda:0')
base_model.model.model.layers.31.post_attention_layernorm.weight Parameter containing:
tensor([0.4336, 0.4375, 0.4414,  ..., 0.4238, 0.4102, 0.4277], device='cuda:0')
base_model.model.model.norm.weight Parameter containing:
tensor([1.8672, 1.8672, 1.8047,  ..., 1.7188, 1.8281, 1.6016], device='cuda:0')
base_model.model.lm_head.weight Parameter containing:
tensor([[-0.0039,  0.0032, -0.0071,  ...,  0.0053, -0.0082,  0.0070],
        [-0.0315,  0.0466, -0.0023,  ..., -0.0211,  0.0173,  0.0334],
        [-0.0125,  0.0036,  0.0195,  ..., -0.0271,  0.0143, -0.0082],
        ...,
        [ 0.0229,  0.0255,  0.0315,  ...,  0.0067, -0.0092, -0.0058],
        [ 0.0080, -0.0088,  0.0063,  ..., -0.0293, -0.0200,  0.0337],
        [-0.0007,  0.0006, -0.0011,  ...,  0.0015, -0.0033,  0.0028]],
       device='cuda:0', dtype=torch.bfloat16)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 263.78it/s]
Loading cached processed dataset at /home/dqwang/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-dca0b82e0f0613ed.arrow
Loading cached split indices for dataset at /home/dqwang/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-688cb6df14df26f3.arrow and /home/dqwang/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-09885592c3e46f1e.arrow
Loading cached processed dataset at /home/dqwang/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee4e17346331630b.arrow
Loading cached processed dataset at /home/dqwang/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ac98af87b01ece38.arrow
Found cached dataset json (/home/dqwang/.cache/huggingface/datasets/json/default-1ceba72b81ec7865/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
Splitting train dataset in train and validation according to `eval_dataset_size`
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 580.41it/s]
wandb: Currently logged in as: tongchen. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.5
wandb: Run data is saved locally in /home/dqwang/scratch/tongchen/qlora/wandb/run-20230724_045220-iw78906h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama2_alpaca-clean_7b
wandb: ⭐️ View project at https://wandb.ai/tongchen/qlora-buffer
wandb: 🚀 View run at https://wandb.ai/tongchen/qlora-buffer/runs/iw78906h
trainable params: 79953920.0 || all params: 3660328960 || trainable: 2.1843370056007205
torch.bfloat16 422060032 0.1153065849032323
torch.uint8 3238002688 0.8846206784649213
torch.float32 266240 7.273663184633547e-05
  0%|          | 0/1875 [00:00<?, ?it/s]  0%|          | 1/1875 [01:00<31:17:25, 60.11s/it]  0%|          | 2/1875 [01:44<26:32:21, 51.01s/it]  0%|          | 3/1875 [02:16<21:59:08, 42.28s/it]  0%|          | 4/1875 [02:40<18:12:27, 35.03s/it]  0%|          | 5/1875 [02:55<14:29:59, 27.91s/it]  0%|          | 6/1875 [03:07<11:40:18, 22.48s/it]  0%|          | 7/1875 [03:57<16:18:56, 31.44s/it]  0%|          | 8/1875 [04:48<19:27:10, 37.51s/it]  0%|          | 9/1875 [05:24<19:18:07, 37.24s/it]  1%|          | 10/1875 [05:50<17:30:23, 33.79s/it]  1%|          | 11/1875 [06:07<14:50:44, 28.67s/it]  1%|          | 12/1875 [06:19<12:11:52, 23.57s/it]  1%|          | 13/1875 [06:57<14:27:03, 27.94s/it]  1%|          | 14/1875 [07:50<18:16:11, 35.34s/it]  1%|          | 15/1875 [08:28<18:41:50, 36.19s/it]  1%|          | 16/1875 [08:55<17:17:44, 33.49s/it]  1%|          | 17/1875 [09:13<14:54:44, 28.89s/it]  1%|          | 18/1875 [09:27<12:29:25, 24.21s/it]  1%|          | 19/1875 [09:51<12:29:40, 24.24s/it]  1%|          | 20/1875 [10:47<17:23:03, 33.74s/it]  1%|          | 21/1875 [11:30<18:48:53, 36.53s/it]  1%|          | 22/1875 [12:00<17:50:55, 34.68s/it]  1%|          | 23/1875 [12:21<15:41:33, 30.50s/it]  1%|▏         | 24/1875 [12:35<13:11:06, 25.64s/it]  1%|▏         | 25/1875 [12:47<11:02:04, 21.47s/it]  1%|▏         | 26/1875 [13:47<16:58:24, 33.05s/it]  1%|▏         | 27/1875 [14:33<18:58:15, 36.96s/it]  1%|▏         | 28/1875 [15:03<17:49:46, 34.75s/it]  2%|▏         | 29/1875 [15:23<15:38:01, 30.49s/it]  2%|▏         | 30/1875 [15:38<13:10:43, 25.71s/it]  2%|▏         | 31/1875 [15:50<11:02:49, 21.57s/it]  2%|▏         | 32/1875 [16:38<15:03:40, 29.42s/it]  2%|▏         | 33/1875 [17:26<18:00:06, 35.18s/it]  2%|▏         | 34/1875 [18:04<18:21:39, 35.90s/it]  2%|▏         | 35/1875 [18:28<16:36:25, 32.49s/it]  2%|▏         | 36/1875 [18:44<14:03:38, 27.52s/it]  2%|▏         | 37/1875 [18:57<11:46:33, 23.07s/it]  2%|▏         | 38/1875 [19:35<14:01:41, 27.49s/it]  2%|▏         | 39/1875 [20:28<17:59:23, 35.27s/it]  2%|▏         | 40/1875 [21:03<17:51:39, 35.04s/it]  2%|▏         | 41/1875 [21:28<16:23:11, 32.17s/it]  2%|▏         | 42/1875 [21:45<14:03:45, 27.62s/it]  2%|▏         | 43/1875 [21:58<11:45:38, 23.11s/it]  2%|▏         | 44/1875 [22:22<11:56:17, 23.47s/it]  2%|▏         | 45/1875 [23:20<17:09:09, 33.74s/it]  2%|▏         | 46/1875 [24:04<18:46:42, 36.96s/it]  3%|▎         | 47/1875 [24:33<17:31:51, 34.52s/it]  3%|▎         | 48/1875 [24:54<15:30:40, 30.56s/it]  3%|▎         | 49/1875 [25:10<13:11:41, 26.01s/it]  3%|▎         | 50/1875 [25:21<10:57:28, 21.62s/it]  3%|▎         | 51/1875 [26:20<16:36:31, 32.78s/it]  3%|▎         | 52/1875 [27:07<18:46:37, 37.08s/it]  3%|▎         | 53/1875 [27:40<18:04:40, 35.72s/it]  3%|▎         | 54/1875 [28:04<16:18:49, 32.25s/it]  3%|▎         | 55/1875 [28:20<13:49:10, 27.34s/it]  3%|▎         | 56/1875 [28:31<11:27:08, 22.67s/it]  3%|▎         | 57/1875 [29:20<15:20:25, 30.38s/it]  3%|▎         | 58/1875 [30:10<18:20:29, 36.34s/it]  3%|▎         | 59/1875 [30:46<18:13:15, 36.12s/it]  3%|▎         | 60/1875 [31:11<16:32:11, 32.80s/it]  3%|▎         | 61/1875 [31:29<14:16:37, 28.33s/it]  3%|▎         | 62/1875 [31:41<11:51:44, 23.55s/it]  3%|▎         | 63/1875 [32:19<14:01:43, 27.87s/it]  3%|▎         | 64/1875 [33:12<17:51:26, 35.50s/it]  3%|▎         | 65/1875 [33:53<18:37:25, 37.04s/it]  4%|▎         | 66/1875 [34:20<17:05:29, 34.01s/it]  4%|▎         | 67/1875 [34:37<14:36:31, 29.09s/it]  4%|▎         | 68/1875 [34:50<12:08:59, 24.21s/it]  4%|▎         | 69/1875 [35:15<12:16:36, 24.47s/it]  4%|▎         | 70/1875 [36:11<17:02:02, 33.97s/it]  4%|▍         | 71/1875 [36:55<18:28:11, 36.86s/it]  4%|▍         | 72/1875 [37:27<17:43:47, 35.40s/it]  4%|▍         | 73/1875 [37:48<15:34:20, 31.11s/it]  4%|▍         | 74/1875 [38:02<13:01:28, 26.03s/it]  4%|▍         | 75/1875 [38:14<10:47:06, 21.57s/it]  4%|▍         | 76/1875 [39:14<16:33:27, 33.13s/it]  4%|▍         | 77/1875 [40:03<19:01:44, 38.10s/it]  4%|▍         | 78/1875 [40:39<18:35:10, 37.23s/it]  4%|▍         | 79/1875 [41:03<16:37:30, 33.32s/it]  4%|▍         | 80/1875 [41:19<14:00:44, 28.10s/it]  4%|▍         | 81/1875 [41:30<11:33:10, 23.18s/it]  4%|▍         | 82/1875 [42:19<15:16:38, 30.67s/it]  4%|▍         | 83/1875 [43:09<18:10:51, 36.52s/it]  4%|▍         | 84/1875 [43:44<17:56:02, 36.05s/it]  5%|▍         | 85/1875 [44:08<16:12:36, 32.60s/it]  5%|▍         | 86/1875 [44:25<13:51:08, 27.87s/it]  5%|▍         | 87/1875 [44:37<11:31:21, 23.20s/it]  5%|▍         | 88/1875 [45:15<13:38:24, 27.48s/it]  5%|▍         | 89/1875 [46:13<18:12:08, 36.69s/it]  5%|▍         | 90/1875 [46:55<19:02:28, 38.40s/it]  5%|▍         | 91/1875 [47:23<17:26:50, 35.21s/it]  5%|▍         | 92/1875 [47:43<15:12:08, 30.69s/it]  5%|▍         | 93/1875 [47:56<12:35:38, 25.44s/it]  5%|▌         | 94/1875 [48:21<12:28:14, 25.21s/it]  5%|▌         | 95/1875 [49:15<16:47:07, 33.95s/it]  5%|▌         | 96/1875 [50:03<18:47:24, 38.02s/it]  5%|▌         | 97/1875 [50:36<17:58:13, 36.39s/it]  5%|▌         | 98/1875 [50:55<15:26:57, 31.30s/it]  5%|▌         | 99/1875 [51:09<12:51:05, 26.05s/it]  5%|▌         | 100/1875 [51:20<10:39:49, 21.63s/it]                                                       5%|▌         | 100/1875 [51:20<10:39:49, 21.63s/it]  5%|▌         | 101/1875 [52:24<16:51:07, 34.20s/it]  5%|▌         | 102/1875 [53:12<18:56:14, 38.45s/it]  5%|▌         | 103/1875 [53:45<18:05:39, 36.76s/it]  6%|▌         | 104/1875 [54:08<16:02:53, 32.62s/it]  6%|▌         | 105/1875 [54:25<13:48:51, 28.10s/it]  6%|▌         | 106/1875 [54:48<13:03:44, 26.58s/it]  6%|▌         | 107/1875 [55:36<16:07:22, 32.83s/it]  6%|▌         | 108/1875 [56:24<18:24:22, 37.50s/it]  6%|▌         | 109/1875 [56:56<17:37:21, 35.92s/it]  6%|▌         | 110/1875 [57:20<15:47:17, 32.20s/it]  6%|▌         | 111/1875 [57:35<13:19:07, 27.18s/it]  6%|▌         | 112/1875 [57:48<11:07:02, 22.70s/it]  6%|▌         | 113/1875 [58:25<13:12:06, 26.97s/it]  6%|▌         | 114/1875 [59:18<17:04:06, 34.89s/it]  6%|▌         | 115/1875 [59:57<17:37:30, 36.05s/it]  6%|▌         | 116/1875 [1:00:25<16:24:48, 33.59s/it]  6%|▌         | 117/1875 [1:00:44<14:16:23, 29.23s/it]  6%|▋         | 118/1875 [1:00:57<11:55:00, 24.42s/it]  6%|▋         | 119/1875 [1:01:21<11:55:24, 24.44s/it]  6%|▋         | 120/1875 [1:02:16<16:24:25, 33.66s/it]  6%|▋         | 121/1875 [1:02:59<17:39:59, 36.26s/it]  7%|▋         | 122/1875 [1:03:29<16:42:48, 34.32s/it]  7%|▋         | 123/1875 [1:03:48<14:35:02, 29.97s/it]  7%|▋         | 124/1875 [1:04:02<12:14:56, 25.18s/it]  7%|▋         | 125/1875 [1:04:14<10:19:15, 21.23s/it]  7%|▋         | 126/1875 [1:05:15<16:01:01, 32.97s/it]  7%|▋         | 127/1875 [1:06:04<18:23:07, 37.86s/it]  7%|▋         | 128/1875 [1:06:35<17:23:55, 35.85s/it]  7%|▋         | 129/1875 [1:06:58<15:25:28, 31.80s/it]  7%|▋         | 130/1875 [1:07:12<12:50:37, 26.50s/it]  7%|▋         | 131/1875 [1:07:23<10:40:26, 22.03s/it]  7%|▋         | 132/1875 [1:08:11<14:22:27, 29.69s/it]  7%|▋         | 133/1875 [1:09:01<17:16:35, 35.70s/it]  7%|▋         | 134/1875 [1:09:37<17:19:27, 35.82s/it]  7%|▋         | 135/1875 [1:10:02<15:49:06, 32.73s/it]  7%|▋         | 136/1875 [1:10:18<13:19:39, 27.59s/it]  7%|▋         | 137/1875 [1:10:30<11:02:58, 22.89s/it]  7%|▋         | 138/1875 [1:11:06<12:57:50, 26.87s/it]  7%|▋         | 139/1875 [1:12:00<16:51:44, 34.97s/it]  7%|▋         | 140/1875 [1:12:38<17:21:19, 36.01s/it]  8%|▊         | 141/1875 [1:13:07<16:13:49, 33.70s/it]  8%|▊         | 142/1875 [1:13:25<14:04:58, 29.25s/it]  8%|▊         | 143/1875 [1:13:38<11:40:23, 24.26s/it]  8%|▊         | 144/1875 [1:14:03<11:41:54, 24.33s/it]  8%|▊         | 145/1875 [1:15:00<16:31:47, 34.40s/it]  8%|▊         | 146/1875 [1:15:46<18:12:13, 37.90s/it]  8%|▊         | 147/1875 [1:16:15<16:53:30, 35.19s/it]  8%|▊         | 148/1875 [1:16:36<14:45:09, 30.75s/it]  8%|▊         | 149/1875 [1:16:49<12:15:35, 25.57s/it]  8%|▊         | 150/1875 [1:17:01<10:13:24, 21.34s/it]  8%|▊         | 151/1875 [1:18:01<15:51:36, 33.12s/it]  8%|▊         | 152/1875 [1:18:46<17:34:41, 36.73s/it]  8%|▊         | 153/1875 [1:19:17<16:40:21, 34.86s/it]  8%|▊         | 154/1875 [1:19:41<15:05:17, 31.56s/it]  8%|▊         | 155/1875 [1:19:56<12:44:56, 26.68s/it]  8%|▊         | 156/1875 [1:20:08<10:38:28, 22.29s/it]  8%|▊         | 157/1875 [1:20:56<14:18:12, 29.97s/it]  8%|▊         | 158/1875 [1:21:43<16:43:25, 35.06s/it]  8%|▊         | 159/1875 [1:22:19<16:54:12, 35.46s/it]  9%|▊         | 160/1875 [1:22:42<15:03:33, 31.61s/it]  9%|▊         | 161/1875 [1:22:57<12:42:10, 26.68s/it]  9%|▊         | 162/1875 [1:23:09<10:35:33, 22.26s/it]  9%|▊         | 163/1875 [1:23:47<12:46:31, 26.86s/it]  9%|▊         | 164/1875 [1:24:37<16:02:06, 33.74s/it]  9%|▉         | 165/1875 [1:25:13<16:27:54, 34.66s/it]  9%|▉         | 166/1875 [1:25:41<15:24:02, 32.44s/it]  9%|▉         | 167/1875 [1:25:58<13:17:25, 28.01s/it]  9%|▉         | 168/1875 [1:26:11<11:06:13, 23.42s/it]  9%|▉         | 169/1875 [1:26:36<11:17:15, 23.82s/it]  9%|▉         | 170/1875 [1:27:32<15:53:44, 33.56s/it]  9%|▉         | 171/1875 [1:28:15<17:09:45, 36.26s/it]  9%|▉         | 172/1875 [1:28:43<16:03:57, 33.96s/it]  9%|▉         | 173/1875 [1:29:04<14:11:52, 30.03s/it]  9%|▉         | 174/1875 [1:29:16<11:41:33, 24.75s/it]  9%|▉         | 175/1875 [1:29:28<9:47:49, 20.75s/it]   9%|▉         | 176/1875 [1:30:26<15:07:01, 32.03s/it]  9%|▉         | 177/1875 [1:31:08<16:25:05, 34.81s/it]  9%|▉         | 178/1875 [1:31:40<16:05:45, 34.15s/it] 10%|▉         | 179/1875 [1:32:01<14:12:00, 30.14s/it] 10%|▉         | 180/1875 [1:32:14<11:49:37, 25.12s/it] 10%|▉         | 181/1875 [1:32:27<10:02:37, 21.34s/it] 10%|▉         | 182/1875 [1:33:16<13:59:44, 29.76s/it] 10%|▉         | 183/1875 [1:34:07<16:54:29, 35.98s/it] 10%|▉         | 184/1875 [1:34:42<16:52:02, 35.91s/it] 10%|▉         | 185/1875 [1:35:07<15:17:42, 32.58s/it] 10%|▉         | 186/1875 [1:35:23<12:57:28, 27.62s/it] 10%|▉         | 187/1875 [1:35:35<10:46:31, 22.98s/it]torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 503])
torch.Size([8, 528])
torch.Size([8, 489])
torch.Size([8, 526])
torch.Size([8, 449])
torch.Size([8, 528])
torch.Size([8, 429])
torch.Size([8, 440])
torch.Size([8, 431])
torch.Size([8, 416])
torch.Size([8, 466])
torch.Size([8, 394])
torch.Size([8, 404])
torch.Size([8, 429])
torch.Size([8, 392])
torch.Size([8, 380])
torch.Size([8, 391])
torch.Size([8, 346])
torch.Size([8, 361])
torch.Size([8, 364])
torch.Size([8, 334])
torch.Size([8, 398])
torch.Size([8, 425])
torch.Size([8, 371])
torch.Size([8, 383])
torch.Size([8, 368])
torch.Size([8, 327])
torch.Size([8, 268])
torch.Size([8, 342])
torch.Size([8, 310])
torch.Size([8, 236])
torch.Size([8, 247])
torch.Size([8, 266])
torch.Size([8, 291])
torch.Size([8, 274])
torch.Size([8, 277])
torch.Size([8, 220])
torch.Size([8, 240])
torch.Size([8, 213])
torch.Size([8, 278])
torch.Size([8, 223])
torch.Size([8, 238])
torch.Size([8, 179])
torch.Size([8, 229])
torch.Size([8, 279])
torch.Size([8, 279])
torch.Size([8, 227])
torch.Size([8, 255])
torch.Size([8, 240])
torch.Size([8, 205])
torch.Size([8, 243])
torch.Size([8, 183])
torch.Size([8, 152])
torch.Size([8, 146])
torch.Size([8, 132])
torch.Size([8, 145])
torch.Size([8, 125])
torch.Size([8, 137])
torch.Size([8, 142])
torch.Size([8, 131])
torch.Size([8, 208])
torch.Size([8, 183])
torch.Size([8, 141])
torch.Size([8, 91])
torch.Size([8, 82])
torch.Size([8, 142])
torch.Size([8, 87])
torch.Size([8, 89])
torch.Size([8, 82])
torch.Size([8, 113])
torch.Size([8, 66])
torch.Size([8, 67])
torch.Size([8, 128])
torch.Size([8, 105])
torch.Size([8, 64])
torch.Size([8, 67])
torch.Size([8, 83])
torch.Size([8, 70])
torch.Size([8, 53])
torch.Size([8, 48])
torch.Size([8, 47])
torch.Size([8, 52])
torch.Size([8, 75])
torch.Size([8, 94])
torch.Size([8, 96])
torch.Size([8, 50])
torch.Size([8, 71])
torch.Size([8, 66])
torch.Size([8, 44])
torch.Size([8, 40])
torch.Size([8, 44])
torch.Size([8, 65])
torch.Size([8, 53])
torch.Size([8, 35])
torch.Size([8, 47])
torch.Size([8, 44])
torch.Size([8, 34])
torch.Size([8, 35])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 516])
torch.Size([8, 515])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 516])
torch.Size([8, 528])
torch.Size([8, 441])
torch.Size([8, 471])
torch.Size([8, 399])
torch.Size([8, 403])
torch.Size([8, 458])
torch.Size([8, 528])
torch.Size([8, 451])
torch.Size([8, 394])
torch.Size([8, 528])
torch.Size([8, 404])
torch.Size([8, 360])
torch.Size([8, 371])
torch.Size([8, 439])
torch.Size([8, 348])
torch.Size([8, 340])
torch.Size([8, 426])
torch.Size([8, 358])
torch.Size([8, 317])
torch.Size([8, 416])
torch.Size([8, 366])
torch.Size([8, 367])
torch.Size([8, 349])
torch.Size([8, 255])
torch.Size([8, 283])
torch.Size([8, 274])
torch.Size([8, 283])
torch.Size([8, 256])
torch.Size([8, 304])
torch.Size([8, 214])
torch.Size([8, 239])
torch.Size([8, 369])
torch.Size([8, 262])
torch.Size([8, 200])
torch.Size([8, 214])
torch.Size([8, 268])
torch.Size([8, 257])
torch.Size([8, 295])
torch.Size([8, 213])
torch.Size([8, 187])
torch.Size([8, 180])
torch.Size([8, 157])
torch.Size([8, 244])
torch.Size([8, 172])
torch.Size([8, 139])
torch.Size([8, 211])
torch.Size([8, 183])
torch.Size([8, 194])
torch.Size([8, 136])
torch.Size([8, 162])
torch.Size([8, 105])
torch.Size([8, 129])
torch.Size([8, 81])
torch.Size([8, 138])
torch.Size([8, 91])
torch.Size([8, 164])
torch.Size([8, 89])
torch.Size([8, 131])
torch.Size([8, 128])
torch.Size([8, 187])
torch.Size([8, 79])
torch.Size([8, 90])
torch.Size([8, 64])
torch.Size([8, 92])
torch.Size([8, 97])
torch.Size([8, 103])
torch.Size([8, 90])
torch.Size([8, 73])
torch.Size([8, 59])
torch.Size([8, 52])
torch.Size([8, 79])
torch.Size([8, 47])
torch.Size([8, 57])
torch.Size([8, 77])
torch.Size([8, 80])
torch.Size([8, 58])
torch.Size([8, 48])
torch.Size([8, 47])
torch.Size([8, 49])
torch.Size([8, 45])
torch.Size([8, 65])
torch.Size([8, 46])
torch.Size([8, 57])
torch.Size([8, 66])
torch.Size([8, 60])
torch.Size([8, 40])
torch.Size([8, 50])
torch.Size([8, 40])
torch.Size([8, 53])
torch.Size([8, 36])
torch.Size([8, 43])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 492])
torch.Size([8, 463])
torch.Size([8, 460])
torch.Size([8, 400])
torch.Size([8, 423])
torch.Size([8, 528])
torch.Size([8, 385])
torch.Size([8, 451])
torch.Size([8, 400])
torch.Size([8, 424])
torch.Size([8, 359])
torch.Size([8, 466])
torch.Size([8, 391])
torch.Size([8, 378])
torch.Size([8, 363])
torch.Size([8, 421])
torch.Size([8, 411])
torch.Size([8, 352])
torch.Size([8, 354])
torch.Size([8, 279])
torch.Size([8, 260])
torch.Size([8, 369])
torch.Size([8, 346])
torch.Size([8, 298])
torch.Size([8, 229])
torch.Size([8, 339])
torch.Size([8, 274])
torch.Size([8, 239])
torch.Size([8, 346])
torch.Size([8, 298])
torch.Size([8, 239])
torch.Size([8, 217])
torch.Size([8, 254])
torch.Size([8, 347])
torch.Size([8, 241])
torch.Size([8, 235])
torch.Size([8, 224])
torch.Size([8, 177])
torch.Size([8, 241])
torch.Size([8, 182])
torch.Size([8, 177])
torch.Size([8, 168])
torch.Size([8, 237])
torch.Size([8, 144])
torch.Size([8, 247])
torch.Size([8, 157])
torch.Size([8, 138])
torch.Size([8, 128])
torch.Size([8, 169])
torch.Size([8, 148])
torch.Size([8, 176])
torch.Size([8, 169])
torch.Size([8, 147])
torch.Size([8, 121])
torch.Size([8, 100])
torch.Size([8, 97])
torch.Size([8, 118])
torch.Size([8, 80])
torch.Size([8, 134])
torch.Size([8, 81])
torch.Size([8, 92])
torch.Size([8, 82])
torch.Size([8, 90])
torch.Size([8, 97])
torch.Size([8, 93])
torch.Size([8, 97])
torch.Size([8, 71])
torch.Size([8, 56])
torch.Size([8, 91])
torch.Size([8, 75])
torch.Size([8, 74])
torch.Size([8, 77])
torch.Size([8, 90])
torch.Size([8, 79])
torch.Size([8, 50])
torch.Size([8, 49])
torch.Size([8, 65])
torch.Size([8, 58])
torch.Size([8, 60])
torch.Size([8, 92])
torch.Size([8, 43])
torch.Size([8, 45])
torch.Size([8, 52])
torch.Size([8, 47])
torch.Size([8, 60])
torch.Size([8, 51])
torch.Size([8, 41])
torch.Size([8, 57])
torch.Size([8, 37])
torch.Size([8, 39])
torch.Size([8, 39])
torch.Size([8, 32])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 488])
torch.Size([8, 528])
torch.Size([8, 496])
torch.Size([8, 457])
torch.Size([8, 453])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 436])
torch.Size([8, 425])
torch.Size([8, 452])
torch.Size([8, 411])
torch.Size([8, 405])
torch.Size([8, 377])
torch.Size([8, 408])
torch.Size([8, 447])
torch.Size([8, 394])
torch.Size([8, 384])
torch.Size([8, 528])
torch.Size([8, 350])
torch.Size([8, 335])
torch.Size([8, 310])
torch.Size([8, 378])
torch.Size([8, 286])
torch.Size([8, 336])
torch.Size([8, 301])
torch.Size([8, 528])
torch.Size([8, 353])
torch.Size([8, 298])
torch.Size([8, 261])
torch.Size([8, 335])
torch.Size([8, 291])
torch.Size([8, 241])
torch.Size([8, 255])
torch.Size([8, 242])
torch.Size([8, 259])
torch.Size([8, 255])
torch.Size([8, 248])
torch.Size([8, 261])
torch.Size([8, 204])
torch.Size([8, 340])
torch.Size([8, 298])
torch.Size([8, 203])
torch.Size([8, 205])
torch.Size([8, 162])
torch.Size([8, 221])
torch.Size([8, 210])
torch.Size([8, 201])
torch.Size([8, 164])
torch.Size([8, 210])
torch.Size([8, 212])
torch.Size([8, 241])
torch.Size([8, 196])
torch.Size([8, 173])
torch.Size([8, 176])
torch.Size([8, 121])
torch.Size([8, 129])
torch.Size([8, 121])
torch.Size([8, 98])
torch.Size([8, 155])
torch.Size([8, 88])
torch.Size([8, 99])
torch.Size([8, 92])
torch.Size([8, 96])
torch.Size([8, 91])
torch.Size([8, 75])
torch.Size([8, 231])
torch.Size([8, 75])
torch.Size([8, 75])
torch.Size([8, 90])
torch.Size([8, 95])
torch.Size([8, 75])
torch.Size([8, 58])
torch.Size([8, 81])
torch.Size([8, 67])
torch.Size([8, 49])
torch.Size([8, 69])
torch.Size([8, 99])
torch.Size([8, 61])
torch.Size([8, 74])
torch.Size([8, 73])
torch.Size([8, 53])
torch.Size([8, 62])
torch.Size([8, 81])
torch.Size([8, 49])
torch.Size([8, 65])
torch.Size([8, 75])
torch.Size([8, 74])
torch.Size([8, 52])
torch.Size([8, 48])
torch.Size([8, 55])
torch.Size([8, 48])
torch.Size([8, 40])
torch.Size([8, 40])
torch.Size([8, 49])
torch.Size([8, 55])
torch.Size([8, 34])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 491])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 469])
torch.Size([8, 484])
torch.Size([8, 434])
torch.Size([8, 465])
torch.Size([8, 427])
torch.Size([8, 461])
torch.Size([8, 463])
torch.Size([8, 528])
torch.Size([8, 417])
torch.Size([8, 458])
torch.Size([8, 388])
torch.Size([8, 528])
torch.Size([8, 401])
torch.Size([8, 320])
torch.Size([8, 405])
torch.Size([8, 372])
torch.Size([8, 360])
torch.Size([8, 326])
torch.Size([8, 528])
torch.Size([8, 337])
torch.Size([8, 287])
torch.Size([8, 351])
torch.Size([8, 311])
torch.Size([8, 316])
torch.Size([8, 240])
torch.Size([8, 266])
torch.Size([8, 290])
torch.Size([8, 274])
torch.Size([8, 236])
torch.Size([8, 234])
torch.Size([8, 214])
torch.Size([8, 233])
torch.Size([8, 198])
torch.Size([8, 213])
torch.Size([8, 196])
torch.Size([8, 215])
torch.Size([8, 196])
torch.Size([8, 229])
torch.Size([8, 232])
torch.Size([8, 192])
torch.Size([8, 250])
torch.Size([8, 206])
torch.Size([8, 182])
torch.Size([8, 174])
torch.Size([8, 127])
torch.Size([8, 181])
torch.Size([8, 131])
torch.Size([8, 128])
torch.Size([8, 121])
torch.Size([8, 140])
torch.Size([8, 113])
torch.Size([8, 175])
torch.Size([8, 116])
torch.Size([8, 106])
torch.Size([8, 106])
torch.Size([8, 152])
torch.Size([8, 123])
torch.Size([8, 101])
torch.Size([8, 148])
torch.Size([8, 94])
torch.Size([8, 83])
torch.Size([8, 128])
torch.Size([8, 72])
torch.Size([8, 54])
torch.Size([8, 72])
torch.Size([8, 107])
torch.Size([8, 82])
torch.Size([8, 55])
torch.Size([8, 120])
torch.Size([8, 74])
torch.Size([8, 47])
torch.Size([8, 56])
torch.Size([8, 65])
torch.Size([8, 76])
torch.Size([8, 45])
torch.Size([8, 103])
torch.Size([8, 65])
torch.Size([8, 47])
torch.Size([8, 69])
torch.Size([8, 56])
torch.Size([8, 54])
torch.Size([8, 47])
torch.Size([8, 48])
torch.Size([8, 86])
torch.Size([8, 46])
torch.Size([8, 72])
torch.Size([8, 46])
torch.Size([8, 35])
torch.Size([8, 36])
torch.Size([8, 41])
torch.Size([8, 39])
torch.Size([8, 40])
torch.Size([8, 32])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 477])
torch.Size([8, 504])
torch.Size([8, 491])
torch.Size([8, 470])
torch.Size([8, 491])
torch.Size([8, 528])
torch.Size([8, 452])
torch.Size([8, 425])
torch.Size([8, 463])
torch.Size([8, 400])
torch.Size([8, 476])
torch.Size([8, 408])
torch.Size([8, 528])
torch.Size([8, 378])
torch.Size([8, 359])
torch.Size([8, 346])
torch.Size([8, 370])
torch.Size([8, 399])
torch.Size([8, 367])
torch.Size([8, 380])
torch.Size([8, 366])
torch.Size([8, 332])
torch.Size([8, 425])
torch.Size([8, 412])
torch.Size([8, 355])
torch.Size([8, 356])
torch.Size([8, 366])
torch.Size([8, 371])
torch.Size([8, 426])
torch.Size([8, 381])
torch.Size([8, 328])
torch.Size([8, 258])
torch.Size([8, 336])
torch.Size([8, 225])
torch.Size([8, 246])
torch.Size([8, 348])
torch.Size([8, 213])
torch.Size([8, 241])
torch.Size([8, 258])
torch.Size([8, 230])
torch.Size([8, 166])
torch.Size([8, 174])
torch.Size([8, 190])
torch.Size([8, 219])
torch.Size([8, 215])
torch.Size([8, 184])
torch.Size([8, 157])
torch.Size([8, 236])
torch.Size([8, 169])
torch.Size([8, 239])
torch.Size([8, 206])
torch.Size([8, 147])
torch.Size([8, 115])
torch.Size([8, 212])
torch.Size([8, 114])
torch.Size([8, 163])
torch.Size([8, 167])
torch.Size([8, 111])
torch.Size([8, 98])
torch.Size([8, 140])
torch.Size([8, 105])
torch.Size([8, 94])
torch.Size([8, 118])
torch.Size([8, 84])
torch.Size([8, 98])
torch.Size([8, 136])
torch.Size([8, 119])
torch.Size([8, 82])
torch.Size([8, 52])
torch.Size([8, 77])
torch.Size([8, 75])
torch.Size([8, 79])
torch.Size([8, 98])
torch.Size([8, 68])
torch.Size([8, 76])
torch.Size([8, 81])
torch.Size([8, 59])
torch.Size([8, 73])
torch.Size([8, 89])
torch.Size([8, 79])
torch.Size([8, 70])
torch.Size([8, 69])
torch.Size([8, 82])
torch.Size([8, 55])
torch.Size([8, 47])
torch.Size([8, 45])
torch.Size([8, 56])
torch.Size([8, 55])
torch.Size([8, 71])
torch.Size([8, 61])
torch.Size([8, 58])
torch.Size([8, 47])
torch.Size([8, 51])
torch.Size([8, 50])
torch.Size([8, 45])
torch.Size([8, 37])
torch.Size([8, 39])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 507])
torch.Size([8, 528])
torch.Size([8, 419])
torch.Size([8, 455])
torch.Size([8, 528])
torch.Size([8, 517])
torch.Size([8, 469])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 336])
torch.Size([8, 491])
torch.Size([8, 360])
torch.Size([8, 368])
torch.Size([8, 382])
torch.Size([8, 318])
torch.Size([8, 361])
torch.Size([8, 349])
torch.Size([8, 381])
torch.Size([8, 306])
torch.Size([8, 359])
torch.Size([8, 249])
torch.Size([8, 264])
torch.Size([8, 282])
torch.Size([8, 268])
torch.Size([8, 298])
torch.Size([8, 329])
torch.Size([8, 321])
torch.Size([8, 224])
torch.Size([8, 219])
torch.Size([8, 221])
torch.Size([8, 226])
torch.Size([8, 185])
torch.Size([8, 185])
torch.Size([8, 269])
torch.Size([8, 192])
torch.Size([8, 265])
torch.Size([8, 171])
torch.Size([8, 217])
torch.Size([8, 161])
torch.Size([8, 197])
torch.Size([8, 154])
torch.Size([8, 151])
torch.Size([8, 185])
torch.Size([8, 172])
torch.Size([8, 234])
torch.Size([8, 231])
torch.Size([8, 121])
torch.Size([8, 149])
torch.Size([8, 166])
torch.Size([8, 148])
torch.Size([8, 125])
torch.Size([8, 101])
torch.Size([8, 107])
torch.Size([8, 174])
torch.Size([8, 107])
torch.Size([8, 106])
torch.Size([8, 82])
torch.Size([8, 107])
torch.Size([8, 82])
torch.Size([8, 87])
torch.Size([8, 91])
torch.Size([8, 122])
torch.Size([8, 86])
torch.Size([8, 86])
torch.Size([8, 102])
torch.Size([8, 68])
torch.Size([8, 97])
torch.Size([8, 75])
torch.Size([8, 52])
torch.Size([8, 58])
torch.Size([8, 59])
torch.Size([8, 122])
torch.Size([8, 91])
torch.Size([8, 51])
torch.Size([8, 65])
torch.Size([8, 46])
torch.Size([8, 63])
torch.Size([8, 44])
torch.Size([8, 47])
torch.Size([8, 68])
torch.Size([8, 47])
torch.Size([8, 58])
torch.Size([8, 49])
torch.Size([8, 48])
torch.Size([8, 42])
torch.Size([8, 62])
torch.Size([8, 46])
torch.Size([8, 48])
torch.Size([8, 42])
torch.Size([8, 39])
torch.Size([8, 38])
torch.Size([8, 30])
torch.Size([8, 42])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 505])
torch.Size([8, 509])
torch.Size([8, 427])
torch.Size([8, 468])
torch.Size([8, 463])
torch.Size([8, 438])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 519])
torch.Size([8, 422])
torch.Size([8, 415])
torch.Size([8, 389])
torch.Size([8, 445])
torch.Size([8, 386])
torch.Size([8, 351])
torch.Size([8, 403])
torch.Size([8, 461])
torch.Size([8, 377])
torch.Size([8, 392])
torch.Size([8, 366])
torch.Size([8, 336])
torch.Size([8, 445])
torch.Size([8, 352])
torch.Size([8, 321])
torch.Size([8, 383])
torch.Size([8, 372])
torch.Size([8, 348])
torch.Size([8, 262])
torch.Size([8, 289])
torch.Size([8, 259])
torch.Size([8, 267])
torch.Size([8, 247])
torch.Size([8, 281])
torch.Size([8, 231])
torch.Size([8, 205])
torch.Size([8, 206])
torch.Size([8, 264])
torch.Size([8, 197])
torch.Size([8, 194])
torch.Size([8, 214])
torch.Size([8, 212])
torch.Size([8, 179])
torch.Size([8, 192])
torch.Size([8, 217])
torch.Size([8, 208])
torch.Size([8, 230])
torch.Size([8, 174])
torch.Size([8, 288])
torch.Size([8, 137])
torch.Size([8, 162])
torch.Size([8, 148])
torch.Size([8, 133])
torch.Size([8, 111])
torch.Size([8, 201])
torch.Size([8, 140])
torch.Size([8, 175])
torch.Size([8, 147])
torch.Size([8, 188])
torch.Size([8, 100])
torch.Size([8, 106])
torch.Size([8, 81])
torch.Size([8, 99])
torch.Size([8, 188])
torch.Size([8, 140])
torch.Size([8, 120])
torch.Size([8, 71])
torch.Size([8, 119])
torch.Size([8, 78])
torch.Size([8, 116])
torch.Size([8, 70])
torch.Size([8, 78])
torch.Size([8, 60])
torch.Size([8, 115])
torch.Size([8, 54])
torch.Size([8, 82])
torch.Size([8, 59])
torch.Size([8, 45])
torch.Size([8, 100])
torch.Size([8, 53])
torch.Size([8, 45])
torch.Size([8, 61])
torch.Size([8, 37])
torch.Size([8, 62])
torch.Size([8, 86])
torch.Size([8, 48])
torch.Size([8, 58])
torch.Size([8, 53])
torch.Size([8, 49])
torch.Size([8, 68])
torch.Size([8, 42])
torch.Size([8, 44])
torch.Size([8, 37])
torch.Size([8, 34])
torch.Size([8, 30])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 525])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 466])
torch.Size([8, 456])
torch.Size([8, 442])
torch.Size([8, 385])
torch.Size([8, 412])
torch.Size([8, 396])
torch.Size([8, 489])
torch.Size([8, 376])
torch.Size([8, 384])
torch.Size([8, 401])
torch.Size([8, 375])
torch.Size([8, 424])
torch.Size([8, 395])
torch.Size([8, 370])
torch.Size([8, 433])
torch.Size([8, 425])
torch.Size([8, 390])
torch.Size([8, 393])
torch.Size([8, 391])
torch.Size([8, 333])
torch.Size([8, 331])
torch.Size([8, 346])
torch.Size([8, 362])
torch.Size([8, 313])
torch.Size([8, 306])
torch.Size([8, 279])
torch.Size([8, 226])
torch.Size([8, 235])
torch.Size([8, 296])
torch.Size([8, 264])
torch.Size([8, 235])
torch.Size([8, 213])
torch.Size([8, 311])
torch.Size([8, 317])
torch.Size([8, 195])
torch.Size([8, 196])
torch.Size([8, 232])
torch.Size([8, 370])
torch.Size([8, 187])
torch.Size([8, 225])
torch.Size([8, 216])
torch.Size([8, 253])
torch.Size([8, 178])
torch.Size([8, 156])
torch.Size([8, 205])
torch.Size([8, 230])
torch.Size([8, 208])
torch.Size([8, 153])
torch.Size([8, 232])
torch.Size([8, 156])
torch.Size([8, 124])
torch.Size([8, 201])
torch.Size([8, 159])
torch.Size([8, 124])
torch.Size([8, 123])
torch.Size([8, 164])
torch.Size([8, 98])
torch.Size([8, 121])
torch.Size([8, 143])
torch.Size([8, 176])
torch.Size([8, 84])
torch.Size([8, 108])
torch.Size([8, 82])
torch.Size([8, 126])
torch.Size([8, 113])
torch.Size([8, 91])
torch.Size([8, 92])
torch.Size([8, 103])
torch.Size([8, 43])
torch.Size([8, 64])
torch.Size([8, 64])
torch.Size([8, 39])
torch.Size([8, 80])
torch.Size([8, 62])
torch.Size([8, 98])
torch.Size([8, 43])
torch.Size([8, 45])
torch.Size([8, 48])
torch.Size([8, 72])
torch.Size([8, 53])
torch.Size([8, 55])
torch.Size([8, 48])
torch.Size([8, 62])
torch.Size([8, 49])
torch.Size([8, 36])
torch.Size([8, 42])
torch.Size([8, 46])
torch.Size([8, 48])
torch.Size([8, 37])
torch.Size([8, 48])
torch.Size([8, 39])
torch.Size([8, 56])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 503])
torch.Size([8, 490])
torch.Size([8, 528])
torch.Size([8, 447])
torch.Size([8, 469])
torch.Size([8, 455])
torch.Size([8, 424])
torch.Size([8, 528])
torch.Size([8, 469])
torch.Size([8, 436])
torch.Size([8, 412])
torch.Size([8, 407])
torch.Size([8, 408])
torch.Size([8, 449])
torch.Size([8, 451])
torch.Size([8, 434])
torch.Size([8, 327])
torch.Size([8, 351])
torch.Size([8, 427])
torch.Size([8, 393])
torch.Size([8, 395])
torch.Size([8, 343])
torch.Size([8, 437])
torch.Size([8, 352])
torch.Size([8, 349])
torch.Size([8, 295])
torch.Size([8, 305])
torch.Size([8, 361])
torch.Size([8, 282])
torch.Size([8, 307])
torch.Size([8, 281])
torch.Size([8, 303])
torch.Size([8, 234])
torch.Size([8, 317])
torch.Size([8, 248])
torch.Size([8, 218])
torch.Size([8, 270])
torch.Size([8, 279])
torch.Size([8, 245])
torch.Size([8, 210])
torch.Size([8, 206])
torch.Size([8, 189])
torch.Size([8, 174])
torch.Size([8, 279])
torch.Size([8, 204])
torch.Size([8, 172])
torch.Size([8, 177])
torch.Size([8, 170])
torch.Size([8, 165])
torch.Size([8, 150])
torch.Size([8, 151])
torch.Size([8, 239])
torch.Size([8, 168])
torch.Size([8, 218])
torch.Size([8, 181])
torch.Size([8, 119])
torch.Size([8, 186])
torch.Size([8, 102])
torch.Size([8, 118])
torch.Size([8, 149])
torch.Size([8, 183])
torch.Size([8, 207])
torch.Size([8, 102])
torch.Size([8, 107])
torch.Size([8, 119])
torch.Size([8, 100])
torch.Size([8, 80])
torch.Size([8, 90])
torch.Size([8, 90])
torch.Size([8, 70])
torch.Size([8, 85])
torch.Size([8, 97])
torch.Size([8, 99])
torch.Size([8, 58])
torch.Size([8, 90])
torch.Size([8, 59])
torch.Size([8, 47])
torch.Size([8, 67])
torch.Size([8, 61])
torch.Size([8, 45])
torch.Size([8, 91])
torch.Size([8, 77])
torch.Size([8, 58])
torch.Size([8, 79])
torch.Size([8, 46])
torch.Size([8, 46])
torch.Size([8, 43])
torch.Size([8, 59])
torch.Size([8, 49])
torch.Size([8, 57])
torch.Size([8, 63])
torch.Size([8, 53])
torch.Size([8, 54])
torch.Size([8, 46])
torch.Size([8, 45])
torch.Size([8, 65])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 492])
torch.Size([8, 518])
torch.Size([8, 528])
torch.Size([8, 485])
torch.Size([8, 477])
torch.Size([8, 469])
torch.Size([8, 439])
torch.Size([8, 528])
torch.Size([8, 432])
torch.Size([8, 407])
torch.Size([8, 409])
torch.Size([8, 427])
torch.Size([8, 380])
torch.Size([8, 399])
torch.Size([8, 395])
torch.Size([8, 362])
torch.Size([8, 357])
torch.Size([8, 429])
torch.Size([8, 325])
torch.Size([8, 353])
torch.Size([8, 322])
torch.Size([8, 430])
torch.Size([8, 380])
torch.Size([8, 363])
torch.Size([8, 385])
torch.Size([8, 311])
torch.Size([8, 250])
torch.Size([8, 333])
torch.Size([8, 284])
torch.Size([8, 275])
torch.Size([8, 284])
torch.Size([8, 238])
torch.Size([8, 332])
torch.Size([8, 316])
torch.Size([8, 275])
torch.Size([8, 271])
torch.Size([8, 209])
torch.Size([8, 164])
torch.Size([8, 204])
torch.Size([8, 259])
torch.Size([8, 172])
torch.Size([8, 252])
torch.Size([8, 229])
torch.Size([8, 165])
torch.Size([8, 202])
torch.Size([8, 131])
torch.Size([8, 144])
torch.Size([8, 182])
torch.Size([8, 144])
torch.Size([8, 235])
torch.Size([8, 149])
torch.Size([8, 191])
torch.Size([8, 127])
torch.Size([8, 145])
torch.Size([8, 99])
torch.Size([8, 121])
torch.Size([8, 131])
torch.Size([8, 90])
torch.Size([8, 95])
torch.Size([8, 85])
torch.Size([8, 102])
torch.Size([8, 99])
torch.Size([8, 80])
torch.Size([8, 89])
torch.Size([8, 136])
torch.Size([8, 89])
torch.Size([8, 64])
torch.Size([8, 109])
torch.Size([8, 63])
torch.Size([8, 55])
torch.Size([8, 64])
torch.Size([8, 105])
torch.Size([8, 93])
torch.Size([8, 88])
torch.Size([8, 68])
torch.Size([8, 47])
torch.Size([8, 72])
torch.Size([8, 62])
torch.Size([8, 40])
torch.Size([8, 47])
torch.Size([8, 56])
torch.Size([8, 43])
torch.Size([8, 99])
torch.Size([8, 43])
torch.Size([8, 46])
torch.Size([8, 72])
torch.Size([8, 50])
torch.Size([8, 47])
torch.Size([8, 42])
torch.Size([8, 39])
torch.Size([8, 36])
torch.Size([8, 53])
torch.Size([8, 43])
torch.Size([8, 98])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 514])
torch.Size([8, 472])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 479])
torch.Size([8, 441])
torch.Size([8, 465])
torch.Size([8, 429])
torch.Size([8, 508])
torch.Size([8, 528])
torch.Size([8, 452])
torch.Size([8, 401])
torch.Size([8, 409])
torch.Size([8, 449])
torch.Size([8, 479])
torch.Size([8, 353])
torch.Size([8, 370])
torch.Size([8, 429])
torch.Size([8, 377])
torch.Size([8, 363])
torch.Size([8, 465])
torch.Size([8, 453])
torch.Size([8, 425])
torch.Size([8, 343])
torch.Size([8, 313])
torch.Size([8, 306])
torch.Size([8, 387])
torch.Size([8, 264])
torch.Size([8, 254])
torch.Size([8, 361])
torch.Size([8, 312])
torch.Size([8, 241])
torch.Size([8, 301])
torch.Size([8, 339])
torch.Size([8, 283])
torch.Size([8, 197])
torch.Size([8, 291])
torch.Size([8, 215])
torch.Size([8, 231])
torch.Size([8, 499])
torch.Size([8, 224])
torch.Size([8, 230])
torch.Size([8, 181])
torch.Size([8, 172])
torch.Size([8, 257])
torch.Size([8, 163])
torch.Size([8, 280])
torch.Size([8, 173])
torch.Size([8, 215])
torch.Size([8, 199])
torch.Size([8, 175])
torch.Size([8, 162])
torch.Size([8, 212])
torch.Size([8, 243])
torch.Size([8, 108])
torch.Size([8, 122])
torch.Size([8, 163])
torch.Size([8, 129])
torch.Size([8, 138])
torch.Size([8, 115])
torch.Size([8, 135])
torch.Size([8, 108])
torch.Size([8, 134])
torch.Size([8, 99])
torch.Size([8, 89])
torch.Size([8, 99])
torch.Size([8, 134])
torch.Size([8, 67])
torch.Size([8, 126])
torch.Size([8, 63])
torch.Size([8, 109])
torch.Size([8, 87])
torch.Size([8, 73])
torch.Size([8, 75])
torch.Size([8, 51])
torch.Size([8, 37])
torch.Size([8, 72])
torch.Size([8, 65])
torch.Size([8, 121])
torch.Size([8, 65])
torch.Size([8, 53])
torch.Size([8, 63])
torch.Size([8, 59])
torch.Size([8, 54])
torch.Size([8, 77])
torch.Size([8, 68])
torch.Size([8, 45])
torch.Size([8, 44])
torch.Size([8, 44])
torch.Size([8, 45])
torch.Size([8, 48])
torch.Size([8, 52])
torch.Size([8, 50])
torch.Size([8, 32])
torch.Size([8, 46])
torch.Size([8, 41])
torch.Size([8, 35])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 522])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 443])
torch.Size([8, 504])
torch.Size([8, 435])
torch.Size([8, 418])
torch.Size([8, 528])
torch.Size([8, 418])
torch.Size([8, 436])
torch.Size([8, 424])
torch.Size([8, 392])
torch.Size([8, 528])
torch.Size([8, 393])
torch.Size([8, 528])
torch.Size([8, 398])
torch.Size([8, 426])
torch.Size([8, 369])
torch.Size([8, 358])
torch.Size([8, 360])
torch.Size([8, 436])
torch.Size([8, 345])
torch.Size([8, 408])
torch.Size([8, 340])
torch.Size([8, 343])
torch.Size([8, 409])
torch.Size([8, 397])
torch.Size([8, 400])
torch.Size([8, 372])
torch.Size([8, 316])
torch.Size([8, 292])
torch.Size([8, 295])
torch.Size([8, 295])
torch.Size([8, 240])
torch.Size([8, 225])
torch.Size([8, 205])
torch.Size([8, 291])
torch.Size([8, 204])
torch.Size([8, 243])
torch.Size([8, 247])
torch.Size([8, 350])
torch.Size([8, 311])
torch.Size([8, 190])
torch.Size([8, 234])
torch.Size([8, 179])
torch.Size([8, 248])
torch.Size([8, 194])
torch.Size([8, 192])
torch.Size([8, 155])
torch.Size([8, 232])
torch.Size([8, 197])
torch.Size([8, 152])
torch.Size([8, 158])
torch.Size([8, 198])
torch.Size([8, 186])
torch.Size([8, 137])
torch.Size([8, 164])
torch.Size([8, 107])
torch.Size([8, 149])
torch.Size([8, 137])
torch.Size([8, 88])
torch.Size([8, 91])
torch.Size([8, 162])
torch.Size([8, 74])
torch.Size([8, 125])
torch.Size([8, 121])
torch.Size([8, 158])
torch.Size([8, 96])
torch.Size([8, 96])
torch.Size([8, 102])
torch.Size([8, 88])
torch.Size([8, 74])
torch.Size([8, 46])
torch.Size([8, 77])
torch.Size([8, 77])
torch.Size([8, 63])
torch.Size([8, 46])
torch.Size([8, 67])
torch.Size([8, 48])
torch.Size([8, 46])
torch.Size([8, 83])
torch.Size([8, 74])
torch.Size([8, 51])
torch.Size([8, 47])
torch.Size([8, 30])
torch.Size([8, 60])
torch.Size([8, 53])
torch.Size([8, 74])
torch.Size([8, 41])
torch.Size([8, 54])
torch.Size([8, 55])
torch.Size([8, 36])
torch.Size([8, 43])
torch.Size([8, 44])
torch.Size([8, 36])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 490])
torch.Size([8, 524])
torch.Size([8, 528])
torch.Size([8, 468])
torch.Size([8, 446])
torch.Size([8, 432])
torch.Size([8, 528])
torch.Size([8, 439])
torch.Size([8, 510])
torch.Size([8, 407])
torch.Size([8, 458])
torch.Size([8, 444])
torch.Size([8, 399])
torch.Size([8, 392])
torch.Size([8, 404])
torch.Size([8, 361])
torch.Size([8, 469])
torch.Size([8, 353])
torch.Size([8, 528])
torch.Size([8, 355])
torch.Size([8, 462])
torch.Size([8, 427])
torch.Size([8, 319])
torch.Size([8, 443])
torch.Size([8, 297])
torch.Size([8, 448])
torch.Size([8, 362])
torch.Size([8, 285])
torch.Size([8, 308])
torch.Size([8, 296])
torch.Size([8, 278])
torch.Size([8, 246])
torch.Size([8, 226])
torch.Size([8, 265])
torch.Size([8, 295])
torch.Size([8, 299])
torch.Size([8, 203])
torch.Size([8, 201])
torch.Size([8, 268])
torch.Size([8, 208])
torch.Size([8, 234])
torch.Size([8, 221])
torch.Size([8, 248])
torch.Size([8, 225])
torch.Size([8, 182])
torch.Size([8, 233])
torch.Size([8, 193])
torch.Size([8, 208])
torch.Size([8, 195])
torch.Size([8, 137])
torch.Size([8, 131])
torch.Size([8, 144])
torch.Size([8, 194])
torch.Size([8, 164])
torch.Size([8, 156])
torch.Size([8, 155])
torch.Size([8, 134])
torch.Size([8, 176])
torch.Size([8, 114])
torch.Size([8, 90])
torch.Size([8, 169])
torch.Size([8, 114])
torch.Size([8, 165])
torch.Size([8, 109])
torch.Size([8, 67])
torch.Size([8, 82])
torch.Size([8, 73])
torch.Size([8, 73])
torch.Size([8, 81])
torch.Size([8, 139])
torch.Size([8, 80])
torch.Size([8, 90])
torch.Size([8, 93])
torch.Size([8, 116])
torch.Size([8, 77])
torch.Size([8, 38])
torch.Size([8, 85])
torch.Size([8, 65])
torch.Size([8, 59])
torch.Size([8, 55])
torch.Size([8, 46])
torch.Size([8, 55])
torch.Size([8, 67])
torch.Size([8, 49])
torch.Size([8, 87])
torch.Size([8, 77])
torch.Size([8, 46])
torch.Size([8, 37])
torch.Size([8, 46])
torch.Size([8, 37])
torch.Size([8, 73])
torch.Size([8, 60])
torch.Size([8, 51])
torch.Size([8, 40])
torch.Size([8, 57])
torch.Size([8, 39])
torch.Size([8, 34])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 477])
torch.Size([8, 526])
torch.Size([8, 528])
torch.Size([8, 454])
torch.Size([8, 466])
torch.Size([8, 528])
torch.Size([8, 465])
torch.Size([8, 495])
torch.Size([8, 429])
torch.Size([8, 476])
torch.Size([8, 478])
torch.Size([8, 528])
torch.Size([8, 493])
torch.Size([8, 528])
torch.Size([8, 372])
torch.Size([8, 418])
torch.Size([8, 451])
torch.Size([8, 452])
torch.Size([8, 403])
torch.Size([8, 316])
torch.Size([8, 482])
torch.Size([8, 353])
torch.Size([8, 342])
torch.Size([8, 354])
torch.Size([8, 333])
torch.Size([8, 343])
torch.Size([8, 319])
torch.Size([8, 408])
torch.Size([8, 327])
torch.Size([8, 311])
torch.Size([8, 304])
torch.Size([8, 275])
torch.Size([8, 267])
torch.Size([8, 275])
torch.Size([8, 289])
torch.Size([8, 246])
torch.Size([8, 264])
torch.Size([8, 236])
torch.Size([8, 198])
torch.Size([8, 202])
torch.Size([8, 201])
torch.Size([8, 213])
torch.Size([8, 200])
torch.Size([8, 258])
torch.Size([8, 170])
torch.Size([8, 180])
torch.Size([8, 190])
torch.Size([8, 162])
torch.Size([8, 165])
torch.Size([8, 164])
torch.Size([8, 164])
torch.Size([8, 160])
torch.Size([8, 228])
torch.Size([8, 148])
torch.Size([8, 146])
torch.Size([8, 151])
torch.Size([8, 153])
torch.Size([8, 138])
torch.Size([8, 107])
torch.Size([8, 116])
torch.Size([8, 116])
torch.Size([8, 183])
torch.Size([8, 159])
torch.Size([8, 104])
torch.Size([8, 112])
torch.Size([8, 89])
torch.Size([8, 72])
torch.Size([8, 96])
torch.Size([8, 94])
torch.Size([8, 70])
torch.Size([8, 137])
torch.Size([8, 70])
torch.Size([8, 47])
torch.Size([8, 90])
torch.Size([8, 65])
torch.Size([8, 56])
torch.Size([8, 56])
torch.Size([8, 59])
torch.Size([8, 62])
torch.Size([8, 68])
torch.Size([8, 50])
torch.Size([8, 49])
torch.Size([8, 46])
torch.Size([8, 80])
torch.Size([8, 60])
torch.Size([8, 55])
torch.Size([8, 63])
torch.Size([8, 56])
torch.Size([8, 50])
torch.Size([8, 42])
torch.Size([8, 49])
torch.Size([8, 37])
torch.Size([8, 73])
torch.Size([8, 34])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 487])
torch.Size([8, 528])
torch.Size([8, 460])
torch.Size([8, 502])
torch.Size([8, 449])
torch.Size([8, 444])
torch.Size([8, 468])
torch.Size([8, 528])
torch.Size([8, 438])
torch.Size([8, 415])
torch.Size([8, 406])
torch.Size([8, 384])
torch.Size([8, 528])
torch.Size([8, 396])
torch.Size([8, 381])
torch.Size([8, 376])
torch.Size([8, 364])
torch.Size([8, 345])
torch.Size([8, 385])
torch.Size([8, 353])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 493])
torch.Size([8, 320])
torch.Size([8, 280])
torch.Size([8, 514])
torch.Size([8, 325])
torch.Size([8, 308])
torch.Size([8, 303])
torch.Size([8, 329])
torch.Size([8, 392])
torch.Size([8, 291])
torch.Size([8, 359])
torch.Size([8, 209])
torch.Size([8, 407])
torch.Size([8, 265])
torch.Size([8, 226])
torch.Size([8, 293])
torch.Size([8, 197])
torch.Size([8, 236])
torch.Size([8, 197])
torch.Size([8, 222])
torch.Size([8, 265])
torch.Size([8, 276])
torch.Size([8, 253])
torch.Size([8, 224])
torch.Size([8, 262])
torch.Size([8, 285])
torch.Size([8, 231])
torch.Size([8, 193])
torch.Size([8, 154])
torch.Size([8, 164])
torch.Size([8, 189])
torch.Size([8, 163])
torch.Size([8, 118])
torch.Size([8, 208])
torch.Size([8, 143])
torch.Size([8, 96])
torch.Size([8, 108])
torch.Size([8, 104])
torch.Size([8, 121])
torch.Size([8, 93])
torch.Size([8, 95])
torch.Size([8, 100])
torch.Size([8, 78])
torch.Size([8, 84])
torch.Size([8, 75])
torch.Size([8, 92])
torch.Size([8, 138])
torch.Size([8, 99])
torch.Size([8, 102])
torch.Size([8, 89])
torch.Size([8, 39])
torch.Size([8, 75])
torch.Size([8, 57])
torch.Size([8, 64])
torch.Size([8, 41])
torch.Size([8, 65])
torch.Size([8, 94])
torch.Size([8, 66])
torch.Size([8, 61])
torch.Size([8, 47])
torch.Size([8, 53])
torch.Size([8, 49])
torch.Size([8, 60])
torch.Size([8, 50])
torch.Size([8, 83])
torch.Size([8, 47])
torch.Size([8, 51])
torch.Size([8, 47])
torch.Size([8, 42])
torch.Size([8, 40])
torch.Size([8, 34])
torch.Size([8, 50])
torch.Size([8, 34])
torch.Size([8, 58])
torch.Size([8, 39])
{'loss': 0.0, 'learning_rate': 0.0002, 'epoch': 0.25}
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 485])
torch.Size([8, 472])
torch.Size([8, 464])
torch.Size([8, 459])
torch.Size([8, 478])
torch.Size([8, 417])
torch.Size([8, 441])
torch.Size([8, 380])
torch.Size([8, 429])
torch.Size([8, 386])
torch.Size([8, 528])
torch.Size([8, 446])
torch.Size([8, 441])
torch.Size([8, 412])
torch.Size([8, 445])
torch.Size([8, 413])
torch.Size([8, 352])
torch.Size([8, 420])
torch.Size([8, 390])
torch.Size([8, 429])
torch.Size([8, 311])
torch.Size([8, 490])
torch.Size([8, 298])
torch.Size([8, 365])
torch.Size([8, 370])
torch.Size([8, 316])
torch.Size([8, 348])
torch.Size([8, 356])
torch.Size([8, 241])
torch.Size([8, 297])
torch.Size([8, 329])
torch.Size([8, 251])
torch.Size([8, 273])
torch.Size([8, 257])
torch.Size([8, 224])
torch.Size([8, 275])
torch.Size([8, 223])
torch.Size([8, 295])
torch.Size([8, 232])
torch.Size([8, 195])
torch.Size([8, 206])
torch.Size([8, 227])
torch.Size([8, 213])
torch.Size([8, 197])
torch.Size([8, 263])
torch.Size([8, 198])
torch.Size([8, 234])
torch.Size([8, 153])
torch.Size([8, 169])
torch.Size([8, 202])
torch.Size([8, 183])
torch.Size([8, 138])
torch.Size([8, 115])
torch.Size([8, 174])
torch.Size([8, 114])
torch.Size([8, 136])
torch.Size([8, 129])
torch.Size([8, 137])
torch.Size([8, 118])
torch.Size([8, 147])
torch.Size([8, 78])
torch.Size([8, 119])
torch.Size([8, 76])
torch.Size([8, 85])
torch.Size([8, 88])
torch.Size([8, 75])
torch.Size([8, 79])
torch.Size([8, 75])
torch.Size([8, 73])
torch.Size([8, 106])
torch.Size([8, 41])
torch.Size([8, 67])
torch.Size([8, 99])
torch.Size([8, 95])
torch.Size([8, 72])
torch.Size([8, 45])
torch.Size([8, 67])
torch.Size([8, 63])
torch.Size([8, 79])
torch.Size([8, 67])
torch.Size([8, 65])
torch.Size([8, 52])
torch.Size([8, 58])
torch.Size([8, 57])
torch.Size([8, 48])
torch.Size([8, 38])
torch.Size([8, 49])
torch.Size([8, 35])
torch.Size([8, 63])
torch.Size([8, 43])
torch.Size([8, 54])
torch.Size([8, 40])
torch.Size([8, 35])
torch.Size([8, 39])
torch.Size([8, 31])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 515])
torch.Size([8, 528])
torch.Size([8, 432])
torch.Size([8, 494])
torch.Size([8, 446])
torch.Size([8, 471])
torch.Size([8, 457])
torch.Size([8, 476])
torch.Size([8, 454])
torch.Size([8, 399])
torch.Size([8, 401])
torch.Size([8, 423])
torch.Size([8, 393])
torch.Size([8, 419])
torch.Size([8, 510])
torch.Size([8, 336])
torch.Size([8, 408])
torch.Size([8, 516])
torch.Size([8, 361])
torch.Size([8, 381])
torch.Size([8, 351])
torch.Size([8, 386])
torch.Size([8, 323])
torch.Size([8, 325])
torch.Size([8, 274])
torch.Size([8, 365])
torch.Size([8, 297])
torch.Size([8, 301])
torch.Size([8, 261])
torch.Size([8, 240])
torch.Size([8, 312])
torch.Size([8, 220])
torch.Size([8, 243])
torch.Size([8, 248])
torch.Size([8, 226])
torch.Size([8, 255])
torch.Size([8, 263])
torch.Size([8, 212])
torch.Size([8, 191])
torch.Size([8, 209])
torch.Size([8, 212])
torch.Size([8, 217])
torch.Size([8, 188])
torch.Size([8, 227])
torch.Size([8, 154])
torch.Size([8, 200])
torch.Size([8, 159])
torch.Size([8, 171])
torch.Size([8, 157])
torch.Size([8, 167])
torch.Size([8, 171])
torch.Size([8, 162])
torch.Size([8, 239])
torch.Size([8, 129])
torch.Size([8, 136])
torch.Size([8, 156])
torch.Size([8, 152])
torch.Size([8, 176])
torch.Size([8, 139])
torch.Size([8, 96])
torch.Size([8, 156])
torch.Size([8, 153])
torch.Size([8, 83])
torch.Size([8, 67])
torch.Size([8, 85])
torch.Size([8, 128])
torch.Size([8, 75])
torch.Size([8, 58])
torch.Size([8, 69])
torch.Size([8, 49])
torch.Size([8, 59])
torch.Size([8, 62])
torch.Size([8, 62])
torch.Size([8, 62])
torch.Size([8, 78])
torch.Size([8, 105])
torch.Size([8, 54])
torch.Size([8, 61])
torch.Size([8, 65])
torch.Size([8, 70])
torch.Size([8, 65])
torch.Size([8, 61])
torch.Size([8, 73])
torch.Size([8, 51])
torch.Size([8, 47])
torch.Size([8, 46])
torch.Size([8, 49])
torch.Size([8, 45])
torch.Size([8, 53])
torch.Size([8, 57])
torch.Size([8, 45])
torch.Size([8, 53])
torch.Size([8, 40])
torch.Size([8, 35])
torch.Size([8, 38])
torch.Size([8, 38])
torch.Size([8, 29])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 495])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 453])
torch.Size([8, 508])
torch.Size([8, 505])
torch.Size([8, 469])
torch.Size([8, 528])
torch.Size([8, 415])
torch.Size([8, 416])
torch.Size([8, 383])
torch.Size([8, 439])
torch.Size([8, 353])
torch.Size([8, 352])
torch.Size([8, 355])
torch.Size([8, 528])
torch.Size([8, 460])
torch.Size([8, 466])
torch.Size([8, 397])
torch.Size([8, 367])
torch.Size([8, 394])
torch.Size([8, 371])
torch.Size([8, 369])
torch.Size([8, 299])
torch.Size([8, 280])
torch.Size([8, 320])
torch.Size([8, 238])
torch.Size([8, 506])
torch.Size([8, 283])
torch.Size([8, 264])
torch.Size([8, 322])
torch.Size([8, 238])
torch.Size([8, 369])
torch.Size([8, 194])
torch.Size([8, 223])
torch.Size([8, 243])
torch.Size([8, 278])
torch.Size([8, 245])
torch.Size([8, 225])
torch.Size([8, 182])
torch.Size([8, 264])
torch.Size([8, 194])
torch.Size([8, 232])
torch.Size([8, 222])
torch.Size([8, 182])
torch.Size([8, 179])
torch.Size([8, 190])
torch.Size([8, 234])
torch.Size([8, 181])
torch.Size([8, 215])
torch.Size([8, 205])
torch.Size([8, 154])
torch.Size([8, 274])
torch.Size([8, 218])
torch.Size([8, 175])
torch.Size([8, 117])
torch.Size([8, 94])
torch.Size([8, 112])
torch.Size([8, 114])
torch.Size([8, 112])
torch.Size([8, 83])
torch.Size([8, 99])
torch.Size([8, 147])
torch.Size([8, 105])
torch.Size([8, 76])
torch.Size([8, 96])
torch.Size([8, 87])
torch.Size([8, 141])
torch.Size([8, 95])
torch.Size([8, 65])
torch.Size([8, 79])
torch.Size([8, 67])
torch.Size([8, 97])
torch.Size([8, 62])
torch.Size([8, 90])
torch.Size([8, 54])
torch.Size([8, 79])
torch.Size([8, 33])
torch.Size([8, 67])
torch.Size([8, 70])
torch.Size([8, 66])
torch.Size([8, 133])
torch.Size([8, 56])
torch.Size([8, 31])
torch.Size([8, 75])
torch.Size([8, 94])
torch.Size([8, 34])
torch.Size([8, 37])
torch.Size([8, 46])
torch.Size([8, 59])
torch.Size([8, 49])
torch.Size([8, 43])
torch.Size([8, 43])
torch.Size([8, 37])
torch.Size([8, 40])
torch.Size([8, 34])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 524])
torch.Size([8, 528])
torch.Size([8, 498])
torch.Size([8, 528])
torch.Size([8, 527])
torch.Size([8, 506])
torch.Size([8, 451])
torch.Size([8, 418])
torch.Size([8, 528])
torch.Size([8, 421])
torch.Size([8, 420])
torch.Size([8, 379])
torch.Size([8, 401])
torch.Size([8, 378])
torch.Size([8, 364])
torch.Size([8, 390])
torch.Size([8, 386])
torch.Size([8, 363])
torch.Size([8, 308])
torch.Size([8, 444])
torch.Size([8, 307])
torch.Size([8, 318])
torch.Size([8, 490])
torch.Size([8, 329])
torch.Size([8, 368])
torch.Size([8, 355])
torch.Size([8, 294])
torch.Size([8, 319])
torch.Size([8, 354])
torch.Size([8, 299])
torch.Size([8, 310])
torch.Size([8, 350])
torch.Size([8, 255])
torch.Size([8, 265])
torch.Size([8, 277])
torch.Size([8, 235])
torch.Size([8, 269])
torch.Size([8, 283])
torch.Size([8, 254])
torch.Size([8, 174])
torch.Size([8, 188])
torch.Size([8, 268])
torch.Size([8, 189])
torch.Size([8, 192])
torch.Size([8, 194])
torch.Size([8, 345])
torch.Size([8, 189])
torch.Size([8, 216])
torch.Size([8, 141])
torch.Size([8, 183])
torch.Size([8, 166])
torch.Size([8, 175])
torch.Size([8, 131])
torch.Size([8, 132])
torch.Size([8, 130])
torch.Size([8, 156])
torch.Size([8, 118])
torch.Size([8, 102])
torch.Size([8, 175])
torch.Size([8, 165])
torch.Size([8, 127])
torch.Size([8, 106])
torch.Size([8, 102])
torch.Size([8, 76])
torch.Size([8, 108])
torch.Size([8, 136])
torch.Size([8, 89])
torch.Size([8, 85])
torch.Size([8, 78])
torch.Size([8, 76])
torch.Size([8, 117])
torch.Size([8, 74])
torch.Size([8, 86])
torch.Size([8, 82])
torch.Size([8, 69])
torch.Size([8, 67])
torch.Size([8, 54])
torch.Size([8, 84])
torch.Size([8, 58])
torch.Size([8, 62])
torch.Size([8, 75])
torch.Size([8, 57])
torch.Size([8, 70])
torch.Size([8, 55])
torch.Size([8, 43])
torch.Size([8, 60])
torch.Size([8, 113])
torch.Size([8, 80])
torch.Size([8, 58])
torch.Size([8, 54])
torch.Size([8, 57])
torch.Size([8, 42])
torch.Size([8, 49])
torch.Size([8, 63])
torch.Size([8, 46])
torch.Size([8, 35])
torch.Size([8, 34])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 504])
torch.Size([8, 528])
torch.Size([8, 451])
torch.Size([8, 528])
torch.Size([8, 458])
torch.Size([8, 435])
torch.Size([8, 446])
torch.Size([8, 458])
torch.Size([8, 516])
torch.Size([8, 464])
torch.Size([8, 423])
torch.Size([8, 523])
torch.Size([8, 489])
torch.Size([8, 494])
torch.Size([8, 368])
torch.Size([8, 482])
torch.Size([8, 355])
torch.Size([8, 450])
torch.Size([8, 436])
torch.Size([8, 368])
torch.Size([8, 327])
torch.Size([8, 340])
torch.Size([8, 293])
torch.Size([8, 370])
torch.Size([8, 292])
torch.Size([8, 433])
torch.Size([8, 528])
torch.Size([8, 329])
torch.Size([8, 304])
torch.Size([8, 232])
torch.Size([8, 218])
torch.Size([8, 335])
torch.Size([8, 230])
torch.Size([8, 224])
torch.Size([8, 279])
torch.Size([8, 234])
torch.Size([8, 237])
torch.Size([8, 282])
torch.Size([8, 259])
torch.Size([8, 196])
torch.Size([8, 234])
torch.Size([8, 182])
torch.Size([8, 252])
torch.Size([8, 208])
torch.Size([8, 283])
torch.Size([8, 180])
torch.Size([8, 167])
torch.Size([8, 159])
torch.Size([8, 151])
torch.Size([8, 129])
torch.Size([8, 213])
torch.Size([8, 183])
torch.Size([8, 121])
torch.Size([8, 129])
torch.Size([8, 171])
torch.Size([8, 112])
torch.Size([8, 128])
torch.Size([8, 129])
torch.Size([8, 167])
torch.Size([8, 126])
torch.Size([8, 81])
torch.Size([8, 113])
torch.Size([8, 91])
torch.Size([8, 88])
torch.Size([8, 115])
torch.Size([8, 64])
torch.Size([8, 139])
torch.Size([8, 77])
torch.Size([8, 79])
torch.Size([8, 68])
torch.Size([8, 58])
torch.Size([8, 69])
torch.Size([8, 89])
torch.Size([8, 43])
torch.Size([8, 66])
torch.Size([8, 63])
torch.Size([8, 34])
torch.Size([8, 53])
torch.Size([8, 80])
torch.Size([8, 49])
torch.Size([8, 54])
torch.Size([8, 55])
torch.Size([8, 54])
torch.Size([8, 46])
torch.Size([8, 71])
torch.Size([8, 54])
torch.Size([8, 51])
torch.Size([8, 64])
torch.Size([8, 52])
torch.Size([8, 54])
torch.Size([8, 45])
torch.Size([8, 43])
torch.Size([8, 50])
torch.Size([8, 45])
torch.Size([8, 37])
torch.Size([8, 45])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 452])
torch.Size([8, 419])
torch.Size([8, 441])
torch.Size([8, 420])
torch.Size([8, 476])
torch.Size([8, 451])
torch.Size([8, 463])
torch.Size([8, 452])
torch.Size([8, 370])
torch.Size([8, 374])
torch.Size([8, 386])
torch.Size([8, 528])
torch.Size([8, 400])
torch.Size([8, 422])
torch.Size([8, 364])
torch.Size([8, 408])
torch.Size([8, 345])
torch.Size([8, 402])
torch.Size([8, 466])
torch.Size([8, 400])
torch.Size([8, 346])
torch.Size([8, 307])
torch.Size([8, 251])
torch.Size([8, 287])
torch.Size([8, 302])
torch.Size([8, 286])
torch.Size([8, 333])
torch.Size([8, 253])
torch.Size([8, 267])
torch.Size([8, 353])
torch.Size([8, 343])
torch.Size([8, 254])
torch.Size([8, 249])
torch.Size([8, 301])
torch.Size([8, 258])
torch.Size([8, 320])
torch.Size([8, 290])
torch.Size([8, 217])
torch.Size([8, 194])
torch.Size([8, 229])
torch.Size([8, 277])
torch.Size([8, 234])
torch.Size([8, 226])
torch.Size([8, 163])
torch.Size([8, 193])
torch.Size([8, 209])
torch.Size([8, 147])
torch.Size([8, 193])
torch.Size([8, 195])
torch.Size([8, 137])
torch.Size([8, 161])
torch.Size([8, 153])
torch.Size([8, 143])
torch.Size([8, 166])
torch.Size([8, 126])
torch.Size([8, 113])
torch.Size([8, 131])
torch.Size([8, 129])
torch.Size([8, 89])
torch.Size([8, 121])
torch.Size([8, 120])
torch.Size([8, 82])
torch.Size([8, 79])
torch.Size([8, 122])
torch.Size([8, 79])
torch.Size([8, 67])
torch.Size([8, 78])
torch.Size([8, 78])
torch.Size([8, 73])
torch.Size([8, 60])
torch.Size([8, 60])
torch.Size([8, 54])
torch.Size([8, 42])
torch.Size([8, 75])
torch.Size([8, 71])
torch.Size([8, 66])
torch.Size([8, 79])
torch.Size([8, 65])
torch.Size([8, 77])
torch.Size([8, 60])
torch.Size([8, 46])
torch.Size([8, 61])
torch.Size([8, 54])
torch.Size([8, 61])
torch.Size([8, 32])
torch.Size([8, 37])
torch.Size([8, 58])
torch.Size([8, 49])
torch.Size([8, 46])
torch.Size([8, 37])
torch.Size([8, 51])
torch.Size([8, 50])
torch.Size([8, 38])
torch.Size([8, 46])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 499])
torch.Size([8, 476])
torch.Size([8, 473])
torch.Size([8, 447])
torch.Size([8, 511])
torch.Size([8, 439])
torch.Size([8, 489])
torch.Size([8, 522])
torch.Size([8, 433])
torch.Size([8, 417])
torch.Size([8, 402])
torch.Size([8, 528])
torch.Size([8, 390])
torch.Size([8, 528])
torch.Size([8, 466])
torch.Size([8, 378])
torch.Size([8, 379])
torch.Size([8, 357])
torch.Size([8, 335])
torch.Size([8, 396])
torch.Size([8, 366])
torch.Size([8, 442])
torch.Size([8, 306])
torch.Size([8, 267])
torch.Size([8, 318])
torch.Size([8, 339])
torch.Size([8, 265])
torch.Size([8, 340])
torch.Size([8, 284])
torch.Size([8, 309])
torch.Size([8, 296])
torch.Size([8, 275])
torch.Size([8, 260])
torch.Size([8, 259])
torch.Size([8, 267])
torch.Size([8, 299])
torch.Size([8, 273])
torch.Size([8, 245])
torch.Size([8, 219])
torch.Size([8, 303])
torch.Size([8, 208])
torch.Size([8, 232])
torch.Size([8, 175])
torch.Size([8, 194])
torch.Size([8, 238])
torch.Size([8, 202])
torch.Size([8, 226])
torch.Size([8, 256])
torch.Size([8, 219])
torch.Size([8, 161])
torch.Size([8, 161])
torch.Size([8, 126])
torch.Size([8, 142])
torch.Size([8, 192])
torch.Size([8, 205])
torch.Size([8, 154])
torch.Size([8, 130])
torch.Size([8, 117])
torch.Size([8, 208])
torch.Size([8, 124])
torch.Size([8, 125])
torch.Size([8, 135])
torch.Size([8, 107])
torch.Size([8, 75])
torch.Size([8, 58])
torch.Size([8, 126])
torch.Size([8, 103])
torch.Size([8, 51])
torch.Size([8, 69])
torch.Size([8, 68])
torch.Size([8, 65])
torch.Size([8, 67])
torch.Size([8, 65])
torch.Size([8, 75])
torch.Size([8, 58])
torch.Size([8, 79])
torch.Size([8, 49])
torch.Size([8, 116])
torch.Size([8, 61])
torch.Size([8, 49])
torch.Size([8, 50])
torch.Size([8, 55])
torch.Size([8, 55])
torch.Size([8, 48])
torch.Size([8, 76])
torch.Size([8, 51])
torch.Size([8, 49])
torch.Size([8, 41])
torch.Size([8, 67])
torch.Size([8, 78])
torch.Size([8, 40])
torch.Size([8, 37])
torch.Size([8, 46])
torch.Size([8, 54])
torch.Size([8, 36])
torch.Size([8, 33])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 486])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 470])
torch.Size([8, 518])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 497])
torch.Size([8, 441])
torch.Size([8, 516])
torch.Size([8, 452])
torch.Size([8, 422])
torch.Size([8, 402])
torch.Size([8, 528])
torch.Size([8, 435])
torch.Size([8, 366])
torch.Size([8, 420])
torch.Size([8, 456])
torch.Size([8, 506])
torch.Size([8, 353])
torch.Size([8, 333])
torch.Size([8, 367])
torch.Size([8, 403])
torch.Size([8, 324])
torch.Size([8, 500])
torch.Size([8, 359])
torch.Size([8, 358])
torch.Size([8, 446])
torch.Size([8, 380])
torch.Size([8, 462])
torch.Size([8, 257])
torch.Size([8, 353])
torch.Size([8, 248])
torch.Size([8, 277])
torch.Size([8, 298])
torch.Size([8, 279])
torch.Size([8, 261])
torch.Size([8, 200])
torch.Size([8, 209])
torch.Size([8, 278])
torch.Size([8, 262])
torch.Size([8, 159])
torch.Size([8, 199])
torch.Size([8, 227])
torch.Size([8, 169])
torch.Size([8, 190])
torch.Size([8, 198])
torch.Size([8, 252])
torch.Size([8, 154])
torch.Size([8, 203])
torch.Size([8, 172])
torch.Size([8, 253])
torch.Size([8, 148])
torch.Size([8, 173])
torch.Size([8, 170])
torch.Size([8, 130])
torch.Size([8, 172])
torch.Size([8, 127])
torch.Size([8, 123])
torch.Size([8, 88])
torch.Size([8, 102])
torch.Size([8, 172])
torch.Size([8, 101])
torch.Size([8, 100])
torch.Size([8, 105])
torch.Size([8, 88])
torch.Size([8, 102])
torch.Size([8, 77])
torch.Size([8, 104])
torch.Size([8, 89])
torch.Size([8, 61])
torch.Size([8, 76])
torch.Size([8, 68])
torch.Size([8, 79])
torch.Size([8, 48])
torch.Size([8, 76])
torch.Size([8, 102])
torch.Size([8, 55])
torch.Size([8, 91])
torch.Size([8, 62])
torch.Size([8, 58])
torch.Size([8, 63])
torch.Size([8, 70])
torch.Size([8, 56])
torch.Size([8, 58])
torch.Size([8, 56])
torch.Size([8, 84])
torch.Size([8, 36])
torch.Size([8, 55])
torch.Size([8, 62])
torch.Size([8, 42])
torch.Size([8, 70])
torch.Size([8, 35])
torch.Size([8, 47])
torch.Size([8, 46])
torch.Size([8, 43])
torch.Size([8, 45])
torch.Size([8, 37])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 527])
torch.Size([8, 528])
torch.Size([8, 469])
torch.Size([8, 528])
torch.Size([8, 456])
torch.Size([8, 528])
torch.Size([8, 516])
torch.Size([8, 405])
torch.Size([8, 463])
torch.Size([8, 528])
torch.Size([8, 443])
torch.Size([8, 384])
torch.Size([8, 528])
torch.Size([8, 361])
torch.Size([8, 352])
torch.Size([8, 479])
torch.Size([8, 418])
torch.Size([8, 476])
torch.Size([8, 410])
torch.Size([8, 434])
torch.Size([8, 315])
torch.Size([8, 327])
torch.Size([8, 355])
torch.Size([8, 349])
torch.Size([8, 382])
torch.Size([8, 270])
torch.Size([8, 337])
torch.Size([8, 313])
torch.Size([8, 251])
torch.Size([8, 302])
torch.Size([8, 305])
torch.Size([8, 222])
torch.Size([8, 220])
torch.Size([8, 203])
torch.Size([8, 271])
torch.Size([8, 195])
torch.Size([8, 340])
torch.Size([8, 246])
torch.Size([8, 217])
torch.Size([8, 198])
torch.Size([8, 263])
torch.Size([8, 258])
torch.Size([8, 159])
torch.Size([8, 236])
torch.Size([8, 154])
torch.Size([8, 210])
torch.Size([8, 170])
torch.Size([8, 182])
torch.Size([8, 265])
torch.Size([8, 221])
torch.Size([8, 290])
torch.Size([8, 236])
torch.Size([8, 117])
torch.Size([8, 183])
torch.Size([8, 130])
torch.Size([8, 128])
torch.Size([8, 173])
torch.Size([8, 123])
torch.Size([8, 138])
torch.Size([8, 89])
torch.Size([8, 120])
torch.Size([8, 99])
torch.Size([8, 139])
torch.Size([8, 164])
torch.Size([8, 92])
torch.Size([8, 143])
torch.Size([8, 78])
torch.Size([8, 81])
torch.Size([8, 73])
torch.Size([8, 83])
torch.Size([8, 91])
torch.Size([8, 67])
torch.Size([8, 72])
torch.Size([8, 84])
torch.Size([8, 69])
torch.Size([8, 96])
torch.Size([8, 58])
torch.Size([8, 57])
torch.Size([8, 68])
torch.Size([8, 89])
torch.Size([8, 80])
torch.Size([8, 54])
torch.Size([8, 53])
torch.Size([8, 49])
torch.Size([8, 57])
torch.Size([8, 37])
torch.Size([8, 37])
torch.Size([8, 50])
torch.Size([8, 106])
torch.Size([8, 41])
torch.Size([8, 46])
torch.Size([8, 41])
torch.Size([8, 52])
torch.Size([8, 37])
torch.Size([8, 43])
torch.Size([8, 37])
torch.Size([8, 30])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 502])
torch.Size([8, 527])
torch.Size([8, 507])
torch.Size([8, 528])
torch.Size([8, 446])
torch.Size([8, 451])
torch.Size([8, 486])
torch.Size([8, 409])
torch.Size([8, 450])
torch.Size([8, 509])
torch.Size([8, 409])
torch.Size([8, 394])
torch.Size([8, 331])
torch.Size([8, 321])
torch.Size([8, 455])
torch.Size([8, 377])
torch.Size([8, 345])
torch.Size([8, 353])
torch.Size([8, 420])
torch.Size([8, 421])
torch.Size([8, 336])
torch.Size([8, 380])
torch.Size([8, 297])
torch.Size([8, 331])
torch.Size([8, 336])
torch.Size([8, 332])
torch.Size([8, 260])
torch.Size([8, 270])
torch.Size([8, 324])
torch.Size([8, 431])
torch.Size([8, 297])
torch.Size([8, 325])
torch.Size([8, 304])
torch.Size([8, 273])
torch.Size([8, 251])
torch.Size([8, 303])
torch.Size([8, 244])
torch.Size([8, 227])
torch.Size([8, 278])
torch.Size([8, 191])
torch.Size([8, 267])
torch.Size([8, 184])
torch.Size([8, 203])
torch.Size([8, 219])
torch.Size([8, 180])
torch.Size([8, 213])
torch.Size([8, 222])
torch.Size([8, 140])
torch.Size([8, 160])
torch.Size([8, 192])
torch.Size([8, 153])
torch.Size([8, 128])
torch.Size([8, 122])
torch.Size([8, 127])
torch.Size([8, 126])
torch.Size([8, 145])
torch.Size([8, 136])
torch.Size([8, 168])
torch.Size([8, 130])
torch.Size([8, 103])
torch.Size([8, 91])
torch.Size([8, 79])
torch.Size([8, 76])
torch.Size([8, 79])
torch.Size([8, 98])
torch.Size([8, 98])
torch.Size([8, 103])
torch.Size([8, 69])
torch.Size([8, 68])
torch.Size([8, 63])
torch.Size([8, 73])
torch.Size([8, 55])
torch.Size([8, 102])
torch.Size([8, 59])
torch.Size([8, 63])
torch.Size([8, 58])
torch.Size([8, 54])
torch.Size([8, 59])
torch.Size([8, 38])
torch.Size([8, 59])
torch.Size([8, 53])
torch.Size([8, 56])
torch.Size([8, 50])
torch.Size([8, 53])
torch.Size([8, 48])
torch.Size([8, 59])
torch.Size([8, 38])
torch.Size([8, 87])
torch.Size([8, 60])
torch.Size([8, 70])
torch.Size([8, 54])
torch.Size([8, 44])
torch.Size([8, 50])
torch.Size([8, 46])
torch.Size([8, 56])
torch.Size([8, 40])
torch.Size([8, 37])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 492])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 504])
torch.Size([8, 470])
torch.Size([8, 437])
torch.Size([8, 446])
torch.Size([8, 415])
torch.Size([8, 419])
torch.Size([8, 414])
torch.Size([8, 405])
torch.Size([8, 402])
torch.Size([8, 376])
torch.Size([8, 468])
torch.Size([8, 456])
torch.Size([8, 333])
torch.Size([8, 333])
torch.Size([8, 333])
torch.Size([8, 371])
torch.Size([8, 329])
torch.Size([8, 339])
torch.Size([8, 293])
torch.Size([8, 342])
torch.Size([8, 321])
torch.Size([8, 319])
torch.Size([8, 275])
torch.Size([8, 323])
torch.Size([8, 285])
torch.Size([8, 311])
torch.Size([8, 283])
torch.Size([8, 328])
torch.Size([8, 258])
torch.Size([8, 238])
torch.Size([8, 268])
torch.Size([8, 260])
torch.Size([8, 207])
torch.Size([8, 230])
torch.Size([8, 182])
torch.Size([8, 209])
torch.Size([8, 236])
torch.Size([8, 219])
torch.Size([8, 217])
torch.Size([8, 172])
torch.Size([8, 209])
torch.Size([8, 262])
torch.Size([8, 226])
torch.Size([8, 285])
torch.Size([8, 173])
torch.Size([8, 169])
torch.Size([8, 191])
torch.Size([8, 171])
torch.Size([8, 136])
torch.Size([8, 114])
torch.Size([8, 134])
torch.Size([8, 117])
torch.Size([8, 143])
torch.Size([8, 151])
torch.Size([8, 159])
torch.Size([8, 102])
torch.Size([8, 71])
torch.Size([8, 91])
torch.Size([8, 81])
torch.Size([8, 147])
torch.Size([8, 129])
torch.Size([8, 116])
torch.Size([8, 104])
torch.Size([8, 138])
torch.Size([8, 73])
torch.Size([8, 86])
torch.Size([8, 69])
torch.Size([8, 106])
torch.Size([8, 71])
torch.Size([8, 74])
torch.Size([8, 56])
torch.Size([8, 63])
torch.Size([8, 66])
torch.Size([8, 62])
torch.Size([8, 52])
torch.Size([8, 54])
torch.Size([8, 56])
torch.Size([8, 41])
torch.Size([8, 64])
torch.Size([8, 51])
torch.Size([8, 63])
torch.Size([8, 46])
torch.Size([8, 71])
torch.Size([8, 47])
torch.Size([8, 71])
torch.Size([8, 59])
torch.Size([8, 42])
torch.Size([8, 66])
torch.Size([8, 51])
torch.Size([8, 48])
torch.Size([8, 37])
torch.Size([8, 43])
torch.Size([8, 32])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 510])
torch.Size([8, 528])
torch.Size([8, 480])
torch.Size([8, 478])
torch.Size([8, 427])
torch.Size([8, 528])
torch.Size([8, 467])
torch.Size([8, 452])
torch.Size([8, 528])
torch.Size([8, 392])
torch.Size([8, 396])
torch.Size([8, 506])
torch.Size([8, 388])
torch.Size([8, 362])
torch.Size([8, 528])
torch.Size([8, 369])
torch.Size([8, 449])
torch.Size([8, 366])
torch.Size([8, 378])
torch.Size([8, 409])
torch.Size([8, 297])
torch.Size([8, 303])
torch.Size([8, 378])
torch.Size([8, 319])
torch.Size([8, 319])
torch.Size([8, 323])
torch.Size([8, 341])
torch.Size([8, 323])
torch.Size([8, 364])
torch.Size([8, 311])
torch.Size([8, 313])
torch.Size([8, 265])
torch.Size([8, 264])
torch.Size([8, 294])
torch.Size([8, 233])
torch.Size([8, 214])
torch.Size([8, 247])
torch.Size([8, 241])
torch.Size([8, 208])
torch.Size([8, 275])
torch.Size([8, 231])
torch.Size([8, 193])
torch.Size([8, 176])
torch.Size([8, 193])
torch.Size([8, 206])
torch.Size([8, 208])
torch.Size([8, 166])
torch.Size([8, 146])
torch.Size([8, 164])
torch.Size([8, 231])
torch.Size([8, 149])
torch.Size([8, 177])
torch.Size([8, 118])
torch.Size([8, 363])
torch.Size([8, 125])
torch.Size([8, 128])
torch.Size([8, 126])
torch.Size([8, 124])
torch.Size([8, 109])
torch.Size([8, 136])
torch.Size([8, 75])
torch.Size([8, 101])
torch.Size([8, 110])
torch.Size([8, 84])
torch.Size([8, 87])
torch.Size([8, 68])
torch.Size([8, 66])
torch.Size([8, 64])
torch.Size([8, 69])
torch.Size([8, 63])
torch.Size([8, 46])
torch.Size([8, 43])
torch.Size([8, 58])
torch.Size([8, 59])
torch.Size([8, 90])
torch.Size([8, 54])
torch.Size([8, 54])
torch.Size([8, 35])
torch.Size([8, 87])
torch.Size([8, 50])
torch.Size([8, 60])
torch.Size([8, 53])
torch.Size([8, 66])
torch.Size([8, 49])
torch.Size([8, 66])
torch.Size([8, 43])
torch.Size([8, 59])
torch.Size([8, 62])
torch.Size([8, 60])
torch.Size([8, 58])
torch.Size([8, 39])
torch.Size([8, 53])
torch.Size([8, 41])
torch.Size([8, 37])
torch.Size([8, 36])
torch.Size([8, 35])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 481])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 475])
torch.Size([8, 446])
torch.Size([8, 491])
torch.Size([8, 428])
torch.Size([8, 455])
torch.Size([8, 477])
torch.Size([8, 388])
torch.Size([8, 455])
torch.Size([8, 480])
torch.Size([8, 390])
torch.Size([8, 404])
torch.Size([8, 406])
torch.Size([8, 404])
torch.Size([8, 368])
torch.Size([8, 372])
torch.Size([8, 298])
torch.Size([8, 283])
torch.Size([8, 341])
torch.Size([8, 335])
torch.Size([8, 320])
torch.Size([8, 286])
torch.Size([8, 301])
torch.Size([8, 316])
torch.Size([8, 289])
torch.Size([8, 326])
torch.Size([8, 224])
torch.Size([8, 237])
torch.Size([8, 309])
torch.Size([8, 256])
torch.Size([8, 289])
torch.Size([8, 287])
torch.Size([8, 375])
torch.Size([8, 217])
torch.Size([8, 528])
torch.Size([8, 220])
torch.Size([8, 198])
torch.Size([8, 197])
torch.Size([8, 164])
torch.Size([8, 145])
torch.Size([8, 200])
torch.Size([8, 230])
torch.Size([8, 203])
torch.Size([8, 185])
torch.Size([8, 317])
torch.Size([8, 134])
torch.Size([8, 199])
torch.Size([8, 127])
torch.Size([8, 131])
torch.Size([8, 116])
torch.Size([8, 178])
torch.Size([8, 111])
torch.Size([8, 132])
torch.Size([8, 113])
torch.Size([8, 110])
torch.Size([8, 147])
torch.Size([8, 95])
torch.Size([8, 111])
torch.Size([8, 127])
torch.Size([8, 65])
torch.Size([8, 78])
torch.Size([8, 76])
torch.Size([8, 141])
torch.Size([8, 70])
torch.Size([8, 69])
torch.Size([8, 99])
torch.Size([8, 81])
torch.Size([8, 68])
torch.Size([8, 69])
torch.Size([8, 46])
torch.Size([8, 57])
torch.Size([8, 63])
torch.Size([8, 103])
torch.Size([8, 69])
torch.Size([8, 38])
torch.Size([8, 69])
torch.Size([8, 134])
torch.Size([8, 50])
torch.Size([8, 51])
torch.Size([8, 44])
torch.Size([8, 67])
torch.Size([8, 84])
torch.Size([8, 74])
torch.Size([8, 52])
torch.Size([8, 65])
torch.Size([8, 77])
torch.Size([8, 44])
torch.Size([8, 49])
torch.Size([8, 56])
torch.Size([8, 56])
torch.Size([8, 38])
torch.Size([8, 48])
torch.Size([8, 42])
torch.Size([8, 34])
torch.Size([8, 31])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 526])
torch.Size([8, 516])
torch.Size([8, 494])
torch.Size([8, 460])
torch.Size([8, 505])
torch.Size([8, 414])
torch.Size([8, 528])
torch.Size([8, 528])
torch.Size([8, 401])
torch.Size([8, 519])
torch.Size([8, 399])
torch.Size([8, 364])
torch.Size([8, 385])
torch.Size([8, 378])
torch.Size([8, 414])
torch.Size([8, 385])
torch.Size([8, 406])
torch.Size([8, 392])
torch.Size([8, 340])
torch.Size([8, 326])
torch.Size([8, 528])
torch.Size([8, 330])
torch.Size([8, 399])
torch.Size([8, 293])
torch.Size([8, 320])
torch.Size([8, 278])
torch.Size([8, 260])
torch.Size([8, 269])
torch.Size([8, 304])
torch.Size([8, 248])
torch.Size([8, 387])
torch.Size([8, 312])
torch.Size([8, 269])
torch.Size([8, 255])
torch.Size([8, 247])
torch.Size([8, 248])
torch.Size([8, 346])
torch.Size([8, 291])
torch.Size([8, 197])
torch.Size([8, 200])
torch.Size([8, 215])
torch.Size([8, 207])
torch.Size([8, 188])
torch.Size([8, 210])
torch.Size([8, 284])
torch.Size([8, 159])
torch.Size([8, 159])
torch.Size([8, 183])
torch.Size([8, 166])
torch.Size([8, 144])
torch.Size([8, 224])
torch.Size([8, 135])
torch.Size([8, 114])
torch.Size([8, 203])
torch.Size([8, 170])
torch.Size([8, 170])
torch.Size([8, 138])
torch.Size([8, 147])
torch.Size([8, 189])
torch.Size([8, 105])
torch.Size([8, 96])
torch.Size([8, 84])
torch.Size([8, 93])
torch.Size([8, 75])
torch.Size([8, 68])
torch.Size([8, 81])
torch.Size([8, 109])
torch.Size([8, 60])
torch.Size([8, 46])
torch.Size([8, 82])
torch.Size([8, 57])
torch.Size([8, 47])
torch.Size([8, 70])
torch.Size([8, 60])
torch.Size([8, 59])
torch.Size([8, 47])
torch.Size([8, 88])
torch.Size([8, 66])
torch.Size([8, 83])
torch.Size([8, 57])
torch.Size([8, 57])
torch.Size([8, 57])
torch.Size([8, 46])
torch.Size([8, 71])
torch.Size([8, 65])
torch.Size([8, 48])
torch.Size([8, 46])
torch.Size([1, 24])

  0%|          | 0/1000 [00:00<?, ?it/s][A
  0%|          | 2/1000 [00:00<01:29, 11.18it/s][A
  0%|          | 4/1000 [00:00<02:26,  6.81it/s][A
  0%|          | 5/1000 [00:00<02:37,  6.31it/s][A
  1%|          | 6/1000 [00:00<02:47,  5.94it/s][A
  1%|          | 7/1000 [00:01<02:51,  5.77it/s][A
  1%|          | 8/1000 [00:01<02:53,  5.71it/s][A
  1%|          | 9/1000 [00:01<03:00,  5.50it/s][A
  1%|          | 10/1000 [00:01<02:59,  5.51it/s][A
  1%|          | 11/1000 [00:01<02:59,  5.50it/s][A
  1%|          | 12/1000 [00:02<03:03,  5.39it/s][A
  1%|▏         | 13/1000 [00:02<03:00,  5.48it/s][A
  1%|▏         | 14/1000 [00:02<03:00,  5.46it/s][A
  2%|▏         | 15/1000 [00:02<03:18,  4.97it/s][A
  2%|▏         | 16/1000 [00:02<03:12,  5.11it/s][A
  2%|▏         | 17/1000 [00:03<03:09,  5.20it/s][A
  2%|▏         | 18/1000 [00:03<03:07,  5.25it/s][A
  2%|▏         | 19/1000 [00:03<03:24,  4.80it/s][A
  2%|▏         | 20/1000 [00:03<03:28,  4.71it/s][A
  2%|▏         | 21/1000 [00:03<03:16,  4.98it/s][A
  2%|▏         | 22/1000 [00:04<03:08,  5.18it/s][A
  2%|▏         | 23/1000 [00:04<03:03,  5.32it/s][A
  2%|▏         | 24/1000 [00:04<03:01,  5.38it/s][A
  2%|▎         | 25/1000 [00:04<02:58,  5.46it/s][A
  3%|▎         | 26/1000 [00:04<02:59,  5.42it/s][A
  3%|▎         | 27/1000 [00:04<02:59,  5.42it/s][A
  3%|▎         | 28/1000 [00:05<03:09,  5.13it/s][A
  3%|▎         | 29/1000 [00:05<03:05,  5.22it/s][A
  3%|▎         | 30/1000 [00:05<03:01,  5.34it/s][A
  3%|▎         | 31/1000 [00:05<03:01,  5.35it/s][A
  3%|▎         | 32/1000 [00:05<03:19,  4.86it/s][A
  3%|▎         | 33/1000 [00:06<03:22,  4.77it/s][A
  3%|▎         | 34/1000 [00:06<03:15,  4.95it/s][A
  4%|▎         | 35/1000 [00:06<03:09,  5.09it/s][A
  4%|▎         | 36/1000 [00:06<03:05,  5.21it/s][A
  4%|▎         | 37/1000 [00:06<03:02,  5.28it/s][A
  4%|▍         | 38/1000 [00:07<02:59,  5.36it/s][A
  4%|▍         | 39/1000 [00:07<03:14,  4.95it/s][A
  4%|▍         | 40/1000 [00:07<03:28,  4.59it/s][A
  4%|▍         | 41/1000 [00:07<03:18,  4.84it/s][A
  4%|▍         | 42/1000 [00:07<03:10,  5.04it/s][A
  4%|▍         | 43/1000 [00:08<03:06,  5.14it/s][A
  4%|▍         | 44/1000 [00:08<02:57,  5.38it/s][A
  4%|▍         | 45/1000 [00:08<02:50,  5.59it/s][A
  5%|▍         | 46/1000 [00:08<02:47,  5.68it/s][A
  5%|▍         | 47/1000 [00:08<02:48,  5.66it/s][A
  5%|▍         | 48/1000 [00:09<03:06,  5.11it/s][A
  5%|▍         | 49/1000 [00:09<03:03,  5.19it/s][A
  5%|▌         | 50/1000 [00:09<03:12,  4.94it/s][A
  5%|▌         | 51/1000 [00:09<03:01,  5.23it/s][A
  5%|▌         | 52/1000 [00:09<03:15,  4.85it/s][A
  5%|▌         | 53/1000 [00:10<03:11,  4.94it/s][A
  5%|▌         | 54/1000 [00:10<03:15,  4.84it/s][A
  6%|▌         | 55/1000 [00:10<03:09,  4.98it/s][A
  6%|▌         | 56/1000 [00:10<03:03,  5.14it/s][A
  6%|▌         | 57/1000 [00:10<03:00,  5.22it/s][A
  6%|▌         | 58/1000 [00:11<02:59,  5.25it/s][A
  6%|▌         | 59/1000 [00:11<02:57,  5.30it/s][A
  6%|▌         | 60/1000 [00:11<03:14,  4.84it/s][A
  6%|▌         | 61/1000 [00:11<03:09,  4.97it/s][A
  6%|▌         | 62/1000 [00:11<03:12,  4.88it/s][A
  6%|▋         | 63/1000 [00:12<03:05,  5.04it/s][A
  6%|▋         | 64/1000 [00:12<03:01,  5.16it/s][A
  6%|▋         | 65/1000 [00:12<03:07,  4.99it/s][A
  7%|▋         | 66/1000 [00:12<03:02,  5.12it/s][A
  7%|▋         | 67/1000 [00:12<03:07,  4.97it/s][A
  7%|▋         | 68/1000 [00:12<02:59,  5.19it/s][A
  7%|▋         | 69/1000 [00:13<02:56,  5.26it/s][A
  7%|▋         | 70/1000 [00:13<02:51,  5.42it/s][A
  7%|▋         | 71/1000 [00:13<02:50,  5.44it/s][A
  7%|▋         | 72/1000 [00:13<02:51,  5.41it/s][A
  7%|▋         | 73/1000 [00:13<03:03,  5.04it/s][A
  7%|▋         | 74/1000 [00:14<02:53,  5.32it/s][A
  8%|▊         | 75/1000 [00:14<03:01,  5.10it/s][A
  8%|▊         | 76/1000 [00:14<03:00,  5.13it/s][A
  8%|▊         | 77/1000 [00:14<02:58,  5.18it/s][A
  8%|▊         | 78/1000 [00:14<02:48,  5.47it/s][A
  8%|▊         | 79/1000 [00:15<02:47,  5.49it/s][A
  8%|▊         | 80/1000 [00:15<02:47,  5.49it/s][A
  8%|▊         | 81/1000 [00:15<02:44,  5.59it/s][A
  8%|▊         | 82/1000 [00:15<02:43,  5.61it/s][A
  8%|▊         | 83/1000 [00:15<02:45,  5.56it/s][A
  8%|▊         | 84/1000 [00:16<03:04,  4.97it/s][A
  8%|▊         | 85/1000 [00:16<02:58,  5.11it/s][A
  9%|▊         | 86/1000 [00:16<02:56,  5.18it/s][A
  9%|▊         | 87/1000 [00:16<03:01,  5.03it/s][A
  9%|▉         | 88/1000 [00:16<02:59,  5.09it/s][A
  9%|▉         | 89/1000 [00:17<03:11,  4.77it/s][A
  9%|▉         | 90/1000 [00:17<02:59,  5.06it/s][A
  9%|▉         | 91/1000 [00:17<03:12,  4.72it/s][A
  9%|▉         | 92/1000 [00:17<03:19,  4.54it/s][A
  9%|▉         | 93/1000 [00:17<03:12,  4.70it/s][A
  9%|▉         | 94/1000 [00:18<03:03,  4.93it/s][A
 10%|▉         | 95/1000 [00:18<02:54,  5.20it/s][A
 10%|▉         | 96/1000 [00:18<02:54,  5.19it/s][A
 10%|▉         | 97/1000 [00:18<02:50,  5.29it/s][A
 10%|▉         | 98/1000 [00:18<02:50,  5.30it/s][A
 10%|▉         | 99/1000 [00:18<02:48,  5.33it/s][A
 10%|█         | 100/1000 [00:19<02:47,  5.37it/s][A
 10%|█         | 101/1000 [00:19<03:02,  4.94it/s][A
 10%|█         | 102/1000 [00:19<02:56,  5.08it/s][A
 10%|█         | 103/1000 [00:19<02:51,  5.22it/s][A
 10%|█         | 104/1000 [00:19<02:43,  5.47it/s][A
 10%|█         | 105/1000 [00:20<02:44,  5.44it/s][A
 11%|█         | 106/1000 [00:20<02:52,  5.19it/s][A
 11%|█         | 107/1000 [00:20<02:58,  5.00it/s][A
 11%|█         | 108/1000 [00:20<03:03,  4.87it/s][A
 11%|█         | 109/1000 [00:20<02:57,  5.02it/s][A
 11%|█         | 110/1000 [00:21<02:52,  5.15it/s][A
 11%|█         | 111/1000 [00:21<02:48,  5.27it/s][A
 11%|█         | 112/1000 [00:21<02:45,  5.38it/s][A
 11%|█▏        | 113/1000 [00:21<02:51,  5.17it/s][A
 11%|█▏        | 114/1000 [00:21<02:50,  5.21it/s][A
 12%|█▏        | 115/1000 [00:22<02:50,  5.20it/s][A
 12%|█▏        | 116/1000 [00:22<02:46,  5.30it/s][A
 12%|█▏        | 117/1000 [00:22<02:46,  5.31it/s][A
 12%|█▏        | 118/1000 [00:22<02:52,  5.11it/s][A
 12%|█▏        | 119/1000 [00:22<02:43,  5.38it/s][A
 12%|█▏        | 120/1000 [00:22<02:43,  5.37it/s][A
 12%|█▏        | 121/1000 [00:23<02:42,  5.41it/s][A
 12%|█▏        | 122/1000 [00:23<02:48,  5.22it/s][A
 12%|█▏        | 123/1000 [00:23<02:44,  5.32it/s][A
 12%|█▏        | 124/1000 [00:23<02:59,  4.88it/s][A
 12%|█▎        | 125/1000 [00:24<03:03,  4.78it/s][A
 13%|█▎        | 126/1000 [00:24<03:04,  4.72it/s][A
 13%|█▎        | 127/1000 [00:24<02:59,  4.87it/s][A
 13%|█▎        | 128/1000 [00:24<02:48,  5.19it/s][A
 13%|█▎        | 129/1000 [00:24<02:53,  5.02it/s][A
 13%|█▎        | 130/1000 [00:25<02:50,  5.10it/s][A
 13%|█▎        | 131/1000 [00:25<02:46,  5.22it/s][A
 13%|█▎        | 132/1000 [00:25<02:43,  5.31it/s][A
 13%|█▎        | 133/1000 [00:25<02:40,  5.39it/s][A
 13%|█▎        | 134/1000 [00:25<02:40,  5.38it/s][A
 14%|█▎        | 135/1000 [00:25<02:37,  5.50it/s][A
 14%|█▎        | 136/1000 [00:26<02:36,  5.53it/s][A
 14%|█▎        | 137/1000 [00:26<02:37,  5.50it/s][A
 14%|█▍        | 138/1000 [00:26<02:45,  5.20it/s][A
 14%|█▍        | 139/1000 [00:26<02:35,  5.52it/s][A
 14%|█▍        | 140/1000 [00:26<02:35,  5.54it/s][A
 14%|█▍        | 141/1000 [00:27<02:35,  5.51it/s][A
 14%|█▍        | 142/1000 [00:27<02:38,  5.42it/s][A
 14%|█▍        | 143/1000 [00:27<02:51,  5.00it/s][A
 14%|█▍        | 144/1000 [00:27<02:48,  5.07it/s][A
 14%|█▍        | 145/1000 [00:27<02:44,  5.19it/s][A
 15%|█▍        | 146/1000 [00:27<02:36,  5.46it/s][A
 15%|█▍        | 147/1000 [00:28<02:37,  5.43it/s][A
 15%|█▍        | 148/1000 [00:28<02:51,  4.98it/s][A
 15%|█▍        | 149/1000 [00:28<02:41,  5.28it/s][A
 15%|█▌        | 150/1000 [00:28<02:33,  5.54it/s][A
 15%|█▌        | 151/1000 [00:28<02:33,  5.53it/s][A
 15%|█▌        | 152/1000 [00:29<02:30,  5.65it/s][A
 15%|█▌        | 153/1000 [00:29<02:28,  5.71it/s][A
 15%|█▌        | 154/1000 [00:29<02:33,  5.52it/s][A
 16%|█▌        | 155/1000 [00:29<02:48,  5.01it/s][A
 16%|█▌        | 156/1000 [00:29<02:38,  5.31it/s][A
 16%|█▌        | 157/1000 [00:30<02:37,  5.36it/s][A
 16%|█▌        | 158/1000 [00:30<02:31,  5.57it/s][A
 16%|█▌        | 159/1000 [00:30<02:31,  5.56it/s][A
 16%|█▌        | 160/1000 [00:30<02:31,  5.53it/s][A
 16%|█▌        | 161/1000 [00:30<02:29,  5.63it/s][A
 16%|█▌        | 162/1000 [00:30<02:30,  5.56it/s][A
 16%|█▋        | 163/1000 [00:31<02:24,  5.77it/s][A
 16%|█▋        | 164/1000 [00:31<02:33,  5.43it/s][A
 16%|█▋        | 165/1000 [00:31<02:35,  5.38it/s][A
 17%|█▋        | 166/1000 [00:31<02:51,  4.86it/s][A
 17%|█▋        | 167/1000 [00:31<02:46,  5.01it/s][A
 17%|█▋        | 168/1000 [00:32<02:39,  5.21it/s][A
 17%|█▋        | 169/1000 [00:32<02:36,  5.31it/s][A
 17%|█▋        | 170/1000 [00:32<02:34,  5.38it/s][A
 17%|█▋        | 171/1000 [00:32<02:33,  5.40it/s][A
 17%|█▋        | 172/1000 [00:32<02:32,  5.44it/s][A
 17%|█▋        | 173/1000 [00:32<02:31,  5.46it/s][A
 17%|█▋        | 174/1000 [00:33<02:44,  5.02it/s][A
 18%|█▊        | 175/1000 [00:33<02:41,  5.12it/s][A
 18%|█▊        | 176/1000 [00:33<02:39,  5.18it/s][A
 18%|█▊        | 177/1000 [00:33<02:36,  5.25it/s][A
 18%|█▊        | 178/1000 [00:33<02:28,  5.53it/s][A
 18%|█▊        | 179/1000 [00:34<02:29,  5.49it/s][A
 18%|█▊        | 180/1000 [00:34<02:28,  5.51it/s][A
 18%|█▊        | 181/1000 [00:34<02:29,  5.47it/s][A
 18%|█▊        | 182/1000 [00:34<02:29,  5.48it/s][A
 18%|█▊        | 183/1000 [00:34<02:35,  5.24it/s][A
 18%|█▊        | 184/1000 [00:35<02:33,  5.31it/s][A
 18%|█▊        | 185/1000 [00:35<02:32,  5.34it/s][A
 19%|█▊        | 186/1000 [00:35<02:45,  4.91it/s][A
 19%|█▊        | 187/1000 [00:35<02:42,  5.00it/s][A
 19%|█▉        | 188/1000 [00:35<02:33,  5.29it/s][A
 19%|█▉        | 189/1000 [00:36<02:31,  5.37it/s][A
 19%|█▉        | 190/1000 [00:36<02:29,  5.40it/s][A
 19%|█▉        | 191/1000 [00:36<02:29,  5.42it/s][A
 19%|█▉        | 192/1000 [00:36<02:30,  5.38it/s][A
 19%|█▉        | 193/1000 [00:36<02:29,  5.41it/s][A
 19%|█▉        | 194/1000 [00:36<02:30,  5.35it/s][A
 20%|█▉        | 195/1000 [00:37<02:28,  5.41it/s][A
 20%|█▉        | 196/1000 [00:37<02:30,  5.34it/s][A
 20%|█▉        | 197/1000 [00:37<02:30,  5.33it/s][A
 20%|█▉        | 198/1000 [00:37<02:37,  5.08it/s][A
 20%|█▉        | 199/1000 [00:37<02:30,  5.32it/s][A
 20%|██        | 200/1000 [00:38<02:25,  5.50it/s][A
 20%|██        | 201/1000 [00:38<02:25,  5.49it/s][A
 20%|██        | 202/1000 [00:38<02:25,  5.48it/s][A
 20%|██        | 203/1000 [00:38<02:27,  5.39it/s][A
 20%|██        | 204/1000 [00:38<02:41,  4.94it/s][A
 20%|██        | 205/1000 [00:39<02:37,  5.06it/s][A
 21%|██        | 206/1000 [00:39<02:46,  4.77it/s][A
 21%|██        | 207/1000 [00:39<02:46,  4.77it/s][A
 21%|██        | 208/1000 [00:39<02:39,  4.96it/s][A
 21%|██        | 209/1000 [00:39<02:42,  4.87it/s][A
 21%|██        | 210/1000 [00:40<02:36,  5.04it/s][A
 21%|██        | 211/1000 [00:40<02:31,  5.22it/s][A
 21%|██        | 212/1000 [00:40<02:39,  4.94it/s][A
 21%|██▏       | 213/1000 [00:40<02:53,  4.54it/s][A
 21%|██▏       | 214/1000 [00:40<02:45,  4.75it/s][A
 22%|██▏       | 215/1000 [00:41<02:38,  4.95it/s][A
 22%|██▏       | 216/1000 [00:41<02:47,  4.68it/s][A
 22%|██▏       | 217/1000 [00:41<02:55,  4.46it/s][A
 22%|██▏       | 218/1000 [00:41<02:46,  4.69it/s][A
 22%|██▏       | 219/1000 [00:41<02:38,  4.92it/s][A
 22%|██▏       | 220/1000 [00:42<02:34,  5.03it/s][A
 22%|██▏       | 221/1000 [00:42<02:44,  4.73it/s][A
 22%|██▏       | 222/1000 [00:42<02:33,  5.07it/s][A
 22%|██▏       | 223/1000 [00:42<02:29,  5.19it/s][A
 22%|██▏       | 224/1000 [00:42<02:26,  5.31it/s][A
 22%|██▎       | 225/1000 [00:43<02:23,  5.40it/s][A
 23%|██▎       | 226/1000 [00:43<02:23,  5.39it/s][A
 23%|██▎       | 227/1000 [00:43<02:23,  5.37it/s][A
 23%|██▎       | 228/1000 [00:43<02:24,  5.34it/s][A
 23%|██▎       | 229/1000 [00:43<02:25,  5.30it/s][A
 23%|██▎       | 230/1000 [00:44<02:32,  5.06it/s][A
 23%|██▎       | 231/1000 [00:44<02:28,  5.16it/s][A
 23%|██▎       | 232/1000 [00:44<02:25,  5.27it/s][A
 23%|██▎       | 233/1000 [00:44<02:36,  4.90it/s][A
 23%|██▎       | 234/1000 [00:44<02:31,  5.07it/s][A
 24%|██▎       | 235/1000 [00:45<02:41,  4.73it/s][A
 24%|██▎       | 236/1000 [00:45<02:47,  4.56it/s][A
 24%|██▎       | 237/1000 [00:45<02:38,  4.80it/s][A
 24%|██▍       | 238/1000 [00:45<02:46,  4.57it/s][A
 24%|██▍       | 239/1000 [00:45<02:38,  4.81it/s][A
 24%|██▍       | 240/1000 [00:46<02:47,  4.54it/s][A
 24%|██▍       | 241/1000 [00:46<02:39,  4.77it/s][A
 24%|██▍       | 242/1000 [00:46<02:34,  4.92it/s][A
 24%|██▍       | 243/1000 [00:46<02:29,  5.07it/s][A
 24%|██▍       | 244/1000 [00:46<02:25,  5.19it/s][A
 24%|██▍       | 245/1000 [00:47<02:22,  5.30it/s][A
 25%|██▍       | 246/1000 [00:47<02:22,  5.29it/s][A
 25%|██▍       | 247/1000 [00:47<02:21,  5.34it/s][A
 25%|██▍       | 248/1000 [00:47<02:20,  5.35it/s][A
 25%|██▍       | 249/1000 [00:47<02:16,  5.50it/s][A
 25%|██▌       | 250/1000 [00:48<02:15,  5.53it/s][A
 25%|██▌       | 251/1000 [00:48<02:27,  5.07it/s][A
 25%|██▌       | 252/1000 [00:48<02:25,  5.15it/s][A
 25%|██▌       | 253/1000 [00:48<02:32,  4.91it/s][A
 25%|██▌       | 254/1000 [00:48<02:24,  5.15it/s][A
 26%|██▌       | 255/1000 [00:49<02:22,  5.24it/s][A
 26%|██▌       | 256/1000 [00:49<02:19,  5.32it/s][A
 26%|██▌       | 257/1000 [00:49<02:17,  5.39it/s][A
 26%|██▌       | 258/1000 [00:49<02:17,  5.38it/s][A
 26%|██▌       | 259/1000 [00:49<02:16,  5.41it/s][A
 26%|██▌       | 260/1000 [00:49<02:15,  5.47it/s][A
 26%|██▌       | 261/1000 [00:50<02:20,  5.25it/s][A
 26%|██▌       | 262/1000 [00:50<02:34,  4.79it/s][A
 26%|██▋       | 263/1000 [00:50<02:35,  4.73it/s][A
 26%|██▋       | 264/1000 [00:50<02:30,  4.90it/s][A
 26%|██▋       | 265/1000 [00:50<02:27,  4.99it/s][A
 27%|██▋       | 266/1000 [00:51<02:23,  5.10it/s][A
 27%|██▋       | 267/1000 [00:51<02:19,  5.24it/s][A
 27%|██▋       | 268/1000 [00:51<02:16,  5.38it/s][A
 27%|██▋       | 269/1000 [00:51<02:09,  5.63it/s][A
 27%|██▋       | 270/1000 [00:51<02:09,  5.62it/s][A
 27%|██▋       | 271/1000 [00:52<02:11,  5.54it/s][A
 27%|██▋       | 272/1000 [00:52<02:18,  5.26it/s][A
 27%|██▋       | 273/1000 [00:52<02:19,  5.22it/s][A
 27%|██▋       | 274/1000 [00:52<02:17,  5.28it/s][A
 28%|██▊       | 275/1000 [00:52<02:10,  5.54it/s][A
 28%|██▊       | 276/1000 [00:52<02:11,  5.49it/s][A
 28%|██▊       | 277/1000 [00:53<02:10,  5.55it/s][A
 28%|██▊       | 278/1000 [00:53<02:11,  5.50it/s][A
 28%|██▊       | 279/1000 [00:53<02:10,  5.52it/s][A
 28%|██▊       | 280/1000 [00:53<02:24,  4.98it/s][A
 28%|██▊       | 281/1000 [00:53<02:22,  5.04it/s][A
 28%|██▊       | 282/1000 [00:54<02:32,  4.71it/s][A
 28%|██▊       | 283/1000 [00:54<02:40,  4.47it/s][A
 28%|██▊       | 284/1000 [00:54<02:38,  4.51it/s][A
 28%|██▊       | 285/1000 [00:54<02:31,  4.72it/s][A
 29%|██▊       | 286/1000 [00:55<02:25,  4.92it/s][A
 29%|██▊       | 287/1000 [00:55<02:19,  5.11it/s][A
 29%|██▉       | 288/1000 [00:55<02:23,  4.97it/s][A
 29%|██▉       | 289/1000 [00:55<02:23,  4.96it/s][A
 29%|██▉       | 290/1000 [00:55<02:34,  4.61it/s][A
 29%|██▉       | 291/1000 [00:56<02:26,  4.84it/s][A
 29%|██▉       | 292/1000 [00:56<02:21,  4.99it/s][A
 29%|██▉       | 293/1000 [00:56<02:17,  5.16it/s][A
 29%|██▉       | 294/1000 [00:56<02:20,  5.02it/s][A
 30%|██▉       | 295/1000 [00:56<02:19,  5.07it/s][A
 30%|██▉       | 296/1000 [00:57<02:15,  5.18it/s][A
 30%|██▉       | 297/1000 [00:57<02:14,  5.22it/s][A
 30%|██▉       | 298/1000 [00:57<02:12,  5.29it/s][A
 30%|██▉       | 299/1000 [00:57<02:11,  5.35it/s][A
 30%|███       | 300/1000 [00:57<02:21,  4.93it/s][A
 30%|███       | 301/1000 [00:58<02:18,  5.06it/s][A
 30%|███       | 302/1000 [00:58<02:26,  4.76it/s][A
 30%|███       | 303/1000 [00:58<02:20,  4.97it/s][A
 30%|███       | 304/1000 [00:58<02:15,  5.13it/s][A
 30%|███       | 305/1000 [00:58<02:07,  5.45it/s][A
 31%|███       | 306/1000 [00:58<02:02,  5.66it/s][A
 31%|███       | 307/1000 [00:59<02:15,  5.12it/s][A
 31%|███       | 308/1000 [00:59<02:13,  5.19it/s][A
 31%|███       | 309/1000 [00:59<02:11,  5.25it/s][A
 31%|███       | 310/1000 [00:59<02:11,  5.27it/s][A
 31%|███       | 311/1000 [00:59<02:15,  5.10it/s][A
 31%|███       | 312/1000 [01:00<02:12,  5.17it/s][A
 31%|███▏      | 313/1000 [01:00<02:22,  4.82it/s][A
 31%|███▏      | 314/1000 [01:00<02:17,  5.00it/s][A
 32%|███▏      | 315/1000 [01:00<02:15,  5.06it/s][A
 32%|███▏      | 316/1000 [01:00<02:07,  5.37it/s][A
 32%|███▏      | 317/1000 [01:01<02:13,  5.13it/s][A
 32%|███▏      | 318/1000 [01:01<02:12,  5.13it/s][A
 32%|███▏      | 319/1000 [01:01<02:11,  5.18it/s][A
 32%|███▏      | 320/1000 [01:01<02:08,  5.28it/s][A
 32%|███▏      | 321/1000 [01:01<02:19,  4.88it/s][A
 32%|███▏      | 322/1000 [01:02<02:09,  5.22it/s][A
 32%|███▏      | 323/1000 [01:02<02:19,  4.84it/s][A
 32%|███▏      | 324/1000 [01:02<02:16,  4.96it/s][A
 32%|███▎      | 325/1000 [01:02<02:24,  4.66it/s][A
 33%|███▎      | 326/1000 [01:02<02:18,  4.87it/s][A
 33%|███▎      | 327/1000 [01:03<02:13,  5.03it/s][A
 33%|███▎      | 328/1000 [01:03<02:11,  5.11it/s][A
 33%|███▎      | 329/1000 [01:03<02:09,  5.20it/s][A
 33%|███▎      | 330/1000 [01:03<02:18,  4.83it/s][A
 33%|███▎      | 331/1000 [01:03<02:16,  4.90it/s][A
 33%|███▎      | 332/1000 [01:04<02:10,  5.12it/s][A
 33%|███▎      | 333/1000 [01:04<02:13,  4.98it/s][A
 33%|███▎      | 334/1000 [01:04<02:05,  5.31it/s][A
 34%|███▎      | 335/1000 [01:04<02:02,  5.44it/s][A
 34%|███▎      | 336/1000 [01:04<02:02,  5.43it/s][A
 34%|███▎      | 337/1000 [01:05<02:01,  5.46it/s][A
 34%|███▍      | 338/1000 [01:05<02:00,  5.50it/s][A
 34%|███▍      | 339/1000 [01:05<02:08,  5.15it/s][A
 34%|███▍      | 340/1000 [01:05<02:17,  4.82it/s][A
 34%|███▍      | 341/1000 [01:05<02:11,  5.00it/s][A
 34%|███▍      | 342/1000 [01:06<02:20,  4.67it/s][A
 34%|███▍      | 343/1000 [01:06<02:21,  4.64it/s][A
 34%|███▍      | 344/1000 [01:06<02:26,  4.48it/s][A
 34%|███▍      | 345/1000 [01:06<02:19,  4.69it/s][A
 35%|███▍      | 346/1000 [01:06<02:08,  5.08it/s][A
 35%|███▍      | 347/1000 [01:07<02:05,  5.22it/s][A
 35%|███▍      | 348/1000 [01:07<02:02,  5.31it/s][A
 35%|███▍      | 349/1000 [01:07<01:57,  5.52it/s][A
 35%|███▌      | 350/1000 [01:07<01:57,  5.51it/s][A
 35%|███▌      | 351/1000 [01:07<01:58,  5.47it/s][A
 35%|███▌      | 352/1000 [01:07<01:58,  5.49it/s][A
 35%|███▌      | 353/1000 [01:08<02:10,  4.94it/s][A
 35%|███▌      | 354/1000 [01:08<02:08,  5.03it/s][A
 36%|███▌      | 355/1000 [01:08<02:06,  5.10it/s][A
 36%|███▌      | 356/1000 [01:08<02:15,  4.76it/s][A
 36%|███▌      | 357/1000 [01:09<02:23,  4.48it/s][A
 36%|███▌      | 358/1000 [01:09<02:16,  4.70it/s][A
 36%|███▌      | 359/1000 [01:09<02:10,  4.92it/s][A
 36%|███▌      | 360/1000 [01:09<02:07,  5.02it/s][A
 36%|███▌      | 361/1000 [01:09<02:00,  5.32it/s][A
 36%|███▌      | 362/1000 [01:09<01:55,  5.52it/s][A
 36%|███▋      | 363/1000 [01:10<01:56,  5.47it/s][A
 36%|███▋      | 364/1000 [01:10<02:06,  5.02it/s][A
 36%|███▋      | 365/1000 [01:10<02:03,  5.13it/s][A
 37%|███▋      | 366/1000 [01:10<02:01,  5.21it/s][A
 37%|███▋      | 367/1000 [01:10<02:00,  5.25it/s][A
 37%|███▋      | 368/1000 [01:11<02:09,  4.88it/s][A
 37%|███▋      | 369/1000 [01:11<02:18,  4.56it/s][A
 37%|███▋      | 370/1000 [01:11<02:13,  4.72it/s][A
 37%|███▋      | 371/1000 [01:11<02:07,  4.94it/s][A
 37%|███▋      | 372/1000 [01:12<02:03,  5.07it/s][A
 37%|███▋      | 373/1000 [01:12<02:00,  5.19it/s][A
 37%|███▋      | 374/1000 [01:12<01:59,  5.22it/s][A
 38%|███▊      | 375/1000 [01:12<01:58,  5.29it/s][A
 38%|███▊      | 376/1000 [01:12<02:09,  4.82it/s][A
 38%|███▊      | 377/1000 [01:13<02:04,  4.99it/s][A
 38%|███▊      | 378/1000 [01:13<02:00,  5.15it/s][A
 38%|███▊      | 379/1000 [01:13<02:09,  4.79it/s][A
 38%|███▊      | 380/1000 [01:13<02:06,  4.89it/s][A
 38%|███▊      | 381/1000 [01:13<02:02,  5.07it/s][A
 38%|███▊      | 382/1000 [01:13<01:59,  5.17it/s][A
 38%|███▊      | 383/1000 [01:14<01:58,  5.23it/s][A
 38%|███▊      | 384/1000 [01:14<01:56,  5.30it/s][A
 38%|███▊      | 385/1000 [01:14<01:51,  5.50it/s][A
 39%|███▊      | 386/1000 [01:14<01:51,  5.49it/s][A
 39%|███▊      | 387/1000 [01:14<01:51,  5.51it/s][A
 39%|███▉      | 388/1000 [01:15<01:50,  5.52it/s][A
 39%|███▉      | 389/1000 [01:15<01:50,  5.54it/s][A
 39%|███▉      | 390/1000 [01:15<01:50,  5.53it/s][A
 39%|███▉      | 391/1000 [01:15<01:50,  5.52it/s][A
 39%|███▉      | 392/1000 [01:15<01:49,  5.54it/s][A
 39%|███▉      | 393/1000 [01:15<01:49,  5.57it/s][A
 39%|███▉      | 394/1000 [01:16<01:54,  5.29it/s][A
 40%|███▉      | 395/1000 [01:16<01:52,  5.40it/s][A
 40%|███▉      | 396/1000 [01:16<01:48,  5.58it/s][A
 40%|███▉      | 397/1000 [01:16<01:48,  5.53it/s][A
 40%|███▉      | 398/1000 [01:16<01:45,  5.73it/s][A
 40%|███▉      | 399/1000 [01:17<01:57,  5.13it/s][A
 40%|████      | 400/1000 [01:17<01:55,  5.18it/s][A
 40%|████      | 401/1000 [01:17<02:04,  4.82it/s][A
 40%|████      | 402/1000 [01:17<02:01,  4.94it/s][A
 40%|████      | 403/1000 [01:17<02:06,  4.74it/s][A
 40%|████      | 404/1000 [01:18<02:02,  4.85it/s][A
 40%|████      | 405/1000 [01:18<01:59,  4.99it/s][A
 41%|████      | 406/1000 [01:18<01:56,  5.11it/s][A
 41%|████      | 407/1000 [01:18<01:59,  4.96it/s][A
 41%|████      | 408/1000 [01:18<01:57,  5.03it/s][A
 41%|████      | 409/1000 [01:19<01:56,  5.09it/s][A
 41%|████      | 410/1000 [01:19<01:53,  5.19it/s][A
 41%|████      | 411/1000 [01:19<01:52,  5.25it/s][A
 41%|████      | 412/1000 [01:19<01:56,  5.04it/s][A
 41%|████▏     | 413/1000 [01:19<01:53,  5.16it/s][A
 41%|████▏     | 414/1000 [01:20<02:01,  4.81it/s][A
 42%|████▏     | 415/1000 [01:20<01:57,  4.99it/s][A
 42%|████▏     | 416/1000 [01:20<02:03,  4.71it/s][A
 42%|████▏     | 417/1000 [01:20<02:00,  4.84it/s][A
 42%|████▏     | 418/1000 [01:20<01:56,  4.99it/s][A
 42%|████▏     | 419/1000 [01:21<02:03,  4.70it/s][A
 42%|████▏     | 420/1000 [01:21<01:58,  4.91it/s][A
 42%|████▏     | 421/1000 [01:21<01:53,  5.12it/s][A
 42%|████▏     | 422/1000 [01:21<01:55,  5.02it/s][A
 42%|████▏     | 423/1000 [01:21<01:50,  5.24it/s][A
 42%|████▏     | 424/1000 [01:22<01:49,  5.28it/s][A
 42%|████▎     | 425/1000 [01:22<01:48,  5.32it/s][A
 43%|████▎     | 426/1000 [01:22<01:48,  5.29it/s][A
 43%|████▎     | 427/1000 [01:22<01:44,  5.48it/s][A
 43%|████▎     | 428/1000 [01:22<01:40,  5.70it/s][A
 43%|████▎     | 429/1000 [01:22<01:40,  5.66it/s][A
 43%|████▎     | 430/1000 [01:23<01:53,  5.03it/s][A
 43%|████▎     | 431/1000 [01:23<01:51,  5.09it/s][A
 43%|████▎     | 432/1000 [01:23<01:49,  5.21it/s][A
 43%|████▎     | 433/1000 [01:23<01:46,  5.31it/s][A
 43%|████▎     | 434/1000 [01:23<01:47,  5.29it/s][A
 44%|████▎     | 435/1000 [01:24<01:51,  5.09it/s][A
 44%|████▎     | 436/1000 [01:24<01:48,  5.20it/s][A
 44%|████▎     | 437/1000 [01:24<01:43,  5.42it/s][A
 44%|████▍     | 438/1000 [01:24<01:49,  5.15it/s][A
 44%|████▍     | 439/1000 [01:24<01:48,  5.15it/s][A
 44%|████▍     | 440/1000 [01:25<01:55,  4.84it/s][A
 44%|████▍     | 441/1000 [01:25<01:51,  5.01it/s][A
 44%|████▍     | 442/1000 [01:25<01:45,  5.27it/s][A
 44%|████▍     | 443/1000 [01:25<01:50,  5.05it/s][A
 44%|████▍     | 444/1000 [01:26<01:59,  4.66it/s][A
 44%|████▍     | 445/1000 [01:26<01:53,  4.87it/s][A
 45%|████▍     | 446/1000 [01:26<01:50,  5.02it/s][A
 45%|████▍     | 447/1000 [01:26<01:47,  5.12it/s][A
 45%|████▍     | 448/1000 [01:26<01:50,  4.98it/s][A
 45%|████▍     | 449/1000 [01:26<01:47,  5.10it/s][A
 45%|████▌     | 450/1000 [01:27<01:44,  5.25it/s][A
 45%|████▌     | 451/1000 [01:27<01:42,  5.34it/s][A
 45%|████▌     | 452/1000 [01:27<01:42,  5.33it/s][A
 45%|████▌     | 453/1000 [01:27<01:40,  5.46it/s][A
 45%|████▌     | 454/1000 [01:27<01:36,  5.67it/s][A
 46%|████▌     | 455/1000 [01:28<01:36,  5.63it/s][A
 46%|████▌     | 456/1000 [01:28<01:37,  5.59it/s][A
 46%|████▌     | 457/1000 [01:28<01:46,  5.09it/s][A
 46%|████▌     | 458/1000 [01:28<01:45,  5.13it/s][A
 46%|████▌     | 459/1000 [01:28<01:43,  5.22it/s][A
 46%|████▌     | 460/1000 [01:28<01:43,  5.24it/s][A
 46%|████▌     | 461/1000 [01:29<01:42,  5.28it/s][A
 46%|████▌     | 462/1000 [01:29<01:41,  5.28it/s][A
 46%|████▋     | 463/1000 [01:29<01:43,  5.20it/s][A
 46%|████▋     | 464/1000 [01:29<01:40,  5.35it/s][A
 46%|████▋     | 465/1000 [01:29<01:39,  5.38it/s][A
 47%|████▋     | 466/1000 [01:30<01:39,  5.39it/s][A
 47%|████▋     | 467/1000 [01:30<01:37,  5.44it/s][A
 47%|████▋     | 468/1000 [01:30<01:37,  5.45it/s][A
 47%|████▋     | 469/1000 [01:30<01:37,  5.46it/s][A
 47%|████▋     | 470/1000 [01:30<01:38,  5.39it/s][A
 47%|████▋     | 471/1000 [01:31<01:46,  4.95it/s][A
 47%|████▋     | 472/1000 [01:31<01:43,  5.09it/s][A
 47%|████▋     | 473/1000 [01:31<01:42,  5.15it/s][A
 47%|████▋     | 474/1000 [01:31<01:40,  5.24it/s][A
 48%|████▊     | 475/1000 [01:31<01:38,  5.32it/s][A
 48%|████▊     | 476/1000 [01:32<01:46,  4.91it/s][A
 48%|████▊     | 477/1000 [01:32<01:43,  5.04it/s][A
 48%|████▊     | 478/1000 [01:32<01:40,  5.18it/s][A
 48%|████▊     | 479/1000 [01:32<01:35,  5.44it/s][A
 48%|████▊     | 480/1000 [01:32<01:35,  5.45it/s][A
 48%|████▊     | 481/1000 [01:33<01:44,  4.96it/s][A
 48%|████▊     | 482/1000 [01:33<01:42,  5.07it/s][A
 48%|████▊     | 483/1000 [01:33<01:40,  5.13it/s][A
 48%|████▊     | 484/1000 [01:33<01:39,  5.19it/s][A
 48%|████▊     | 485/1000 [01:33<01:41,  5.09it/s][A
 49%|████▊     | 486/1000 [01:33<01:38,  5.20it/s][A
 49%|████▊     | 487/1000 [01:34<01:37,  5.27it/s][A
 49%|████▉     | 488/1000 [01:34<01:37,  5.28it/s][A
 49%|████▉     | 489/1000 [01:34<01:36,  5.31it/s][A
 49%|████▉     | 490/1000 [01:34<01:34,  5.37it/s][A
 49%|████▉     | 491/1000 [01:34<01:31,  5.55it/s][A
 49%|████▉     | 492/1000 [01:35<01:31,  5.57it/s][A
 49%|████▉     | 493/1000 [01:35<01:36,  5.25it/s][A
 49%|████▉     | 494/1000 [01:35<01:32,  5.46it/s][A
 50%|████▉     | 495/1000 [01:35<01:32,  5.48it/s][A
 50%|████▉     | 496/1000 [01:35<01:30,  5.57it/s][A
 50%|████▉     | 497/1000 [01:35<01:31,  5.52it/s][A
 50%|████▉     | 498/1000 [01:36<01:30,  5.52it/s][A
 50%|████▉     | 499/1000 [01:36<01:30,  5.53it/s][A
 50%|█████     | 500/1000 [01:36<01:35,  5.25it/s][A
 50%|█████     | 501/1000 [01:36<01:33,  5.34it/s][A
 50%|█████     | 502/1000 [01:36<01:32,  5.38it/s][A
 50%|█████     | 503/1000 [01:37<01:32,  5.38it/s][A
 50%|█████     | 504/1000 [01:37<01:32,  5.34it/s][A
 50%|█████     | 505/1000 [01:37<01:42,  4.82it/s][A
 51%|█████     | 506/1000 [01:37<01:38,  5.01it/s][A
 51%|█████     | 507/1000 [01:37<01:35,  5.18it/s][A
 51%|█████     | 508/1000 [01:38<01:32,  5.30it/s][A
 51%|█████     | 509/1000 [01:38<01:31,  5.38it/s][A
 51%|█████     | 510/1000 [01:38<01:29,  5.48it/s][A
 51%|█████     | 511/1000 [01:38<01:30,  5.42it/s][A
 51%|█████     | 512/1000 [01:38<01:38,  4.97it/s][A
 51%|█████▏    | 513/1000 [01:39<01:38,  4.96it/s][A
 51%|█████▏    | 514/1000 [01:39<01:44,  4.65it/s][A
 52%|█████▏    | 515/1000 [01:39<01:40,  4.84it/s][A
 52%|█████▏    | 516/1000 [01:39<01:36,  5.00it/s][A
 52%|█████▏    | 517/1000 [01:39<01:34,  5.11it/s][A
 52%|█████▏    | 518/1000 [01:40<01:32,  5.18it/s][A
 52%|█████▏    | 519/1000 [01:40<01:30,  5.29it/s][A
 52%|█████▏    | 520/1000 [01:40<01:30,  5.32it/s][A
 52%|█████▏    | 521/1000 [01:40<01:30,  5.31it/s][A
 52%|█████▏    | 522/1000 [01:40<01:29,  5.35it/s][A
 52%|█████▏    | 523/1000 [01:41<01:38,  4.86it/s][A
 52%|█████▏    | 524/1000 [01:41<01:35,  4.96it/s][A
 52%|█████▎    | 525/1000 [01:41<01:31,  5.19it/s][A
 53%|█████▎    | 526/1000 [01:41<01:39,  4.78it/s][A
 53%|█████▎    | 527/1000 [01:41<01:36,  4.90it/s][A
 53%|█████▎    | 528/1000 [01:42<01:30,  5.24it/s][A
 53%|█████▎    | 529/1000 [01:42<01:25,  5.52it/s][A
 53%|█████▎    | 530/1000 [01:42<01:33,  5.02it/s][A
 53%|█████▎    | 531/1000 [01:42<01:32,  5.08it/s][A
 53%|█████▎    | 532/1000 [01:42<01:26,  5.41it/s][A
 53%|█████▎    | 533/1000 [01:42<01:26,  5.41it/s][A
 53%|█████▎    | 534/1000 [01:43<01:25,  5.42it/s][A
 54%|█████▎    | 535/1000 [01:43<01:33,  4.98it/s][A
 54%|█████▎    | 536/1000 [01:43<01:31,  5.09it/s][A
 54%|█████▎    | 537/1000 [01:43<01:28,  5.21it/s][A
 54%|█████▍    | 538/1000 [01:43<01:32,  5.00it/s][A
 54%|█████▍    | 539/1000 [01:44<01:30,  5.10it/s][A
 54%|█████▍    | 540/1000 [01:44<01:27,  5.28it/s][A
 54%|█████▍    | 541/1000 [01:44<01:22,  5.57it/s][A
 54%|█████▍    | 542/1000 [01:44<01:20,  5.69it/s][A
 54%|█████▍    | 543/1000 [01:44<01:20,  5.67it/s][A
 54%|█████▍    | 544/1000 [01:45<01:21,  5.58it/s][A
 55%|█████▍    | 545/1000 [01:45<01:22,  5.50it/s][A
 55%|█████▍    | 546/1000 [01:45<01:30,  5.02it/s][A
 55%|█████▍    | 547/1000 [01:45<01:36,  4.69it/s][A
 55%|█████▍    | 548/1000 [01:45<01:29,  5.05it/s][A
 55%|█████▍    | 549/1000 [01:45<01:23,  5.41it/s][A
 55%|█████▌    | 550/1000 [01:46<01:23,  5.38it/s][A
 55%|█████▌    | 551/1000 [01:46<01:20,  5.58it/s][A
 55%|█████▌    | 552/1000 [01:46<01:21,  5.47it/s][A
 55%|█████▌    | 553/1000 [01:46<01:21,  5.48it/s][A
 55%|█████▌    | 554/1000 [01:46<01:18,  5.66it/s][A
 56%|█████▌    | 555/1000 [01:47<01:18,  5.64it/s][A
 56%|█████▌    | 556/1000 [01:47<01:18,  5.62it/s][A
 56%|█████▌    | 557/1000 [01:47<01:17,  5.73it/s][A
 56%|█████▌    | 558/1000 [01:47<01:18,  5.60it/s][A
 56%|█████▌    | 559/1000 [01:47<01:19,  5.56it/s][A
 56%|█████▌    | 560/1000 [01:47<01:19,  5.55it/s][A
 56%|█████▌    | 561/1000 [01:48<01:27,  5.04it/s][A
 56%|█████▌    | 562/1000 [01:48<01:25,  5.12it/s][A
 56%|█████▋    | 563/1000 [01:48<01:23,  5.23it/s][A
 56%|█████▋    | 564/1000 [01:48<01:22,  5.31it/s][A
 56%|█████▋    | 565/1000 [01:48<01:21,  5.33it/s][A
 57%|█████▋    | 566/1000 [01:49<01:21,  5.35it/s][A
 57%|█████▋    | 567/1000 [01:49<01:20,  5.36it/s][A
 57%|█████▋    | 568/1000 [01:49<01:20,  5.36it/s][A
 57%|█████▋    | 569/1000 [01:49<01:18,  5.52it/s][A
 57%|█████▋    | 570/1000 [01:49<01:19,  5.43it/s][A
 57%|█████▋    | 571/1000 [01:50<01:16,  5.64it/s][A
 57%|█████▋    | 572/1000 [01:50<01:16,  5.57it/s][A
 57%|█████▋    | 573/1000 [01:50<01:16,  5.55it/s][A
 57%|█████▋    | 574/1000 [01:50<01:22,  5.15it/s][A
 57%|█████▊    | 575/1000 [01:50<01:26,  4.91it/s][A
 58%|█████▊    | 576/1000 [01:51<01:23,  5.06it/s][A
 58%|█████▊    | 577/1000 [01:51<01:28,  4.76it/s][A
 58%|█████▊    | 578/1000 [01:51<01:25,  4.94it/s][A
 58%|█████▊    | 579/1000 [01:51<01:22,  5.08it/s][A
 58%|█████▊    | 580/1000 [01:51<01:20,  5.23it/s][A
 58%|█████▊    | 581/1000 [01:52<01:27,  4.79it/s][A
 58%|█████▊    | 582/1000 [01:52<01:23,  4.98it/s][A
 58%|█████▊    | 583/1000 [01:52<01:23,  5.00it/s][A
 58%|█████▊    | 584/1000 [01:52<01:29,  4.66it/s][A
 58%|█████▊    | 585/1000 [01:52<01:33,  4.44it/s][A
 59%|█████▊    | 586/1000 [01:53<01:30,  4.57it/s][A
 59%|█████▊    | 587/1000 [01:53<01:26,  4.78it/s][A
 59%|█████▉    | 588/1000 [01:53<01:20,  5.11it/s][A
 59%|█████▉    | 589/1000 [01:53<01:19,  5.16it/s][A
 59%|█████▉    | 590/1000 [01:53<01:24,  4.83it/s][A
 59%|█████▉    | 591/1000 [01:54<01:21,  4.99it/s][A
 59%|█████▉    | 592/1000 [01:54<01:26,  4.69it/s][A
 59%|█████▉    | 593/1000 [01:54<01:23,  4.88it/s][A
 59%|█████▉    | 594/1000 [01:54<01:28,  4.59it/s][A
 60%|█████▉    | 595/1000 [01:54<01:24,  4.77it/s][A
 60%|█████▉    | 596/1000 [01:55<01:21,  4.96it/s][A
 60%|█████▉    | 597/1000 [01:55<01:25,  4.69it/s][A
 60%|█████▉    | 598/1000 [01:55<01:22,  4.90it/s][A
 60%|█████▉    | 599/1000 [01:55<01:23,  4.83it/s][A
 60%|██████    | 600/1000 [01:55<01:20,  5.00it/s][A
 60%|██████    | 601/1000 [01:56<01:18,  5.07it/s][A
 60%|██████    | 602/1000 [01:56<01:17,  5.13it/s][A
 60%|██████    | 603/1000 [01:56<01:16,  5.20it/s][A
 60%|██████    | 604/1000 [01:56<01:21,  4.85it/s][A
 60%|██████    | 605/1000 [01:56<01:19,  4.94it/s][A
 61%|██████    | 606/1000 [01:57<01:17,  5.07it/s][A
 61%|██████    | 607/1000 [01:57<01:22,  4.77it/s][A
 61%|██████    | 608/1000 [01:57<01:19,  4.92it/s][A
 61%|██████    | 609/1000 [01:57<01:16,  5.08it/s][A
 61%|██████    | 610/1000 [01:57<01:14,  5.22it/s][A
 61%|██████    | 611/1000 [01:58<01:11,  5.47it/s][A
 61%|██████    | 612/1000 [01:58<01:08,  5.69it/s][A
 61%|██████▏   | 613/1000 [01:58<01:08,  5.64it/s][A
 61%|██████▏   | 614/1000 [01:58<01:10,  5.50it/s][A
 62%|██████▏   | 615/1000 [01:58<01:17,  4.98it/s][A
 62%|██████▏   | 616/1000 [01:59<01:15,  5.08it/s][A
 62%|██████▏   | 617/1000 [01:59<01:13,  5.18it/s][A
 62%|██████▏   | 618/1000 [01:59<01:12,  5.27it/s][A
 62%|██████▏   | 619/1000 [01:59<01:18,  4.85it/s][A
 62%|██████▏   | 620/1000 [01:59<01:16,  4.98it/s][A
 62%|██████▏   | 621/1000 [02:00<01:13,  5.15it/s][A
 62%|██████▏   | 622/1000 [02:00<01:12,  5.24it/s][A
 62%|██████▏   | 623/1000 [02:00<01:11,  5.26it/s][A
 62%|██████▏   | 624/1000 [02:00<01:10,  5.35it/s][A
 62%|██████▎   | 625/1000 [02:00<01:09,  5.40it/s][A
 63%|██████▎   | 626/1000 [02:00<01:07,  5.53it/s][A
 63%|██████▎   | 627/1000 [02:01<01:07,  5.55it/s][A
 63%|██████▎   | 628/1000 [02:01<01:07,  5.49it/s][A
 63%|██████▎   | 629/1000 [02:01<01:08,  5.45it/s][A
 63%|██████▎   | 630/1000 [02:01<01:05,  5.62it/s][A
 63%|██████▎   | 631/1000 [02:01<01:06,  5.55it/s][A
 63%|██████▎   | 632/1000 [02:02<01:06,  5.49it/s][A
 63%|██████▎   | 633/1000 [02:02<01:08,  5.39it/s][A
 63%|██████▎   | 634/1000 [02:02<01:07,  5.38it/s][A
 64%|██████▎   | 635/1000 [02:02<01:07,  5.42it/s][A
 64%|██████▎   | 636/1000 [02:02<01:07,  5.42it/s][A
 64%|██████▎   | 637/1000 [02:02<01:06,  5.46it/s][A
 64%|██████▍   | 638/1000 [02:03<01:06,  5.43it/s][A
 64%|██████▍   | 639/1000 [02:03<01:07,  5.36it/s][A
 64%|██████▍   | 640/1000 [02:03<01:09,  5.20it/s][A
 64%|██████▍   | 641/1000 [02:03<01:11,  5.01it/s][A
 64%|██████▍   | 642/1000 [02:03<01:09,  5.13it/s][A
 64%|██████▍   | 643/1000 [02:04<01:15,  4.73it/s][A
 64%|██████▍   | 644/1000 [02:04<01:13,  4.87it/s][A
 64%|██████▍   | 645/1000 [02:04<01:10,  5.05it/s][A
 65%|██████▍   | 646/1000 [02:04<01:14,  4.75it/s][A
 65%|██████▍   | 647/1000 [02:04<01:11,  4.93it/s][A
 65%|██████▍   | 648/1000 [02:05<01:12,  4.86it/s][A
 65%|██████▍   | 649/1000 [02:05<01:09,  5.02it/s][A
 65%|██████▌   | 650/1000 [02:05<01:10,  4.95it/s][A
 65%|██████▌   | 651/1000 [02:05<01:12,  4.84it/s][A
 65%|██████▌   | 652/1000 [02:06<01:09,  5.02it/s][A
 65%|██████▌   | 653/1000 [02:06<01:07,  5.10it/s][A
 65%|██████▌   | 654/1000 [02:06<01:06,  5.21it/s][A
 66%|██████▌   | 655/1000 [02:06<01:05,  5.27it/s][A
 66%|██████▌   | 656/1000 [02:06<01:05,  5.27it/s][A
 66%|██████▌   | 657/1000 [02:06<01:11,  4.82it/s][A
 66%|██████▌   | 658/1000 [02:07<01:08,  4.99it/s][A
 66%|██████▌   | 659/1000 [02:07<01:03,  5.34it/s][A
 66%|██████▌   | 660/1000 [02:07<01:02,  5.42it/s][A
 66%|██████▌   | 661/1000 [02:07<01:02,  5.44it/s][A
 66%|██████▌   | 662/1000 [02:07<01:02,  5.41it/s][A
 66%|██████▋   | 663/1000 [02:08<01:05,  5.16it/s][A
 66%|██████▋   | 664/1000 [02:08<01:01,  5.43it/s][A
 66%|██████▋   | 665/1000 [02:08<01:04,  5.17it/s][A
 67%|██████▋   | 666/1000 [02:08<01:10,  4.75it/s][A
 67%|██████▋   | 667/1000 [02:08<01:10,  4.70it/s][A
 67%|██████▋   | 668/1000 [02:09<01:06,  5.02it/s][A
 67%|██████▋   | 669/1000 [02:09<01:10,  4.67it/s][A
 67%|██████▋   | 670/1000 [02:09<01:07,  4.88it/s][A
 67%|██████▋   | 671/1000 [02:09<01:05,  5.06it/s][A
 67%|██████▋   | 672/1000 [02:09<01:03,  5.16it/s][A
 67%|██████▋   | 673/1000 [02:10<01:00,  5.39it/s][A
 67%|██████▋   | 674/1000 [02:10<01:00,  5.38it/s][A
 68%|██████▊   | 675/1000 [02:10<01:00,  5.40it/s][A
 68%|██████▊   | 676/1000 [02:10<00:59,  5.43it/s][A
 68%|██████▊   | 677/1000 [02:10<01:02,  5.19it/s][A
 68%|██████▊   | 678/1000 [02:11<01:00,  5.35it/s][A
 68%|██████▊   | 679/1000 [02:11<00:59,  5.38it/s][A
 68%|██████▊   | 680/1000 [02:11<00:59,  5.38it/s][A
 68%|██████▊   | 681/1000 [02:11<00:59,  5.36it/s][A
 68%|██████▊   | 682/1000 [02:11<01:02,  5.11it/s][A
 68%|██████▊   | 683/1000 [02:12<01:06,  4.76it/s][A
 68%|██████▊   | 684/1000 [02:12<01:04,  4.94it/s][A
 68%|██████▊   | 685/1000 [02:12<01:01,  5.11it/s][A
 69%|██████▊   | 686/1000 [02:12<00:59,  5.27it/s][A
 69%|██████▊   | 687/1000 [02:12<00:58,  5.35it/s][A
 69%|██████▉   | 688/1000 [02:12<00:55,  5.58it/s][A
 69%|██████▉   | 689/1000 [02:13<00:56,  5.51it/s][A
 69%|██████▉   | 690/1000 [02:13<00:57,  5.42it/s][A
 69%|██████▉   | 691/1000 [02:13<00:56,  5.47it/s][A
 69%|██████▉   | 692/1000 [02:13<00:59,  5.19it/s][A
 69%|██████▉   | 693/1000 [02:13<01:01,  5.00it/s][A
 69%|██████▉   | 694/1000 [02:14<00:59,  5.13it/s][A
 70%|██████▉   | 695/1000 [02:14<00:58,  5.24it/s][A
 70%|██████▉   | 696/1000 [02:14<00:57,  5.30it/s][A
 70%|██████▉   | 697/1000 [02:14<00:59,  5.09it/s][A
 70%|██████▉   | 698/1000 [02:14<00:58,  5.19it/s][A
 70%|██████▉   | 699/1000 [02:15<00:59,  5.04it/s][A
 70%|███████   | 700/1000 [02:15<01:00,  4.97it/s][A
 70%|███████   | 701/1000 [02:15<01:03,  4.69it/s][A
 70%|███████   | 702/1000 [02:15<01:01,  4.86it/s][A
 70%|███████   | 703/1000 [02:15<01:04,  4.57it/s][A
 70%|███████   | 704/1000 [02:16<01:05,  4.55it/s][A
 70%|███████   | 705/1000 [02:16<01:07,  4.35it/s][A
 71%|███████   | 706/1000 [02:16<01:07,  4.35it/s][A
 71%|███████   | 707/1000 [02:16<01:07,  4.37it/s][A
 71%|███████   | 708/1000 [02:17<01:05,  4.44it/s][A
 71%|███████   | 709/1000 [02:17<01:02,  4.69it/s][A
 71%|███████   | 710/1000 [02:17<01:03,  4.59it/s][A
 71%|███████   | 711/1000 [02:17<01:03,  4.53it/s][A
 71%|███████   | 712/1000 [02:17<01:04,  4.50it/s][A
 71%|███████▏  | 713/1000 [02:18<01:01,  4.69it/s][A
 71%|███████▏  | 714/1000 [02:18<00:58,  4.92it/s][A
 72%|███████▏  | 715/1000 [02:18<00:56,  5.04it/s][A
 72%|███████▏  | 716/1000 [02:18<00:53,  5.35it/s][A
 72%|███████▏  | 717/1000 [02:18<00:52,  5.37it/s][A
 72%|███████▏  | 718/1000 [02:19<00:52,  5.42it/s][A
 72%|███████▏  | 719/1000 [02:19<00:54,  5.20it/s][A
 72%|███████▏  | 720/1000 [02:19<00:53,  5.19it/s][A
 72%|███████▏  | 721/1000 [02:19<00:55,  5.01it/s][A
 72%|███████▏  | 722/1000 [02:19<00:54,  5.12it/s][A
 72%|███████▏  | 723/1000 [02:20<00:52,  5.24it/s][A
 72%|███████▏  | 724/1000 [02:20<00:52,  5.29it/s][A
 72%|███████▎  | 725/1000 [02:20<00:51,  5.34it/s][A
 73%|███████▎  | 726/1000 [02:20<00:51,  5.32it/s][A
 73%|███████▎  | 727/1000 [02:20<00:55,  4.96it/s][A
 73%|███████▎  | 728/1000 [02:21<00:53,  5.09it/s][A
 73%|███████▎  | 729/1000 [02:21<00:56,  4.76it/s][A
 73%|███████▎  | 730/1000 [02:21<00:54,  4.95it/s][A
 73%|███████▎  | 731/1000 [02:21<00:53,  5.04it/s][A
 73%|███████▎  | 732/1000 [02:21<00:51,  5.18it/s][A
 73%|███████▎  | 733/1000 [02:21<00:50,  5.26it/s][A
 73%|███████▎  | 734/1000 [02:22<00:48,  5.48it/s][A
 74%|███████▎  | 735/1000 [02:22<00:51,  5.18it/s][A
 74%|███████▎  | 736/1000 [02:22<00:50,  5.27it/s][A
 74%|███████▎  | 737/1000 [02:22<00:48,  5.38it/s][A
 74%|███████▍  | 738/1000 [02:22<00:52,  4.98it/s][A
 74%|███████▍  | 739/1000 [02:23<00:51,  5.10it/s][A
 74%|███████▍  | 740/1000 [02:23<00:49,  5.23it/s][A
 74%|███████▍  | 741/1000 [02:23<00:53,  4.88it/s][A
 74%|███████▍  | 742/1000 [02:23<00:51,  5.03it/s][A
 74%|███████▍  | 743/1000 [02:24<00:55,  4.67it/s][A
 74%|███████▍  | 744/1000 [02:24<00:52,  4.88it/s][A
 74%|███████▍  | 745/1000 [02:24<00:50,  5.02it/s][A
 75%|███████▍  | 746/1000 [02:24<00:49,  5.14it/s][A
 75%|███████▍  | 747/1000 [02:24<00:52,  4.80it/s][A
 75%|███████▍  | 748/1000 [02:25<00:55,  4.56it/s][A
 75%|███████▍  | 749/1000 [02:25<00:56,  4.42it/s][A
 75%|███████▌  | 750/1000 [02:25<00:53,  4.67it/s][A
 75%|███████▌  | 751/1000 [02:25<00:49,  5.05it/s][A
 75%|███████▌  | 752/1000 [02:25<00:47,  5.17it/s][A
 75%|███████▌  | 753/1000 [02:25<00:47,  5.25it/s][A
 75%|███████▌  | 754/1000 [02:26<00:46,  5.29it/s][A
 76%|███████▌  | 755/1000 [02:26<00:48,  5.01it/s][A
 76%|███████▌  | 756/1000 [02:26<00:52,  4.67it/s][A
 76%|███████▌  | 757/1000 [02:26<00:53,  4.58it/s][A
 76%|███████▌  | 758/1000 [02:27<00:52,  4.62it/s][A
 76%|███████▌  | 759/1000 [02:27<00:51,  4.70it/s][A
 76%|███████▌  | 760/1000 [02:27<00:51,  4.62it/s][A
 76%|███████▌  | 761/1000 [02:27<00:49,  4.85it/s][A
 76%|███████▌  | 762/1000 [02:27<00:47,  5.02it/s][A
 76%|███████▋  | 763/1000 [02:28<00:45,  5.18it/s][A
 76%|███████▋  | 764/1000 [02:28<00:44,  5.25it/s][A
 76%|███████▋  | 765/1000 [02:28<00:48,  4.80it/s][A
 77%|███████▋  | 766/1000 [02:28<00:47,  4.93it/s][A
 77%|███████▋  | 767/1000 [02:28<00:46,  5.03it/s][A
 77%|███████▋  | 768/1000 [02:29<00:44,  5.18it/s][A
 77%|███████▋  | 769/1000 [02:29<00:48,  4.81it/s][A
 77%|███████▋  | 770/1000 [02:29<00:50,  4.60it/s][A
 77%|███████▋  | 771/1000 [02:29<00:47,  4.81it/s][A
 77%|███████▋  | 772/1000 [02:29<00:45,  5.01it/s][A
 77%|███████▋  | 773/1000 [02:30<00:46,  4.88it/s][A
 77%|███████▋  | 774/1000 [02:30<00:44,  5.04it/s][A
 78%|███████▊  | 775/1000 [02:30<00:43,  5.12it/s][A
 78%|███████▊  | 776/1000 [02:30<00:46,  4.83it/s][A
 78%|███████▊  | 777/1000 [02:30<00:44,  4.97it/s][A
 78%|███████▊  | 778/1000 [02:31<00:43,  5.12it/s][A
 78%|███████▊  | 779/1000 [02:31<00:41,  5.28it/s][A
 78%|███████▊  | 780/1000 [02:31<00:41,  5.26it/s][A
 78%|███████▊  | 781/1000 [02:31<00:41,  5.32it/s][A
 78%|███████▊  | 782/1000 [02:31<00:40,  5.39it/s][A
 78%|███████▊  | 783/1000 [02:32<00:40,  5.38it/s][A
 78%|███████▊  | 784/1000 [02:32<00:38,  5.56it/s][A
 78%|███████▊  | 785/1000 [02:32<00:38,  5.54it/s][A
 79%|███████▊  | 786/1000 [02:32<00:37,  5.73it/s][A
 79%|███████▊  | 787/1000 [02:32<00:37,  5.65it/s][A
 79%|███████▉  | 788/1000 [02:32<00:40,  5.28it/s][A
 79%|███████▉  | 789/1000 [02:33<00:39,  5.36it/s][A
 79%|███████▉  | 790/1000 [02:33<00:38,  5.40it/s][A
 79%|███████▉  | 791/1000 [02:33<00:38,  5.42it/s][A
 79%|███████▉  | 792/1000 [02:33<00:38,  5.39it/s][A
 79%|███████▉  | 793/1000 [02:33<00:38,  5.38it/s][A
 79%|███████▉  | 794/1000 [02:34<00:38,  5.40it/s][A
 80%|███████▉  | 795/1000 [02:34<00:38,  5.33it/s][A
 80%|███████▉  | 796/1000 [02:34<00:37,  5.38it/s][A
 80%|███████▉  | 797/1000 [02:34<00:37,  5.42it/s][A
 80%|███████▉  | 798/1000 [02:34<00:39,  5.13it/s][A
 80%|███████▉  | 799/1000 [02:34<00:38,  5.23it/s][A
 80%|████████  | 800/1000 [02:35<00:40,  4.94it/s][A
 80%|████████  | 801/1000 [02:35<00:37,  5.29it/s][A
 80%|████████  | 802/1000 [02:35<00:37,  5.30it/s][A
 80%|████████  | 803/1000 [02:35<00:38,  5.08it/s][A
 80%|████████  | 804/1000 [02:35<00:38,  5.15it/s][A
 80%|████████  | 805/1000 [02:36<00:40,  4.77it/s][A
 81%|████████  | 806/1000 [02:36<00:41,  4.66it/s][A
 81%|████████  | 807/1000 [02:36<00:39,  4.89it/s][A
 81%|████████  | 808/1000 [02:36<00:38,  5.03it/s][A
 81%|████████  | 809/1000 [02:37<00:38,  4.94it/s][A
 81%|████████  | 810/1000 [02:37<00:36,  5.23it/s][A
 81%|████████  | 811/1000 [02:37<00:34,  5.48it/s][A
 81%|████████  | 812/1000 [02:37<00:37,  5.02it/s][A
 81%|████████▏ | 813/1000 [02:37<00:39,  4.69it/s][A
 81%|████████▏ | 814/1000 [02:37<00:36,  5.08it/s][A
 82%|████████▏ | 815/1000 [02:38<00:35,  5.21it/s][A
 82%|████████▏ | 816/1000 [02:38<00:35,  5.21it/s][A
 82%|████████▏ | 817/1000 [02:38<00:34,  5.28it/s][A
 82%|████████▏ | 818/1000 [02:38<00:34,  5.26it/s][A
 82%|████████▏ | 819/1000 [02:38<00:33,  5.43it/s][A
 82%|████████▏ | 820/1000 [02:39<00:33,  5.42it/s][A
 82%|████████▏ | 821/1000 [02:39<00:33,  5.37it/s][A
 82%|████████▏ | 822/1000 [02:39<00:33,  5.38it/s][A
 82%|████████▏ | 823/1000 [02:39<00:31,  5.64it/s][A
 82%|████████▏ | 824/1000 [02:39<00:33,  5.25it/s][A
 82%|████████▎ | 825/1000 [02:40<00:33,  5.29it/s][A
 83%|████████▎ | 826/1000 [02:40<00:34,  5.08it/s][A
 83%|████████▎ | 827/1000 [02:40<00:33,  5.20it/s][A
 83%|████████▎ | 828/1000 [02:40<00:32,  5.25it/s][A
 83%|████████▎ | 829/1000 [02:40<00:32,  5.31it/s][A
 83%|████████▎ | 830/1000 [02:40<00:31,  5.38it/s][A
 83%|████████▎ | 831/1000 [02:41<00:31,  5.40it/s][A
 83%|████████▎ | 832/1000 [02:41<00:30,  5.47it/s][A
 83%|████████▎ | 833/1000 [02:41<00:32,  5.19it/s][A
 83%|████████▎ | 834/1000 [02:41<00:31,  5.25it/s][A
 84%|████████▎ | 835/1000 [02:41<00:30,  5.34it/s][A
 84%|████████▎ | 836/1000 [02:42<00:30,  5.37it/s][A
 84%|████████▎ | 837/1000 [02:42<00:30,  5.43it/s][A
 84%|████████▍ | 838/1000 [02:42<00:29,  5.44it/s][A
 84%|████████▍ | 839/1000 [02:42<00:35,  4.58it/s][A
 84%|████████▍ | 840/1000 [02:42<00:35,  4.56it/s][A
 84%|████████▍ | 841/1000 [02:43<00:33,  4.79it/s][A
 84%|████████▍ | 842/1000 [02:43<00:31,  4.99it/s][A
 84%|████████▍ | 843/1000 [02:43<00:30,  5.10it/s][A
 84%|████████▍ | 844/1000 [02:43<00:32,  4.83it/s][A
 84%|████████▍ | 845/1000 [02:44<00:33,  4.62it/s][A
 85%|████████▍ | 846/1000 [02:44<00:31,  4.82it/s][A
 85%|████████▍ | 847/1000 [02:44<00:30,  5.01it/s][A
 85%|████████▍ | 848/1000 [02:44<00:29,  5.18it/s][A
 85%|████████▍ | 849/1000 [02:44<00:31,  4.85it/s][A
 85%|████████▌ | 850/1000 [02:45<00:32,  4.60it/s][A
 85%|████████▌ | 851/1000 [02:45<00:29,  5.00it/s][A
 85%|████████▌ | 852/1000 [02:45<00:28,  5.12it/s][A
 85%|████████▌ | 853/1000 [02:45<00:28,  5.21it/s][A
 85%|████████▌ | 854/1000 [02:45<00:30,  4.86it/s][A
 86%|████████▌ | 855/1000 [02:45<00:28,  5.01it/s][A
 86%|████████▌ | 856/1000 [02:46<00:27,  5.26it/s][A
 86%|████████▌ | 857/1000 [02:46<00:26,  5.31it/s][A
 86%|████████▌ | 858/1000 [02:46<00:26,  5.38it/s][A
 86%|████████▌ | 859/1000 [02:46<00:26,  5.40it/s][A
 86%|████████▌ | 860/1000 [02:46<00:25,  5.45it/s][A
 86%|████████▌ | 861/1000 [02:47<00:25,  5.54it/s][A
 86%|████████▌ | 862/1000 [02:47<00:24,  5.52it/s][A
 86%|████████▋ | 863/1000 [02:47<00:24,  5.54it/s][A
 86%|████████▋ | 864/1000 [02:47<00:24,  5.53it/s][A
 86%|████████▋ | 865/1000 [02:47<00:24,  5.52it/s][A
 87%|████████▋ | 866/1000 [02:47<00:24,  5.43it/s][A
 87%|████████▋ | 867/1000 [02:48<00:26,  4.95it/s][A
 87%|████████▋ | 868/1000 [02:48<00:26,  5.02it/s][A
 87%|████████▋ | 869/1000 [02:48<00:25,  5.13it/s][A
 87%|████████▋ | 870/1000 [02:48<00:26,  4.97it/s][A
 87%|████████▋ | 871/1000 [02:49<00:25,  4.99it/s][A
 87%|████████▋ | 872/1000 [02:49<00:25,  5.12it/s][A
 87%|████████▋ | 873/1000 [02:49<00:24,  5.20it/s][A
 87%|████████▋ | 874/1000 [02:49<00:23,  5.28it/s][A
 88%|████████▊ | 875/1000 [02:49<00:23,  5.39it/s][A
 88%|████████▊ | 876/1000 [02:49<00:22,  5.42it/s][A
 88%|████████▊ | 877/1000 [02:50<00:22,  5.46it/s][A
 88%|████████▊ | 878/1000 [02:50<00:22,  5.52it/s][A
 88%|████████▊ | 879/1000 [02:50<00:22,  5.44it/s][A
 88%|████████▊ | 880/1000 [02:50<00:21,  5.48it/s][A
 88%|████████▊ | 881/1000 [02:50<00:21,  5.55it/s][A
 88%|████████▊ | 882/1000 [02:50<00:21,  5.60it/s][A
 88%|████████▊ | 883/1000 [02:51<00:20,  5.80it/s][A
 88%|████████▊ | 884/1000 [02:51<00:20,  5.72it/s][A
 88%|████████▊ | 885/1000 [02:51<00:20,  5.56it/s][A
 89%|████████▊ | 886/1000 [02:51<00:21,  5.23it/s][A
 89%|████████▊ | 887/1000 [02:51<00:21,  5.22it/s][A
 89%|████████▉ | 888/1000 [02:52<00:21,  5.33it/s][A
 89%|████████▉ | 889/1000 [02:52<00:20,  5.46it/s][A
 89%|████████▉ | 890/1000 [02:52<00:21,  5.11it/s][A
 89%|████████▉ | 891/1000 [02:52<00:22,  4.95it/s][A
 89%|████████▉ | 892/1000 [02:52<00:21,  5.08it/s][A
 89%|████████▉ | 893/1000 [02:53<00:20,  5.22it/s][A
 89%|████████▉ | 894/1000 [02:53<00:21,  4.99it/s][A
 90%|████████▉ | 895/1000 [02:53<00:19,  5.27it/s][A
 90%|████████▉ | 896/1000 [02:53<00:21,  4.92it/s][A
 90%|████████▉ | 897/1000 [02:53<00:20,  5.07it/s][A
 90%|████████▉ | 898/1000 [02:54<00:19,  5.19it/s][A
 90%|████████▉ | 899/1000 [02:54<00:18,  5.35it/s][A
 90%|█████████ | 900/1000 [02:54<00:18,  5.31it/s][A
 90%|█████████ | 901/1000 [02:54<00:18,  5.35it/s][A
 90%|█████████ | 902/1000 [02:54<00:20,  4.86it/s][A
 90%|█████████ | 903/1000 [02:55<00:20,  4.77it/s][A
 90%|█████████ | 904/1000 [02:55<00:19,  4.94it/s][A
 90%|█████████ | 905/1000 [02:55<00:18,  5.24it/s][A
 91%|█████████ | 906/1000 [02:55<00:19,  4.94it/s][A
 91%|█████████ | 907/1000 [02:55<00:18,  5.03it/s][A
 91%|█████████ | 908/1000 [02:56<00:17,  5.13it/s][A
 91%|█████████ | 909/1000 [02:56<00:17,  5.06it/s][A
 91%|█████████ | 910/1000 [02:56<00:18,  5.00it/s][A
 91%|█████████ | 911/1000 [02:56<00:16,  5.40it/s][A
 91%|█████████ | 912/1000 [02:56<00:14,  5.90it/s][A
 91%|█████████▏| 913/1000 [02:56<00:13,  6.31it/s][A
 91%|█████████▏| 914/1000 [02:57<00:15,  5.54it/s][A
 92%|█████████▏| 915/1000 [02:57<00:14,  6.04it/s][A
 92%|█████████▏| 916/1000 [02:57<00:14,  5.66it/s][A
 92%|█████████▏| 917/1000 [02:57<00:13,  6.08it/s][A
 92%|█████████▏| 918/1000 [02:57<00:14,  5.69it/s][A
 92%|█████████▏| 919/1000 [02:57<00:14,  5.77it/s][A
 92%|█████████▏| 920/1000 [02:58<00:13,  6.04it/s][A
 92%|█████████▏| 921/1000 [02:58<00:12,  6.42it/s][A
 92%|█████████▏| 922/1000 [02:58<00:13,  5.91it/s][A
 92%|█████████▏| 923/1000 [02:58<00:12,  5.96it/s][A
 92%|█████████▏| 924/1000 [02:58<00:11,  6.40it/s][A
 92%|█████████▎| 925/1000 [02:58<00:12,  6.14it/s][A
 93%|█████████▎| 926/1000 [02:59<00:12,  5.70it/s][A
 93%|█████████▎| 927/1000 [02:59<00:11,  6.09it/s][A
 93%|█████████▎| 928/1000 [02:59<00:11,  6.53it/s][A
 93%|█████████▎| 929/1000 [02:59<00:12,  5.54it/s][A
 93%|█████████▎| 930/1000 [02:59<00:11,  5.95it/s][A
 93%|█████████▎| 931/1000 [02:59<00:12,  5.36it/s][A
 93%|█████████▎| 932/1000 [03:00<00:13,  4.97it/s][A
 93%|█████████▎| 933/1000 [03:00<00:12,  5.56it/s][A
 93%|█████████▎| 934/1000 [03:00<00:10,  6.12it/s][A
 94%|█████████▎| 935/1000 [03:00<00:09,  6.60it/s][A
 94%|█████████▎| 936/1000 [03:00<00:09,  6.98it/s][A
 94%|█████████▎| 937/1000 [03:00<00:09,  6.49it/s][A
 94%|█████████▍| 938/1000 [03:01<00:09,  6.56it/s][A
 94%|█████████▍| 939/1000 [03:01<00:08,  6.92it/s][A
 94%|█████████▍| 940/1000 [03:01<00:08,  6.90it/s][A
 94%|█████████▍| 941/1000 [03:01<00:09,  6.46it/s][A
 94%|█████████▍| 942/1000 [03:01<00:09,  6.01it/s][A
 94%|█████████▍| 943/1000 [03:01<00:09,  5.98it/s][A
 94%|█████████▍| 944/1000 [03:02<00:09,  5.85it/s][A
 94%|█████████▍| 945/1000 [03:02<00:09,  5.70it/s][A
 95%|█████████▍| 946/1000 [03:02<00:10,  5.35it/s][A
 95%|█████████▍| 947/1000 [03:02<00:09,  5.42it/s][A
 95%|█████████▍| 948/1000 [03:02<00:09,  5.49it/s][A
 95%|█████████▍| 949/1000 [03:02<00:09,  5.52it/s][A
 95%|█████████▌| 950/1000 [03:03<00:09,  5.48it/s][A
 95%|█████████▌| 951/1000 [03:03<00:09,  5.19it/s][A
 95%|█████████▌| 952/1000 [03:03<00:09,  5.27it/s][A
 95%|█████████▌| 953/1000 [03:03<00:09,  4.90it/s][A
 95%|█████████▌| 954/1000 [03:04<00:10,  4.58it/s][A
 96%|█████████▌| 955/1000 [03:04<00:10,  4.42it/s][A
 96%|█████████▌| 956/1000 [03:04<00:09,  4.68it/s][A
 96%|█████████▌| 957/1000 [03:04<00:08,  4.90it/s][A
 96%|█████████▌| 958/1000 [03:04<00:09,  4.64it/s][A
 96%|█████████▌| 959/1000 [03:05<00:09,  4.48it/s][A
 96%|█████████▌| 960/1000 [03:05<00:08,  4.60it/s][A
 96%|█████████▌| 961/1000 [03:05<00:08,  4.82it/s][A
 96%|█████████▌| 962/1000 [03:05<00:07,  5.00it/s][A
 96%|█████████▋| 963/1000 [03:05<00:07,  5.15it/s][A
 96%|█████████▋| 964/1000 [03:06<00:06,  5.25it/s][A
 96%|█████████▋| 965/1000 [03:06<00:06,  5.34it/s][A
 97%|█████████▋| 966/1000 [03:06<00:06,  5.30it/s][A
 97%|█████████▋| 967/1000 [03:06<00:05,  5.52it/s][A
 97%|█████████▋| 968/1000 [03:06<00:05,  5.51it/s][A
 97%|█████████▋| 969/1000 [03:06<00:05,  5.48it/s][A
 97%|█████████▋| 970/1000 [03:07<00:05,  5.02it/s][A
 97%|█████████▋| 971/1000 [03:07<00:05,  5.13it/s][A
 97%|█████████▋| 972/1000 [03:07<00:05,  5.22it/s][A
 97%|█████████▋| 973/1000 [03:07<00:05,  5.28it/s][A
 97%|█████████▋| 974/1000 [03:07<00:04,  5.27it/s][A
 98%|█████████▊| 975/1000 [03:08<00:04,  5.33it/s][A
 98%|█████████▊| 976/1000 [03:08<00:04,  5.11it/s][A
 98%|█████████▊| 977/1000 [03:08<00:04,  5.11it/s][A
 98%|█████████▊| 978/1000 [03:08<00:04,  4.75it/s][A
 98%|█████████▊| 979/1000 [03:08<00:04,  4.81it/s][A
 98%|█████████▊| 980/1000 [03:09<00:04,  4.93it/s][A
 98%|█████████▊| 981/1000 [03:09<00:03,  5.00it/s][A
 98%|█████████▊| 982/1000 [03:09<00:03,  5.11it/s][A
 98%|█████████▊| 983/1000 [03:09<00:03,  5.19it/s][A
 98%|█████████▊| 984/1000 [03:09<00:03,  5.24it/s][A
 98%|█████████▊| 985/1000 [03:10<00:02,  5.44it/s][A
 99%|█████████▊| 986/1000 [03:10<00:02,  5.56it/s][A
 99%|█████████▊| 987/1000 [03:10<00:02,  5.55it/s][A
 99%|█████████▉| 988/1000 [03:10<00:02,  5.50it/s][A
 99%|█████████▉| 989/1000 [03:10<00:01,  5.51it/s][A
 99%|█████████▉| 990/1000 [03:11<00:01,  5.51it/s][A
 99%|█████████▉| 991/1000 [03:11<00:01,  5.05it/s][A
 99%|█████████▉| 992/1000 [03:11<00:01,  5.17it/s][A
 99%|█████████▉| 993/1000 [03:11<00:01,  5.23it/s][A
 99%|█████████▉| 994/1000 [03:11<00:01,  5.43it/s][A
100%|█████████▉| 995/1000 [03:11<00:00,  5.68it/s][A
100%|█████████▉| 996/1000 [03:12<00:00,  5.71it/s][A
100%|█████████▉| 997/1000 [03:12<00:00,  5.87it/s][A
100%|█████████▉| 998/1000 [03:12<00:00,  5.79it/s][A
100%|█████████▉| 999/1000 [03:12<00:00,  5.21it/s][A
100%|██████████| 1000/1000 [03:12<00:00,  5.02it/s][A                                                       
                                                   [A 10%|▉         | 187/1875 [1:38:49<10:46:31, 22.98s/it]
100%|██████████| 1000/1000 [03:13<00:00,  5.02it/s][A
                                                   [Atorch.Size([1, 162])
torch.Size([1, 42])
torch.Size([1, 324])
torch.Size([1, 37])
torch.Size([1, 109])
torch.Size([1, 159])
torch.Size([1, 42])
torch.Size([1, 33])
torch.Size([1, 62])
torch.Size([1, 23])
torch.Size([1, 303])
torch.Size([1, 54])
torch.Size([1, 66])
torch.Size([1, 467])
torch.Size([1, 26])
torch.Size([1, 33])
torch.Size([1, 40])
torch.Size([1, 528])
torch.Size([1, 372])
torch.Size([1, 196])
torch.Size([1, 64])
torch.Size([1, 24])
torch.Size([1, 22])
torch.Size([1, 37])
torch.Size([1, 26])
torch.Size([1, 251])
torch.Size([1, 355])
torch.Size([1, 22])
torch.Size([1, 244])
torch.Size([1, 35])
torch.Size([1, 528])
torch.Size([1, 328])
torch.Size([1, 109])
torch.Size([1, 170])
torch.Size([1, 29])
torch.Size([1, 187])
torch.Size([1, 241])
torch.Size([1, 397])
torch.Size([1, 524])
torch.Size([1, 23])
torch.Size([1, 55])
torch.Size([1, 195])
torch.Size([1, 24])
torch.Size([1, 76])
torch.Size([1, 30])
torch.Size([1, 56])
torch.Size([1, 454])
torch.Size([1, 23])
torch.Size([1, 100])
torch.Size([1, 25])
torch.Size([1, 486])
torch.Size([1, 310])
torch.Size([1, 381])
torch.Size([1, 47])
torch.Size([1, 27])
torch.Size([1, 40])
torch.Size([1, 281])
torch.Size([1, 166])
torch.Size([1, 528])
torch.Size([1, 72])
torch.Size([1, 370])
torch.Size([1, 39])
torch.Size([1, 23])
torch.Size([1, 322])
torch.Size([1, 235])
torch.Size([1, 379])
torch.Size([1, 133])
torch.Size([1, 187])
torch.Size([1, 255])
torch.Size([1, 27])
torch.Size([1, 27])
torch.Size([1, 389])
torch.Size([1, 91])
torch.Size([1, 333])
torch.Size([1, 303])
torch.Size([1, 283])
torch.Size([1, 34])
torch.Size([1, 34])
torch.Size([1, 33])
torch.Size([1, 85])
torch.Size([1, 56])
torch.Size([1, 246])
torch.Size([1, 525])
torch.Size([1, 26])
torch.Size([1, 40])
torch.Size([1, 361])
torch.Size([1, 41])
torch.Size([1, 473])
torch.Size([1, 58])
torch.Size([1, 467])
torch.Size([1, 131])
torch.Size([1, 82])
torch.Size([1, 152])
torch.Size([1, 225])
torch.Size([1, 76])
torch.Size([1, 37])
torch.Size([1, 60])
torch.Size([1, 98])
torch.Size([1, 254])
torch.Size([1, 412])
torch.Size([1, 29])
torch.Size([1, 24])
torch.Size([1, 32])
torch.Size([1, 44])
torch.Size([1, 321])
torch.Size([1, 154])
torch.Size([1, 333])
torch.Size([1, 41])
torch.Size([1, 61])
torch.Size([1, 151])
torch.Size([1, 36])
torch.Size([1, 371])
torch.Size([1, 268])
torch.Size([1, 261])
torch.Size([1, 226])
torch.Size([1, 227])
torch.Size([1, 362])
torch.Size([1, 141])
torch.Size([1, 71])
torch.Size([1, 164])
torch.Size([1, 36])
torch.Size([1, 60])
torch.Size([1, 422])
torch.Size([1, 325])
torch.Size([1, 330])
torch.Size([1, 183])
torch.Size([1, 53])
torch.Size([1, 361])
torch.Size([1, 245])
torch.Size([1, 249])
torch.Size([1, 34])
torch.Size([1, 98])
torch.Size([1, 250])
torch.Size([1, 119])
torch.Size([1, 251])
torch.Size([1, 29])
torch.Size([1, 343])
torch.Size([1, 26])
torch.Size([1, 34])
torch.Size([1, 45])
torch.Size([1, 316])
torch.Size([1, 411])
torch.Size([1, 304])
torch.Size([1, 21])
torch.Size([1, 45])
torch.Size([1, 295])
torch.Size([1, 458])
torch.Size([1, 57])
torch.Size([1, 57])
torch.Size([1, 88])
torch.Size([1, 229])
torch.Size([1, 39])
torch.Size([1, 195])
torch.Size([1, 457])
torch.Size([1, 51])
torch.Size([1, 96])
torch.Size([1, 30])
torch.Size([1, 92])
torch.Size([1, 201])
torch.Size([1, 37])
torch.Size([1, 79])
torch.Size([1, 46])
torch.Size([1, 347])
torch.Size([1, 135])
torch.Size([1, 528])
torch.Size([1, 191])
torch.Size([1, 112])
torch.Size([1, 92])
torch.Size([1, 97])
torch.Size([1, 194])
torch.Size([1, 26])
torch.Size([1, 69])
torch.Size([1, 413])
torch.Size([1, 178])
torch.Size([1, 298])
torch.Size([1, 32])
torch.Size([1, 28])
torch.Size([1, 130])
torch.Size([1, 118])
torch.Size([1, 31])
torch.Size([1, 28])
torch.Size([1, 334])
torch.Size([1, 54])
torch.Size([1, 174])
torch.Size([1, 448])
torch.Size([1, 160])
torch.Size([1, 135])
torch.Size([1, 72])
torch.Size([1, 56])
torch.Size([1, 19])
torch.Size([1, 293])
torch.Size([1, 56])
torch.Size([1, 143])
torch.Size([1, 97])
torch.Size([1, 83])
torch.Size([1, 297])
torch.Size([1, 343])
torch.Size([1, 206])
torch.Size([1, 216])
torch.Size([1, 209])
torch.Size([1, 167])
torch.Size([1, 298])
torch.Size([1, 432])
torch.Size([1, 31])
torch.Size([1, 417])
torch.Size([1, 240])
torch.Size([1, 40])
torch.Size([1, 326])
torch.Size([1, 57])
torch.Size([1, 125])
torch.Size([1, 26])
torch.Size([1, 526])
torch.Size([1, 35])
torch.Size([1, 34])
torch.Size([1, 464])
torch.Size([1, 430])
torch.Size([1, 85])
torch.Size([1, 22])
torch.Size([1, 244])
torch.Size([1, 424])
torch.Size([1, 71])
torch.Size([1, 48])
torch.Size([1, 23])
torch.Size([1, 116])
torch.Size([1, 38])
torch.Size([1, 73])
torch.Size([1, 286])
torch.Size([1, 275])
torch.Size([1, 336])
torch.Size([1, 48])
torch.Size([1, 100])
torch.Size([1, 415])
torch.Size([1, 91])
torch.Size([1, 58])
torch.Size([1, 390])
torch.Size([1, 34])
torch.Size([1, 468])
torch.Size([1, 85])
torch.Size([1, 528])
torch.Size([1, 51])
torch.Size([1, 277])
torch.Size([1, 27])
torch.Size([1, 104])
torch.Size([1, 97])
torch.Size([1, 287])
torch.Size([1, 61])
torch.Size([1, 37])
torch.Size([1, 76])
torch.Size([1, 45])
torch.Size([1, 431])
torch.Size([1, 48])
torch.Size([1, 60])
torch.Size([1, 25])
torch.Size([1, 117])
torch.Size([1, 43])
torch.Size([1, 147])
torch.Size([1, 167])
torch.Size([1, 26])
torch.Size([1, 42])
torch.Size([1, 328])
torch.Size([1, 528])
torch.Size([1, 381])
torch.Size([1, 213])
torch.Size([1, 171])
torch.Size([1, 183])
torch.Size([1, 117])
torch.Size([1, 59])
torch.Size([1, 164])
torch.Size([1, 18])
torch.Size([1, 209])
torch.Size([1, 384])
torch.Size([1, 297])
torch.Size([1, 212])
torch.Size([1, 31])
torch.Size([1, 18])
torch.Size([1, 145])
torch.Size([1, 163])
torch.Size([1, 30])
torch.Size([1, 528])
torch.Size([1, 28])
torch.Size([1, 503])
torch.Size([1, 528])
torch.Size([1, 366])
torch.Size([1, 262])
torch.Size([1, 24])
torch.Size([1, 35])
torch.Size([1, 347])
torch.Size([1, 273])
torch.Size([1, 524])
torch.Size([1, 125])
torch.Size([1, 76])
torch.Size([1, 30])
torch.Size([1, 367])
torch.Size([1, 225])
torch.Size([1, 33])
torch.Size([1, 34])
torch.Size([1, 164])
torch.Size([1, 20])
torch.Size([1, 435])
torch.Size([1, 113])
torch.Size([1, 441])
torch.Size([1, 228])
torch.Size([1, 31])
torch.Size([1, 33])
torch.Size([1, 47])
torch.Size([1, 409])
torch.Size([1, 29])
torch.Size([1, 94])
torch.Size([1, 91])
torch.Size([1, 382])
torch.Size([1, 176])
torch.Size([1, 456])
torch.Size([1, 116])
torch.Size([1, 286])
torch.Size([1, 76])
torch.Size([1, 344])
torch.Size([1, 283])
torch.Size([1, 244])
torch.Size([1, 83])
torch.Size([1, 476])
torch.Size([1, 96])
torch.Size([1, 483])
torch.Size([1, 70])
torch.Size([1, 509])
torch.Size([1, 32])
torch.Size([1, 63])
torch.Size([1, 81])
torch.Size([1, 143])
torch.Size([1, 443])
torch.Size([1, 319])
torch.Size([1, 55])
torch.Size([1, 373])
torch.Size([1, 29])
torch.Size([1, 24])
torch.Size([1, 29])
torch.Size([1, 19])
torch.Size([1, 37])
torch.Size([1, 327])
torch.Size([1, 394])
torch.Size([1, 48])
torch.Size([1, 493])
torch.Size([1, 37])
torch.Size([1, 482])
torch.Size([1, 96])
torch.Size([1, 139])
torch.Size([1, 99])
torch.Size([1, 157])
torch.Size([1, 47])
torch.Size([1, 155])
torch.Size([1, 120])
torch.Size([1, 192])
torch.Size([1, 528])
torch.Size([1, 47])
torch.Size([1, 289])
torch.Size([1, 403])
torch.Size([1, 528])
torch.Size([1, 43])
torch.Size([1, 219])
torch.Size([1, 44])
torch.Size([1, 34])
torch.Size([1, 116])
torch.Size([1, 51])
torch.Size([1, 401])
torch.Size([1, 20])
torch.Size([1, 37])
torch.Size([1, 52])
torch.Size([1, 421])
torch.Size([1, 528])
torch.Size([1, 292])
torch.Size([1, 90])
torch.Size([1, 115])
torch.Size([1, 233])
torch.Size([1, 65])
torch.Size([1, 44])
torch.Size([1, 521])
torch.Size([1, 35])
torch.Size([1, 256])
torch.Size([1, 500])
torch.Size([1, 315])
torch.Size([1, 113])
torch.Size([1, 164])
torch.Size([1, 33])
torch.Size([1, 159])
torch.Size([1, 138])
torch.Size([1, 45])
torch.Size([1, 55])
torch.Size([1, 44])
torch.Size([1, 32])
torch.Size([1, 148])
torch.Size([1, 170])
torch.Size([1, 49])
torch.Size([1, 47])
torch.Size([1, 372])
torch.Size([1, 157])
torch.Size([1, 216])
torch.Size([1, 99])
torch.Size([1, 89])
torch.Size([1, 488])
torch.Size([1, 232])
torch.Size([1, 446])
torch.Size([1, 48])
torch.Size([1, 391])
torch.Size([1, 295])
torch.Size([1, 29])
torch.Size([1, 130])
torch.Size([1, 336])
torch.Size([1, 283])
torch.Size([1, 279])
torch.Size([1, 215])
torch.Size([1, 199])
torch.Size([1, 358])
torch.Size([1, 200])
torch.Size([1, 395])
torch.Size([1, 37])
torch.Size([1, 431])
torch.Size([1, 292])
torch.Size([1, 79])
torch.Size([1, 395])
torch.Size([1, 185])
torch.Size([1, 30])
torch.Size([1, 291])
torch.Size([1, 243])
torch.Size([1, 38])
torch.Size([1, 170])
torch.Size([1, 313])
torch.Size([1, 122])
torch.Size([1, 42])
torch.Size([1, 29])
torch.Size([1, 525])
torch.Size([1, 36])
torch.Size([1, 27])
torch.Size([1, 102])
torch.Size([1, 311])
torch.Size([1, 380])
torch.Size([1, 39])
torch.Size([1, 212])
torch.Size([1, 321])
torch.Size([1, 69])
torch.Size([1, 428])
torch.Size([1, 37])
torch.Size([1, 30])
torch.Size([1, 341])
torch.Size([1, 528])
torch.Size([1, 31])
torch.Size([1, 33])
torch.Size([1, 96])
torch.Size([1, 327])
torch.Size([1, 207])
torch.Size([1, 64])
torch.Size([1, 212])
torch.Size([1, 87])
torch.Size([1, 28])
torch.Size([1, 46])
torch.Size([1, 114])
torch.Size([1, 173])
torch.Size([1, 420])
torch.Size([1, 267])
torch.Size([1, 61])
torch.Size([1, 154])
torch.Size([1, 226])
torch.Size([1, 59])
torch.Size([1, 47])
torch.Size([1, 34])
torch.Size([1, 158])
torch.Size([1, 180])
torch.Size([1, 69])
torch.Size([1, 53])
torch.Size([1, 38])
torch.Size([1, 288])
torch.Size([1, 404])
torch.Size([1, 26])
torch.Size([1, 101])
torch.Size([1, 52])
torch.Size([1, 238])
torch.Size([1, 416])
torch.Size([1, 227])
torch.Size([1, 23])
torch.Size([1, 90])
torch.Size([1, 221])
torch.Size([1, 461])
torch.Size([1, 45])
torch.Size([1, 261])
torch.Size([1, 265])
torch.Size([1, 61])
torch.Size([1, 25])
torch.Size([1, 190])
torch.Size([1, 188])
torch.Size([1, 218])
torch.Size([1, 22])
torch.Size([1, 33])
torch.Size([1, 31])
torch.Size([1, 357])
torch.Size([1, 29])
torch.Size([1, 27])
torch.Size([1, 230])
torch.Size([1, 242])
torch.Size([1, 29])
torch.Size([1, 28])
torch.Size([1, 376])
torch.Size([1, 88])
torch.Size([1, 66])
torch.Size([1, 154])
torch.Size([1, 307])
torch.Size([1, 528])
torch.Size([1, 124])
torch.Size([1, 189])
torch.Size([1, 23])
torch.Size([1, 33])
torch.Size([1, 81])
torch.Size([1, 203])
torch.Size([1, 427])
torch.Size([1, 308])
torch.Size([1, 485])
torch.Size([1, 108])
torch.Size([1, 23])
torch.Size([1, 29])
torch.Size([1, 70])
torch.Size([1, 62])
torch.Size([1, 45])
torch.Size([1, 275])
torch.Size([1, 49])
torch.Size([1, 516])
torch.Size([1, 40])
torch.Size([1, 125])
torch.Size([1, 528])
torch.Size([1, 274])
torch.Size([1, 40])
torch.Size([1, 26])
torch.Size([1, 442])
torch.Size([1, 24])
torch.Size([1, 48])
torch.Size([1, 214])
torch.Size([1, 124])
torch.Size([1, 446])
torch.Size([1, 270])
torch.Size([1, 25])
torch.Size([1, 350])
torch.Size([1, 132])
torch.Size([1, 33])
torch.Size([1, 35])
torch.Size([1, 63])
torch.Size([1, 176])
torch.Size([1, 36])
torch.Size([1, 274])
torch.Size([1, 456])
torch.Size([1, 491])
torch.Size([1, 30])
torch.Size([1, 47])
torch.Size([1, 51])
torch.Size([1, 74])
torch.Size([1, 316])
torch.Size([1, 27])
torch.Size([1, 186])
torch.Size([1, 244])
torch.Size([1, 113])
torch.Size([1, 29])
torch.Size([1, 37])
torch.Size([1, 59])
torch.Size([1, 27])
torch.Size([1, 462])
torch.Size([1, 35])
torch.Size([1, 246])
torch.Size([1, 20])
torch.Size([1, 165])
torch.Size([1, 230])
torch.Size([1, 43])
torch.Size([1, 26])
torch.Size([1, 23])
torch.Size([1, 311])
torch.Size([1, 31])
torch.Size([1, 27])
torch.Size([1, 25])
torch.Size([1, 40])
torch.Size([1, 320])
torch.Size([1, 180])
torch.Size([1, 400])
torch.Size([1, 162])
torch.Size([1, 59])
torch.Size([1, 138])
torch.Size([1, 528])
torch.Size([1, 126])
torch.Size([1, 26])
torch.Size([1, 528])
torch.Size([1, 443])
torch.Size([1, 298])
torch.Size([1, 163])
torch.Size([1, 27])
torch.Size([1, 29])
torch.Size([1, 431])
torch.Size([1, 218])
torch.Size([1, 449])
torch.Size([1, 57])
torch.Size([1, 528])
torch.Size([1, 264])
torch.Size([1, 52])
torch.Size([1, 411])
torch.Size([1, 62])
torch.Size([1, 329])
torch.Size([1, 197])
torch.Size([1, 69])
torch.Size([1, 307])
torch.Size([1, 43])
torch.Size([1, 399])
torch.Size([1, 264])
torch.Size([1, 150])
torch.Size([1, 422])
torch.Size([1, 78])
torch.Size([1, 52])
torch.Size([1, 63])
torch.Size([1, 151])
torch.Size([1, 41])
torch.Size([1, 103])
torch.Size([1, 297])
torch.Size([1, 425])
torch.Size([1, 27])
torch.Size([1, 46])
torch.Size([1, 29])
torch.Size([1, 466])
torch.Size([1, 73])
torch.Size([1, 221])
torch.Size([1, 60])
torch.Size([1, 221])
torch.Size([1, 102])
torch.Size([1, 217])
torch.Size([1, 114])
torch.Size([1, 20])
torch.Size([1, 291])
torch.Size([1, 90])
torch.Size([1, 157])
torch.Size([1, 50])
torch.Size([1, 73])
torch.Size([1, 82])
torch.Size([1, 229])
torch.Size([1, 24])
torch.Size([1, 268])
torch.Size([1, 91])
torch.Size([1, 26])
torch.Size([1, 307])
torch.Size([1, 292])
torch.Size([1, 322])
torch.Size([1, 34])
torch.Size([1, 528])
torch.Size([1, 158])
torch.Size([1, 238])
torch.Size([1, 438])
torch.Size([1, 33])
torch.Size([1, 365])
torch.Size([1, 37])
torch.Size([1, 191])
torch.Size([1, 56])
torch.Size([1, 28])
torch.Size([1, 263])
torch.Size([1, 27])
torch.Size([1, 26])
torch.Size([1, 120])
torch.Size([1, 528])
torch.Size([1, 50])
torch.Size([1, 26])
torch.Size([1, 56])
torch.Size([1, 98])
torch.Size([1, 27])
torch.Size([1, 336])
torch.Size([1, 30])
torch.Size([1, 333])
torch.Size([1, 528])
torch.Size([1, 353])
torch.Size([1, 41])
torch.Size([1, 491])
torch.Size([1, 24])
torch.Size([1, 167])
torch.Size([1, 73])
torch.Size([1, 217])
torch.Size([1, 45])
torch.Size([1, 229])
torch.Size([1, 21])
torch.Size([1, 323])
torch.Size([1, 177])
torch.Size([1, 28])
torch.Size([1, 43])
torch.Size([1, 265])
torch.Size([1, 376])
torch.Size([1, 454])
torch.Size([1, 178])
torch.Size([1, 96])
torch.Size([1, 35])
torch.Size([1, 39])
torch.Size([1, 22])
torch.Size([1, 28])
torch.Size([1, 282])
torch.Size([1, 112])
torch.Size([1, 382])
torch.Size([1, 381])
torch.Size([1, 36])
torch.Size([1, 31])
torch.Size([1, 70])
torch.Size([1, 332])
torch.Size([1, 153])
torch.Size([1, 268])
torch.Size([1, 144])
torch.Size([1, 468])
torch.Size([1, 187])
torch.Size([1, 528])
torch.Size([1, 355])
torch.Size([1, 528])
torch.Size([1, 233])
torch.Size([1, 30])
torch.Size([1, 38])
torch.Size([1, 158])
torch.Size([1, 375])
torch.Size([1, 31])
torch.Size([1, 122])
torch.Size([1, 273])
torch.Size([1, 120])
torch.Size([1, 117])
torch.Size([1, 48])
torch.Size([1, 32])
torch.Size([1, 27])
torch.Size([1, 366])
torch.Size([1, 74])
torch.Size([1, 381])
torch.Size([1, 24])
torch.Size([1, 41])
torch.Size([1, 47])
torch.Size([1, 28])
torch.Size([1, 285])
torch.Size([1, 395])
torch.Size([1, 31])
torch.Size([1, 488])
torch.Size([1, 33])
torch.Size([1, 285])
torch.Size([1, 198])
torch.Size([1, 23])
torch.Size([1, 39])
torch.Size([1, 339])
torch.Size([1, 102])
torch.Size([1, 31])
torch.Size([1, 402])
torch.Size([1, 216])
torch.Size([1, 53])
torch.Size([1, 395])
torch.Size([1, 83])
torch.Size([1, 521])
torch.Size([1, 60])
torch.Size([1, 246])
torch.Size([1, 237])
torch.Size([1, 411])
torch.Size([1, 462])
torch.Size([1, 430])
torch.Size([1, 27])
torch.Size([1, 92])
torch.Size([1, 198])
torch.Size([1, 28])
torch.Size([1, 75])
torch.Size([1, 47])
torch.Size([1, 401])
torch.Size([1, 260])
torch.Size([1, 119])
torch.Size([1, 25])
torch.Size([1, 22])
torch.Size([1, 108])
torch.Size([1, 151])
torch.Size([1, 54])
torch.Size([1, 35])
torch.Size([1, 528])
torch.Size([1, 254])
torch.Size([1, 269])
torch.Size([1, 107])
torch.Size([1, 469])
torch.Size([1, 393])
torch.Size([1, 164])
torch.Size([1, 29])
torch.Size([1, 347])
torch.Size([1, 37])
torch.Size([1, 316])
torch.Size([1, 385])
torch.Size([1, 42])
torch.Size([1, 166])
torch.Size([1, 20])
torch.Size([1, 29])
torch.Size([1, 221])
torch.Size([1, 34])
torch.Size([1, 268])
torch.Size([1, 68])
torch.Size([1, 140])
torch.Size([1, 36])
torch.Size([1, 38])
torch.Size([1, 347])
torch.Size([1, 127])
torch.Size([1, 64])
torch.Size([1, 52])
torch.Size([1, 263])
torch.Size([1, 19])
torch.Size([1, 218])
torch.Size([1, 99])
torch.Size([1, 27])
torch.Size([1, 46])
torch.Size([1, 336])
torch.Size([1, 54])
torch.Size([1, 99])
torch.Size([1, 88])
torch.Size([1, 108])
torch.Size([1, 337])
torch.Size([1, 149])
torch.Size([1, 506])
torch.Size([1, 273])
torch.Size([1, 39])
torch.Size([1, 47])
torch.Size([1, 369])
torch.Size([1, 42])
torch.Size([1, 97])
torch.Size([1, 414])
torch.Size([1, 431])
torch.Size([1, 89])
torch.Size([1, 50])
torch.Size([1, 299])
torch.Size([1, 121])
torch.Size([1, 65])
torch.Size([1, 69])
torch.Size([1, 31])
torch.Size([1, 305])
torch.Size([1, 83])
torch.Size([1, 22])
torch.Size([1, 373])
torch.Size([1, 143])
torch.Size([1, 353])
torch.Size([1, 119])
torch.Size([1, 41])
torch.Size([1, 40])
torch.Size([1, 99])
torch.Size([1, 81])
torch.Size([1, 33])
torch.Size([1, 382])
torch.Size([1, 73])
torch.Size([1, 36])
torch.Size([1, 216])
torch.Size([1, 121])
torch.Size([1, 37])
torch.Size([1, 528])
torch.Size([1, 357])
torch.Size([1, 228])
torch.Size([1, 34])
torch.Size([1, 319])
torch.Size([1, 199])
torch.Size([1, 395])
torch.Size([1, 147])
torch.Size([1, 69])
torch.Size([1, 27])
torch.Size([1, 415])
torch.Size([1, 427])
torch.Size([1, 30])
torch.Size([1, 89])
torch.Size([1, 106])
torch.Size([1, 404])
torch.Size([1, 50])
torch.Size([1, 254])
torch.Size([1, 242])
torch.Size([1, 54])
torch.Size([1, 72])
torch.Size([1, 26])
torch.Size([1, 67])
torch.Size([1, 30])
torch.Size([1, 100])
torch.Size([1, 195])
torch.Size([1, 26])
torch.Size([1, 34])
torch.Size([1, 485])
torch.Size([1, 273])
torch.Size([1, 48])
torch.Size([1, 342])
torch.Size([1, 220])
torch.Size([1, 40])
torch.Size([1, 23])
torch.Size([1, 224])
torch.Size([1, 135])
torch.Size([1, 23])
torch.Size([1, 74])
torch.Size([1, 168])
torch.Size([1, 292])
torch.Size([1, 31])
torch.Size([1, 26])
torch.Size([1, 217])
torch.Size([1, 135])
torch.Size([1, 222])
torch.Size([1, 315])
torch.Size([1, 332])
torch.Size([1, 290])
torch.Size([1, 64])
torch.Size([1, 44])
torch.Size([1, 196])
torch.Size([1, 347])
torch.Size([1, 28])
torch.Size([1, 34])
torch.Size([1, 87])
torch.Size([1, 30])
torch.Size([1, 440])
torch.Size([1, 94])
torch.Size([1, 105])
torch.Size([1, 208])
torch.Size([1, 309])
torch.Size([1, 108])
torch.Size([1, 528])
torch.Size([1, 357])
torch.Size([1, 242])
torch.Size([1, 27])
torch.Size([1, 108])
torch.Size([1, 306])
torch.Size([1, 113])
torch.Size([1, 331])
torch.Size([1, 358])
torch.Size([1, 175])
torch.Size([1, 90])
torch.Size([1, 68])
torch.Size([1, 456])
torch.Size([1, 53])
torch.Size([1, 330])
torch.Size([1, 69])
torch.Size([1, 328])
torch.Size([1, 220])
torch.Size([1, 166])
torch.Size([1, 87])
torch.Size([1, 374])
torch.Size([1, 256])
torch.Size([1, 29])
torch.Size([1, 279])
torch.Size([1, 375])
torch.Size([1, 26])
torch.Size([1, 45])
torch.Size([1, 528])
torch.Size([1, 111])
torch.Size([1, 437])
torch.Size([1, 446])
torch.Size([1, 38])
torch.Size([1, 26])
torch.Size([1, 33])
torch.Size([1, 20])
torch.Size([1, 303])
torch.Size([1, 160])
torch.Size([1, 30])
torch.Size([1, 151])
torch.Size([1, 219])
torch.Size([1, 318])
torch.Size([1, 68])
torch.Size([1, 51])
torch.Size([1, 35])
torch.Size([1, 339])
torch.Size([1, 140])
torch.Size([1, 128])
torch.Size([1, 44])
torch.Size([1, 96])
torch.Size([1, 348])
torch.Size([1, 89])
torch.Size([1, 431])
torch.Size([1, 513])
torch.Size([1, 404])
torch.Size([1, 42])
torch.Size([1, 159])
torch.Size([1, 444])
torch.Size([1, 422])
torch.Size([1, 25])
torch.Size([1, 221])
torch.Size([1, 32])
torch.Size([1, 178])
torch.Size([1, 50])
torch.Size([1, 203])
torch.Size([1, 75])
torch.Size([1, 158])
torch.Size([1, 217])
torch.Size([1, 56])
torch.Size([1, 389])
torch.Size([1, 189])
torch.Size([1, 108])
torch.Size([1, 166])
torch.Size([1, 310])
torch.Size([1, 38])
torch.Size([1, 356])
torch.Size([1, 274])
torch.Size([1, 392])
torch.Size([1, 272])
torch.Size([1, 266])
torch.Size([1, 313])
torch.Size([1, 79])
torch.Size([1, 197])
torch.Size([1, 42])
torch.Size([1, 26])
torch.Size([1, 56])
torch.Size([1, 217])
torch.Size([1, 51])
torch.Size([1, 44])
torch.Size([1, 68])
torch.Size([1, 405])
torch.Size([1, 243])
torch.Size([1, 38])
torch.Size([1, 29])
torch.Size([1, 29])
torch.Size([1, 227])
torch.Size([1, 62])
torch.Size([1, 106])
torch.Size([1, 407])
torch.Size([1, 341])
{'eval_loss': nan, 'eval_runtime': 193.3983, 'eval_samples_per_second': 5.171, 'eval_steps_per_second': 5.171, 'epoch': 0.47}

  0%|          | 0/1531 [00:00<?, ?it/s][A
  0%|          | 1/1531 [00:00<08:50,  2.88it/s][A
  0%|          | 2/1531 [00:00<06:57,  3.66it/s][A
  0%|          | 3/1531 [00:00<06:36,  3.86it/s][A
  0%|          | 4/1531 [00:01<06:09,  4.13it/s][A
  0%|          | 5/1531 [00:01<05:54,  4.31it/s][A
  0%|          | 6/1531 [00:01<05:45,  4.42it/s][A
  0%|          | 7/1531 [00:01<05:53,  4.31it/s][A
  1%|          | 8/1531 [00:01<05:58,  4.24it/s][A
  1%|          | 9/1531 [00:02<05:50,  4.35it/s][A
  1%|          | 10/1531 [00:02<05:43,  4.42it/s][A
  1%|          | 11/1531 [00:02<05:51,  4.32it/s][A
  1%|          | 12/1531 [00:02<05:45,  4.39it/s][A
  1%|          | 13/1531 [00:03<05:53,  4.29it/s][A
  1%|          | 14/1531 [00:03<05:57,  4.24it/s][A
  1%|          | 15/1531 [00:03<06:01,  4.19it/s][A
  1%|          | 16/1531 [00:03<05:51,  4.31it/s][A
  1%|          | 17/1531 [00:04<05:58,  4.23it/s][A
  1%|          | 18/1531 [00:04<06:03,  4.17it/s][A
  1%|          | 19/1531 [00:04<06:03,  4.16it/s][A
  1%|▏         | 20/1531 [00:04<06:01,  4.18it/s][A
  1%|▏         | 21/1531 [00:04<05:51,  4.30it/s][A
  1%|▏         | 22/1531 [00:05<05:56,  4.24it/s][A
  2%|▏         | 23/1531 [00:05<05:58,  4.20it/s][A
  2%|▏         | 24/1531 [00:05<06:00,  4.18it/s][A
  2%|▏         | 25/1531 [00:05<06:00,  4.17it/s][A
  2%|▏         | 26/1531 [00:06<06:33,  3.82it/s][A
  2%|▏         | 27/1531 [00:06<06:51,  3.66it/s][A
  2%|▏         | 28/1531 [00:06<07:06,  3.52it/s][A
  2%|▏         | 29/1531 [00:07<07:16,  3.44it/s][A
  2%|▏         | 30/1531 [00:07<07:24,  3.38it/s][A
  2%|▏         | 31/1531 [00:07<07:25,  3.37it/s][A
  2%|▏         | 32/1531 [00:08<07:27,  3.35it/s][A
  2%|▏         | 33/1531 [00:08<07:36,  3.28it/s][A
  2%|▏         | 34/1531 [00:08<07:36,  3.28it/s][A
  2%|▏         | 35/1531 [00:08<07:19,  3.40it/s][A
  2%|▏         | 36/1531 [00:09<07:25,  3.35it/s][A
  2%|▏         | 37/1531 [00:09<07:28,  3.33it/s][A
  2%|▏         | 38/1531 [00:09<07:30,  3.31it/s][A
  3%|▎         | 39/1531 [00:10<07:31,  3.31it/s][A
  3%|▎         | 40/1531 [00:10<07:31,  3.30it/s][A
  3%|▎         | 41/1531 [00:10<07:15,  3.42it/s][A
  3%|▎         | 42/1531 [00:11<07:21,  3.37it/s][A
  3%|▎         | 43/1531 [00:11<07:28,  3.32it/s][A
  3%|▎         | 44/1531 [00:11<07:13,  3.43it/s][A
  3%|▎         | 45/1531 [00:11<07:21,  3.37it/s][A
  3%|▎         | 46/1531 [00:12<07:23,  3.34it/s][A
  3%|▎         | 47/1531 [00:12<07:07,  3.47it/s][A
  3%|▎         | 48/1531 [00:12<06:57,  3.55it/s][A
  3%|▎         | 49/1531 [00:13<07:08,  3.46it/s][A
  3%|▎         | 50/1531 [00:13<07:13,  3.41it/s][A
  3%|▎         | 51/1531 [00:13<07:01,  3.51it/s][A
  3%|▎         | 52/1531 [00:13<07:09,  3.44it/s][A
  3%|▎         | 53/1531 [00:14<06:49,  3.61it/s][A
  4%|▎         | 54/1531 [00:14<06:36,  3.73it/s][A
  4%|▎         | 55/1531 [00:14<06:27,  3.81it/s][A
  4%|▎         | 56/1531 [00:14<06:19,  3.88it/s][A
  4%|▎         | 57/1531 [00:15<06:14,  3.94it/s][A
  4%|▍         | 58/1531 [00:15<06:11,  3.96it/s][A
  4%|▍         | 59/1531 [00:15<06:09,  3.99it/s][A
  4%|▍         | 60/1531 [00:15<06:07,  4.00it/s][A
  4%|▍         | 61/1531 [00:16<06:05,  4.02it/s][A
  4%|▍         | 62/1531 [00:16<06:04,  4.03it/s][A
  4%|▍         | 63/1531 [00:16<06:04,  4.03it/s][A
  4%|▍         | 64/1531 [00:16<06:04,  4.03it/s][A
  4%|▍         | 65/1531 [00:17<06:03,  4.04it/s][A
  4%|▍         | 66/1531 [00:17<06:02,  4.04it/s][A
  4%|▍         | 67/1531 [00:17<06:02,  4.04it/s][A
  4%|▍         | 68/1531 [00:17<06:02,  4.04it/s][A
  5%|▍         | 69/1531 [00:18<06:05,  4.00it/s][A
  5%|▍         | 70/1531 [00:18<06:07,  3.98it/s][A
  5%|▍         | 71/1531 [00:18<06:02,  4.02it/s][A
  5%|▍         | 72/1531 [00:18<06:02,  4.03it/s][A
  5%|▍         | 73/1531 [00:19<06:01,  4.04it/s][A
  5%|▍         | 74/1531 [00:19<05:59,  4.05it/s][A
  5%|▍         | 75/1531 [00:19<05:59,  4.05it/s][A
  5%|▍         | 76/1531 [00:19<05:59,  4.05it/s][A
  5%|▌         | 77/1531 [00:20<06:00,  4.03it/s][A
  5%|▌         | 78/1531 [00:20<05:59,  4.04it/s][A
  5%|▌         | 79/1531 [00:20<05:59,  4.04it/s][A
  5%|▌         | 80/1531 [00:20<05:59,  4.04it/s][A
  5%|▌         | 81/1531 [00:21<05:58,  4.04it/s][A
  5%|▌         | 82/1531 [00:21<06:02,  4.00it/s][A
  5%|▌         | 83/1531 [00:21<06:05,  3.96it/s][A
  5%|▌         | 84/1531 [00:21<06:07,  3.94it/s][A
  6%|▌         | 85/1531 [00:22<06:08,  3.92it/s][A
  6%|▌         | 86/1531 [00:22<06:06,  3.94it/s][A
  6%|▌         | 87/1531 [00:22<06:10,  3.90it/s][A
  6%|▌         | 88/1531 [00:22<06:10,  3.90it/s][A
  6%|▌         | 89/1531 [00:23<06:07,  3.92it/s][A
  6%|▌         | 90/1531 [00:23<06:09,  3.90it/s][A
  6%|▌         | 91/1531 [00:23<06:09,  3.90it/s][A
  6%|▌         | 92/1531 [00:23<06:11,  3.88it/s][A
  6%|▌         | 93/1531 [00:24<06:14,  3.84it/s][A
  6%|▌         | 94/1531 [00:24<06:13,  3.85it/s][A
  6%|▌         | 95/1531 [00:24<06:12,  3.86it/s][A
  6%|▋         | 96/1531 [00:25<06:11,  3.86it/s][A
  6%|▋         | 97/1531 [00:25<06:08,  3.89it/s][A
  6%|▋         | 98/1531 [00:25<06:10,  3.86it/s][A
  6%|▋         | 99/1531 [00:25<06:12,  3.85it/s][A
  7%|▋         | 100/1531 [00:26<06:16,  3.81it/s][A
  7%|▋         | 101/1531 [00:26<06:16,  3.80it/s][A
  7%|▋         | 102/1531 [00:26<06:38,  3.59it/s][A
  7%|▋         | 103/1531 [00:26<06:29,  3.67it/s][A
  7%|▋         | 104/1531 [00:27<06:44,  3.53it/s][A
  7%|▋         | 105/1531 [00:27<06:35,  3.60it/s][A
  7%|▋         | 106/1531 [00:27<07:12,  3.30it/s][A
  7%|▋         | 107/1531 [00:28<07:49,  3.03it/s][A
  7%|▋         | 108/1531 [00:28<08:03,  2.94it/s][A
  7%|▋         | 109/1531 [00:28<08:11,  2.89it/s][A
  7%|▋         | 110/1531 [00:29<08:20,  2.84it/s][A
  7%|▋         | 111/1531 [00:29<08:24,  2.82it/s][A
  7%|▋         | 112/1531 [00:30<08:26,  2.80it/s][A
  7%|▋         | 113/1531 [00:30<08:37,  2.74it/s][A
  7%|▋         | 114/1531 [00:30<08:44,  2.70it/s][A
  8%|▊         | 115/1531 [00:31<08:46,  2.69it/s][A
  8%|▊         | 116/1531 [00:31<08:42,  2.71it/s][A
  8%|▊         | 117/1531 [00:31<08:16,  2.85it/s][A
  8%|▊         | 118/1531 [00:32<07:41,  3.06it/s][A
  8%|▊         | 119/1531 [00:32<07:30,  3.13it/s][A
  8%|▊         | 120/1531 [00:32<07:28,  3.15it/s][A
  8%|▊         | 121/1531 [00:33<07:21,  3.19it/s][A
  8%|▊         | 122/1531 [00:33<07:00,  3.35it/s][A
  8%|▊         | 123/1531 [00:33<07:01,  3.34it/s][A
  8%|▊         | 124/1531 [00:33<07:01,  3.34it/s][A
  8%|▊         | 125/1531 [00:34<06:48,  3.44it/s][A
  8%|▊         | 126/1531 [00:34<06:37,  3.54it/s][A
  8%|▊         | 127/1531 [00:34<06:33,  3.56it/s][A
  8%|▊         | 128/1531 [00:35<06:28,  3.61it/s][A
  8%|▊         | 129/1531 [00:35<06:39,  3.51it/s][A
  8%|▊         | 130/1531 [00:35<06:27,  3.61it/s][A
  9%|▊         | 131/1531 [00:35<06:19,  3.69it/s][A
  9%|▊         | 132/1531 [00:36<06:14,  3.73it/s][A
  9%|▊         | 133/1531 [00:36<06:11,  3.77it/s][A
  9%|▉         | 134/1531 [00:36<06:08,  3.79it/s][A
  9%|▉         | 135/1531 [00:36<06:05,  3.82it/s][A
  9%|▉         | 136/1531 [00:37<06:22,  3.65it/s][A
  9%|▉         | 137/1531 [00:37<06:15,  3.71it/s][A
  9%|▉         | 138/1531 [00:37<06:11,  3.75it/s][A
  9%|▉         | 139/1531 [00:37<06:13,  3.72it/s][A
  9%|▉         | 140/1531 [00:38<06:09,  3.77it/s][A
  9%|▉         | 141/1531 [00:38<06:08,  3.77it/s][A
  9%|▉         | 142/1531 [00:38<06:04,  3.81it/s][A
  9%|▉         | 143/1531 [00:38<06:01,  3.84it/s][A
  9%|▉         | 144/1531 [00:39<05:58,  3.87it/s][A
  9%|▉         | 145/1531 [00:39<06:01,  3.83it/s][A
 10%|▉         | 146/1531 [00:39<05:58,  3.87it/s][A
 10%|▉         | 147/1531 [00:40<05:57,  3.87it/s][A
 10%|▉         | 148/1531 [00:40<05:54,  3.90it/s][A
 10%|▉         | 149/1531 [00:40<05:55,  3.88it/s][A
 10%|▉         | 150/1531 [00:40<05:55,  3.88it/s][A
 10%|▉         | 151/1531 [00:41<05:56,  3.87it/s][A
 10%|▉         | 152/1531 [00:41<06:01,  3.82it/s][A
 10%|▉         | 153/1531 [00:41<05:58,  3.85it/s][A
 10%|█         | 154/1531 [00:41<05:54,  3.88it/s][A
 10%|█         | 155/1531 [00:42<05:52,  3.91it/s][A
 10%|█         | 156/1531 [00:42<05:57,  3.85it/s][A
 10%|█         | 157/1531 [00:42<05:59,  3.83it/s][A
 10%|█         | 158/1531 [00:42<05:58,  3.83it/s][A
 10%|█         | 159/1531 [00:43<05:58,  3.83it/s][A
 10%|█         | 160/1531 [00:43<05:56,  3.84it/s][A
 11%|█         | 161/1531 [00:43<05:50,  3.91it/s][A
 11%|█         | 162/1531 [00:43<05:52,  3.88it/s][A
 11%|█         | 163/1531 [00:44<05:48,  3.93it/s][A
 11%|█         | 164/1531 [00:44<05:49,  3.91it/s][A
 11%|█         | 165/1531 [00:44<05:44,  3.97it/s][A
 11%|█         | 166/1531 [00:44<05:42,  3.98it/s][A
 11%|█         | 167/1531 [00:45<05:38,  4.02it/s][A
 11%|█         | 168/1531 [00:45<05:35,  4.07it/s][A
 11%|█         | 169/1531 [00:45<05:34,  4.08it/s][A
 11%|█         | 170/1531 [00:45<05:35,  4.06it/s][A
 11%|█         | 171/1531 [00:46<05:35,  4.05it/s][A
 11%|█         | 172/1531 [00:46<05:14,  4.32it/s][A
 11%|█▏        | 173/1531 [00:46<05:09,  4.39it/s][A
 11%|█▏        | 174/1531 [00:46<05:04,  4.45it/s][A
 11%|█▏        | 175/1531 [00:46<05:02,  4.49it/s][A
 11%|█▏        | 176/1531 [00:47<04:54,  4.61it/s][A
 12%|█▏        | 177/1531 [00:47<04:58,  4.53it/s][A
 12%|█▏        | 178/1531 [00:47<05:00,  4.50it/s][A
 12%|█▏        | 179/1531 [00:47<04:50,  4.66it/s][A
 12%|█▏        | 180/1531 [00:48<04:42,  4.79it/s][A
 12%|█▏        | 181/1531 [00:48<04:45,  4.73it/s][A
 12%|█▏        | 182/1531 [00:48<04:48,  4.68it/s][A
 12%|█▏        | 183/1531 [00:48<04:48,  4.68it/s][A
 12%|█▏        | 184/1531 [00:48<04:50,  4.63it/s][A
 12%|█▏        | 185/1531 [00:49<04:51,  4.62it/s][A
 12%|█▏        | 186/1531 [00:49<04:43,  4.74it/s][A
 12%|█▏        | 187/1531 [00:49<04:35,  4.87it/s][A
 12%|█▏        | 188/1531 [00:49<04:31,  4.94it/s][A
 12%|█▏        | 189/1531 [00:49<04:38,  4.83it/s][A
 12%|█▏        | 190/1531 [00:50<04:41,  4.76it/s][A
 12%|█▏        | 191/1531 [00:50<04:34,  4.87it/s][A
 13%|█▎        | 192/1531 [00:50<04:30,  4.96it/s][A
 13%|█▎        | 193/1531 [00:50<04:38,  4.81it/s][A
 13%|█▎        | 194/1531 [00:50<04:33,  4.90it/s][A
 13%|█▎        | 195/1531 [00:51<04:37,  4.81it/s][A
 13%|█▎        | 196/1531 [00:51<04:32,  4.91it/s][A
 13%|█▎        | 197/1531 [00:51<04:37,  4.81it/s][A
 13%|█▎        | 198/1531 [00:51<05:14,  4.24it/s][A
 13%|█▎        | 199/1531 [00:52<05:43,  3.88it/s][A
 13%|█▎        | 200/1531 [00:52<05:47,  3.83it/s][A
 13%|█▎        | 201/1531 [00:52<06:02,  3.67it/s][A
 13%|█▎        | 202/1531 [00:53<06:02,  3.66it/s][A
 13%|█▎        | 203/1531 [00:53<06:23,  3.46it/s][A
 13%|█▎        | 204/1531 [00:53<06:30,  3.40it/s][A
 13%|█▎        | 205/1531 [00:53<06:33,  3.37it/s][A
 13%|█▎        | 206/1531 [00:54<06:39,  3.32it/s][A
 14%|█▎        | 207/1531 [00:54<06:45,  3.26it/s][A
 14%|█▎        | 208/1531 [00:54<06:42,  3.29it/s][A
 14%|█▎        | 209/1531 [00:55<06:42,  3.29it/s][A
 14%|█▎        | 210/1531 [00:55<06:22,  3.45it/s][A
 14%|█▍        | 211/1531 [00:55<06:05,  3.61it/s][A
 14%|█▍        | 212/1531 [00:55<05:53,  3.73it/s][A
 14%|█▍        | 213/1531 [00:56<05:46,  3.81it/s][A
 14%|█▍        | 214/1531 [00:56<05:41,  3.86it/s][A
 14%|█▍        | 215/1531 [00:56<05:36,  3.91it/s][A
 14%|█▍        | 216/1531 [00:56<05:34,  3.93it/s][A
 14%|█▍        | 217/1531 [00:57<05:32,  3.95it/s][A
 14%|█▍        | 218/1531 [00:57<05:32,  3.95it/s][A
 14%|█▍        | 219/1531 [00:57<05:28,  3.99it/s][A
 14%|█▍        | 220/1531 [00:57<05:27,  4.01it/s][A
 14%|█▍        | 221/1531 [00:58<05:30,  3.97it/s][A
 15%|█▍        | 222/1531 [00:58<05:28,  3.98it/s][A
 15%|█▍        | 223/1531 [00:58<05:25,  4.02it/s][A
 15%|█▍        | 224/1531 [00:58<05:23,  4.04it/s][A
 15%|█▍        | 225/1531 [00:59<05:28,  3.98it/s][A
 15%|█▍        | 226/1531 [00:59<05:34,  3.90it/s][A
 15%|█▍        | 227/1531 [00:59<05:36,  3.88it/s][A
 15%|█▍        | 228/1531 [00:59<05:35,  3.88it/s][A
 15%|█▍        | 229/1531 [01:00<05:39,  3.84it/s][A
 15%|█▌        | 230/1531 [01:00<05:40,  3.82it/s][A
 15%|█▌        | 231/1531 [01:00<05:40,  3.82it/s][A
 15%|█▌        | 232/1531 [01:01<05:55,  3.65it/s][A
 15%|█▌        | 233/1531 [01:01<05:54,  3.66it/s][A
 15%|█▌        | 234/1531 [01:01<05:52,  3.68it/s][A
 15%|█▌        | 235/1531 [01:01<05:48,  3.71it/s][A
 15%|█▌        | 236/1531 [01:02<05:48,  3.71it/s][A
 15%|█▌        | 237/1531 [01:02<06:02,  3.57it/s][A
 16%|█▌        | 238/1531 [01:02<05:56,  3.63it/s][A
 16%|█▌        | 239/1531 [01:02<05:51,  3.67it/s][A
 16%|█▌        | 240/1531 [01:03<05:49,  3.70it/s][A
 16%|█▌        | 241/1531 [01:03<06:02,  3.56it/s][A
 16%|█▌        | 242/1531 [01:03<05:54,  3.63it/s][A
 16%|█▌        | 243/1531 [01:04<06:05,  3.53it/s][A
 16%|█▌        | 244/1531 [01:04<05:57,  3.60it/s][A
 16%|█▌        | 245/1531 [01:04<05:49,  3.68it/s][A
 16%|█▌        | 246/1531 [01:04<06:03,  3.53it/s][A
 16%|█▌        | 247/1531 [01:05<05:58,  3.58it/s][A
 16%|█▌        | 248/1531 [01:05<05:53,  3.63it/s][A
 16%|█▋        | 249/1531 [01:05<05:49,  3.67it/s][A
 16%|█▋        | 250/1531 [01:06<05:44,  3.72it/s][A
 16%|█▋        | 251/1531 [01:06<05:41,  3.75it/s][A
 16%|█▋        | 252/1531 [01:06<05:41,  3.75it/s][A
 17%|█▋        | 253/1531 [01:06<05:39,  3.76it/s][A
 17%|█▋        | 254/1531 [01:07<05:37,  3.79it/s][A
 17%|█▋        | 255/1531 [01:07<05:36,  3.80it/s][A
 17%|█▋        | 256/1531 [01:07<05:34,  3.81it/s][A
 17%|█▋        | 257/1531 [01:07<05:33,  3.82it/s][A
 17%|█▋        | 258/1531 [01:08<05:32,  3.83it/s][A
 17%|█▋        | 259/1531 [01:08<05:35,  3.79it/s][A
 17%|█▋        | 260/1531 [01:08<05:50,  3.63it/s][A
 17%|█▋        | 261/1531 [01:08<06:00,  3.53it/s][A
 17%|█▋        | 262/1531 [01:09<05:51,  3.61it/s][A
 17%|█▋        | 263/1531 [01:09<05:46,  3.66it/s][A
 17%|█▋        | 264/1531 [01:09<05:44,  3.67it/s][A
 17%|█▋        | 265/1531 [01:10<05:40,  3.72it/s][A
 17%|█▋        | 266/1531 [01:10<05:36,  3.76it/s][A
 17%|█▋        | 267/1531 [01:10<05:50,  3.60it/s][A
 18%|█▊        | 268/1531 [01:10<05:58,  3.52it/s][A
 18%|█▊        | 269/1531 [01:11<06:07,  3.43it/s][A
 18%|█▊        | 270/1531 [01:11<06:17,  3.34it/s][A
 18%|█▊        | 271/1531 [01:11<06:18,  3.33it/s][A
 18%|█▊        | 272/1531 [01:12<06:22,  3.29it/s][A
 18%|█▊        | 273/1531 [01:12<06:24,  3.27it/s][A
 18%|█▊        | 274/1531 [01:12<06:24,  3.27it/s][A
 18%|█▊        | 275/1531 [01:13<06:28,  3.23it/s][A
 18%|█▊        | 276/1531 [01:13<06:30,  3.21it/s][A
 18%|█▊        | 277/1531 [01:13<07:01,  2.98it/s][A
 18%|█▊        | 278/1531 [01:14<06:49,  3.06it/s][A
 18%|█▊        | 279/1531 [01:14<07:01,  2.97it/s][A
 18%|█▊        | 280/1531 [01:14<07:10,  2.91it/s][A
 18%|█▊        | 281/1531 [01:15<06:42,  3.11it/s][A
 18%|█▊        | 282/1531 [01:15<06:18,  3.30it/s][A
 18%|█▊        | 283/1531 [01:15<05:58,  3.48it/s][A
 19%|█▊        | 284/1531 [01:15<05:43,  3.63it/s][A
 19%|█▊        | 285/1531 [01:16<05:32,  3.75it/s][A
 19%|█▊        | 286/1531 [01:16<05:28,  3.80it/s][A
 19%|█▊        | 287/1531 [01:16<05:21,  3.87it/s][A
 19%|█▉        | 288/1531 [01:16<05:17,  3.91it/s][A
 19%|█▉        | 289/1531 [01:17<05:17,  3.92it/s][A
 19%|█▉        | 290/1531 [01:17<05:19,  3.88it/s][A
 19%|█▉        | 291/1531 [01:17<05:36,  3.69it/s][A
 19%|█▉        | 292/1531 [01:17<05:32,  3.73it/s][A
 19%|█▉        | 293/1531 [01:18<06:02,  3.41it/s][A
 19%|█▉        | 294/1531 [01:18<06:07,  3.37it/s][A
 19%|█▉        | 295/1531 [01:18<05:54,  3.49it/s][A
 19%|█▉        | 296/1531 [01:19<05:57,  3.45it/s][A
 19%|█▉        | 297/1531 [01:19<06:00,  3.42it/s][A
 19%|█▉        | 298/1531 [01:19<05:51,  3.51it/s][A
 20%|█▉        | 299/1531 [01:19<05:42,  3.60it/s][A
 20%|█▉        | 300/1531 [01:20<06:39,  3.08it/s][A
 20%|█▉        | 301/1531 [01:20<06:16,  3.26it/s][A
 20%|█▉        | 302/1531 [01:20<06:08,  3.34it/s][A
 20%|█▉        | 303/1531 [01:21<06:07,  3.34it/s][A
 20%|█▉        | 304/1531 [01:21<06:12,  3.29it/s][A
 20%|█▉        | 305/1531 [01:21<05:55,  3.45it/s][A
 20%|█▉        | 306/1531 [01:22<05:44,  3.56it/s][A
 20%|██        | 307/1531 [01:22<05:35,  3.65it/s][A
 20%|██        | 308/1531 [01:22<05:34,  3.65it/s][A
 20%|██        | 309/1531 [01:22<05:35,  3.65it/s][A
 20%|██        | 310/1531 [01:23<05:29,  3.70it/s][A
 20%|██        | 311/1531 [01:23<05:50,  3.48it/s][A
 20%|██        | 312/1531 [01:23<06:59,  2.91it/s][A
 20%|██        | 313/1531 [01:24<06:36,  3.07it/s][A
 21%|██        | 314/1531 [01:24<06:25,  3.15it/s][A
 21%|██        | 315/1531 [01:24<06:05,  3.33it/s][A
 21%|██        | 316/1531 [01:25<05:51,  3.45it/s][A
 21%|██        | 317/1531 [01:25<06:09,  3.28it/s][A
 21%|██        | 318/1531 [01:25<06:01,  3.35it/s][A
 21%|██        | 319/1531 [01:25<05:49,  3.47it/s][A
 21%|██        | 320/1531 [01:26<06:30,  3.10it/s][A
 21%|██        | 321/1531 [01:26<06:06,  3.30it/s][A
 21%|██        | 322/1531 [01:26<05:51,  3.44it/s][A
 21%|██        | 323/1531 [01:27<05:50,  3.44it/s][A
 21%|██        | 324/1531 [01:27<05:52,  3.43it/s][A
 21%|██        | 325/1531 [01:27<05:40,  3.54it/s][A
 21%|██▏       | 326/1531 [01:27<05:34,  3.60it/s][A
 21%|██▏       | 327/1531 [01:28<05:31,  3.64it/s][A
 21%|██▏       | 328/1531 [01:28<05:31,  3.63it/s][A
 21%|██▏       | 329/1531 [01:28<05:25,  3.69it/s][A
 22%|██▏       | 330/1531 [01:29<05:45,  3.48it/s][A
 22%|██▏       | 331/1531 [01:29<05:39,  3.53it/s][A
 22%|██▏       | 332/1531 [01:29<05:39,  3.53it/s][A
 22%|██▏       | 333/1531 [01:29<05:41,  3.51it/s][A
 22%|██▏       | 334/1531 [01:30<06:02,  3.30it/s][A
 22%|██▏       | 335/1531 [01:30<05:59,  3.33it/s][A
 22%|██▏       | 336/1531 [01:30<05:41,  3.50it/s][A
 22%|██▏       | 337/1531 [01:31<05:47,  3.43it/s][A
 22%|██▏       | 338/1531 [01:31<05:36,  3.55it/s][A
 22%|██▏       | 339/1531 [01:31<05:28,  3.63it/s][A
 22%|██▏       | 340/1531 [01:31<05:24,  3.67it/s][A
 22%|██▏       | 341/1531 [01:32<05:20,  3.71it/s][A
 22%|██▏       | 342/1531 [01:32<05:17,  3.75it/s][A
 22%|██▏       | 343/1531 [01:32<05:15,  3.77it/s][A
 22%|██▏       | 344/1531 [01:33<05:27,  3.62it/s][A
 23%|██▎       | 345/1531 [01:33<06:05,  3.25it/s][A
 23%|██▎       | 346/1531 [01:33<06:33,  3.01it/s][A
 23%|██▎       | 347/1531 [01:34<07:13,  2.73it/s][A
 23%|██▎       | 348/1531 [01:34<07:20,  2.68it/s][A
 23%|██▎       | 349/1531 [01:35<07:33,  2.61it/s][A
 23%|██▎       | 350/1531 [01:35<07:41,  2.56it/s][A
 23%|██▎       | 351/1531 [01:35<07:42,  2.55it/s][A
 23%|██▎       | 352/1531 [01:36<07:43,  2.55it/s][A
 23%|██▎       | 353/1531 [01:36<07:51,  2.50it/s][A
 23%|██▎       | 354/1531 [01:37<10:08,  1.93it/s][A
 23%|██▎       | 355/1531 [01:38<12:56,  1.52it/s][A
 23%|██▎       | 356/1531 [01:39<13:58,  1.40it/s][A
 23%|██▎       | 357/1531 [01:39<13:45,  1.42it/s][A
 23%|██▎       | 358/1531 [01:40<13:25,  1.46it/s][A
 23%|██▎       | 359/1531 [01:41<14:18,  1.36it/s][A
 24%|██▎       | 360/1531 [01:42<14:28,  1.35it/s][A
 24%|██▎       | 361/1531 [01:42<14:34,  1.34it/s][A
 24%|██▎       | 362/1531 [01:43<14:59,  1.30it/s][A
 24%|██▎       | 363/1531 [01:44<14:15,  1.37it/s][A
 24%|██▍       | 364/1531 [01:45<14:52,  1.31it/s][A
 24%|██▍       | 365/1531 [01:46<15:18,  1.27it/s][A
 24%|██▍       | 366/1531 [01:46<15:27,  1.26it/s][A
 24%|██▍       | 367/1531 [01:47<14:30,  1.34it/s][A
 24%|██▍       | 368/1531 [01:48<14:25,  1.34it/s][A
 24%|██▍       | 369/1531 [01:48<14:05,  1.37it/s][A
 24%|██▍       | 370/1531 [01:49<14:35,  1.33it/s][A
 24%|██▍       | 371/1531 [01:50<14:32,  1.33it/s][A
 24%|██▍       | 372/1531 [01:50<11:35,  1.67it/s][A
 24%|██▍       | 373/1531 [01:51<09:28,  2.04it/s][A
 24%|██▍       | 374/1531 [01:51<08:01,  2.40it/s][A
 24%|██▍       | 375/1531 [01:51<07:00,  2.75it/s][A
 25%|██▍       | 376/1531 [01:51<06:18,  3.05it/s][A
 25%|██▍       | 377/1531 [01:51<05:49,  3.30it/s][A
 25%|██▍       | 378/1531 [01:52<05:28,  3.51it/s][A
 25%|██▍       | 379/1531 [01:52<05:13,  3.67it/s][A
 25%|██▍       | 380/1531 [01:52<05:01,  3.81it/s][A
 25%|██▍       | 381/1531 [01:52<04:54,  3.90it/s][A
 25%|██▍       | 382/1531 [01:53<04:49,  3.97it/s][A
 25%|██▌       | 383/1531 [01:53<04:46,  4.00it/s][A
 25%|██▌       | 384/1531 [01:53<04:44,  4.04it/s][A
 25%|██▌       | 385/1531 [01:53<04:42,  4.06it/s][A
 25%|██▌       | 386/1531 [01:54<04:40,  4.08it/s][A
 25%|██▌       | 387/1531 [01:54<04:40,  4.07it/s][A
 25%|██▌       | 388/1531 [01:54<04:39,  4.08it/s][A
 25%|██▌       | 389/1531 [01:54<04:39,  4.09it/s][A
 25%|██▌       | 390/1531 [01:55<04:39,  4.09it/s][A
 26%|██▌       | 391/1531 [01:55<04:40,  4.07it/s][A
 26%|██▌       | 392/1531 [01:55<04:38,  4.10it/s][A
 26%|██▌       | 393/1531 [01:55<04:37,  4.10it/s][A
 26%|██▌       | 394/1531 [01:56<04:42,  4.02it/s][A
 26%|██▌       | 395/1531 [01:56<04:45,  3.98it/s][A
 26%|██▌       | 396/1531 [01:56<04:46,  3.96it/s][A
 26%|██▌       | 397/1531 [01:56<04:49,  3.92it/s][A
 26%|██▌       | 398/1531 [01:57<04:49,  3.91it/s][A
 26%|██▌       | 399/1531 [01:57<04:46,  3.94it/s][A
 26%|██▌       | 400/1531 [01:57<04:47,  3.93it/s][A
 26%|██▌       | 401/1531 [01:57<04:47,  3.93it/s][A
 26%|██▋       | 402/1531 [01:58<04:48,  3.91it/s][A
 26%|██▋       | 403/1531 [01:58<04:48,  3.90it/s][A
 26%|██▋       | 404/1531 [01:58<04:46,  3.94it/s][A
 26%|██▋       | 405/1531 [01:58<04:44,  3.96it/s][A
 27%|██▋       | 406/1531 [01:59<04:42,  3.99it/s][A
 27%|██▋       | 407/1531 [01:59<04:41,  3.99it/s][A
 27%|██▋       | 408/1531 [01:59<04:40,  4.00it/s][A
 27%|██▋       | 409/1531 [01:59<04:42,  3.97it/s][A
 27%|██▋       | 410/1531 [02:00<04:42,  3.96it/s][A
 27%|██▋       | 411/1531 [02:00<04:42,  3.97it/s][A
 27%|██▋       | 412/1531 [02:00<04:41,  3.97it/s][A
 27%|██▋       | 413/1531 [02:00<04:41,  3.98it/s][A
 27%|██▋       | 414/1531 [02:01<04:41,  3.96it/s][A
 27%|██▋       | 415/1531 [02:01<04:40,  3.99it/s][A
 27%|██▋       | 416/1531 [02:01<04:36,  4.04it/s][A
 27%|██▋       | 417/1531 [02:01<04:32,  4.09it/s][A
 27%|██▋       | 418/1531 [02:02<04:32,  4.09it/s][A
 27%|██▋       | 419/1531 [02:02<04:32,  4.08it/s][A
 27%|██▋       | 420/1531 [02:02<04:31,  4.09it/s][A
 27%|██▋       | 421/1531 [02:02<04:31,  4.09it/s][A
 28%|██▊       | 422/1531 [02:03<04:30,  4.11it/s][A
 28%|██▊       | 423/1531 [02:03<04:30,  4.10it/s][A
 28%|██▊       | 424/1531 [02:03<04:31,  4.07it/s][A
 28%|██▊       | 425/1531 [02:03<04:31,  4.08it/s][A
 28%|██▊       | 426/1531 [02:04<04:30,  4.09it/s][A
 28%|██▊       | 427/1531 [02:04<04:30,  4.08it/s][A
 28%|██▊       | 428/1531 [02:04<04:30,  4.07it/s][A
 28%|██▊       | 429/1531 [02:04<04:29,  4.09it/s][A
 28%|██▊       | 430/1531 [02:05<04:29,  4.09it/s][A
 28%|██▊       | 431/1531 [02:05<04:29,  4.08it/s][A
 28%|██▊       | 432/1531 [02:05<04:29,  4.07it/s][A
 28%|██▊       | 433/1531 [02:05<04:30,  4.06it/s][A
 28%|██▊       | 434/1531 [02:06<04:29,  4.07it/s][A
 28%|██▊       | 435/1531 [02:06<04:29,  4.07it/s][A
 28%|██▊       | 436/1531 [02:06<04:29,  4.07it/s][A
 29%|██▊       | 437/1531 [02:06<04:30,  4.05it/s][A
 29%|██▊       | 438/1531 [02:07<04:29,  4.05it/s][A
 29%|██▊       | 439/1531 [02:07<04:28,  4.07it/s][A
 29%|██▊       | 440/1531 [02:07<04:27,  4.08it/s][A
 29%|██▉       | 441/1531 [02:07<04:26,  4.09it/s][A
 29%|██▉       | 442/1531 [02:08<04:26,  4.09it/s][A
 29%|██▉       | 443/1531 [02:08<04:25,  4.10it/s][A
 29%|██▉       | 444/1531 [02:08<04:25,  4.09it/s][A
 29%|██▉       | 445/1531 [02:08<04:25,  4.09it/s][A
 29%|██▉       | 446/1531 [02:09<04:25,  4.09it/s][A
 29%|██▉       | 447/1531 [02:09<04:25,  4.08it/s][A
 29%|██▉       | 448/1531 [02:09<04:25,  4.08it/s][A
 29%|██▉       | 449/1531 [02:09<04:25,  4.07it/s][A
 29%|██▉       | 450/1531 [02:10<04:25,  4.08it/s][A
 29%|██▉       | 451/1531 [02:10<04:25,  4.06it/s][A
 30%|██▉       | 452/1531 [02:10<04:24,  4.08it/s][A
 30%|██▉       | 453/1531 [02:10<04:24,  4.08it/s][A
 30%|██▉       | 454/1531 [02:11<04:25,  4.06it/s][A
 30%|██▉       | 455/1531 [02:11<04:25,  4.06it/s][A
 30%|██▉       | 456/1531 [02:11<04:23,  4.08it/s][A
 30%|██▉       | 457/1531 [02:11<04:22,  4.09it/s][A
 30%|██▉       | 458/1531 [02:12<04:27,  4.01it/s][A
 30%|██▉       | 459/1531 [02:12<04:31,  3.95it/s][A
 30%|███       | 460/1531 [02:12<04:32,  3.93it/s][A
 30%|███       | 461/1531 [02:12<04:34,  3.90it/s][A
 30%|███       | 462/1531 [02:13<04:38,  3.84it/s][A
 30%|███       | 463/1531 [02:13<04:37,  3.85it/s][A
 30%|███       | 464/1531 [02:13<04:37,  3.85it/s][A
 30%|███       | 465/1531 [02:13<04:36,  3.86it/s][A
 30%|███       | 466/1531 [02:14<04:36,  3.86it/s][A
 31%|███       | 467/1531 [02:14<04:36,  3.85it/s][A
 31%|███       | 468/1531 [02:14<04:37,  3.82it/s][A
 31%|███       | 469/1531 [02:14<04:38,  3.82it/s][A
 31%|███       | 470/1531 [02:15<04:37,  3.83it/s][A
 31%|███       | 471/1531 [02:15<04:37,  3.81it/s][A
 31%|███       | 472/1531 [02:15<04:37,  3.81it/s][A
 31%|███       | 473/1531 [02:15<04:37,  3.81it/s][A
 31%|███       | 474/1531 [02:16<04:35,  3.84it/s][A
 31%|███       | 475/1531 [02:16<04:35,  3.83it/s][A
 31%|███       | 476/1531 [02:16<04:33,  3.86it/s][A
 31%|███       | 477/1531 [02:16<04:33,  3.86it/s][A
 31%|███       | 478/1531 [02:17<04:33,  3.86it/s][A
 31%|███▏      | 479/1531 [02:17<04:34,  3.83it/s][A
 31%|███▏      | 480/1531 [02:17<04:33,  3.84it/s][A
 31%|███▏      | 481/1531 [02:18<04:32,  3.85it/s][A
 31%|███▏      | 482/1531 [02:18<04:32,  3.84it/s][A
 32%|███▏      | 483/1531 [02:18<04:31,  3.86it/s][A
 32%|███▏      | 484/1531 [02:18<04:31,  3.85it/s][A
 32%|███▏      | 485/1531 [02:19<04:32,  3.83it/s][A
 32%|███▏      | 486/1531 [02:19<04:32,  3.84it/s][A
 32%|███▏      | 487/1531 [02:19<04:26,  3.91it/s][A
 32%|███▏      | 488/1531 [02:19<04:21,  3.99it/s][A
 32%|███▏      | 489/1531 [02:20<04:20,  3.99it/s][A
 32%|███▏      | 490/1531 [02:20<04:18,  4.02it/s][A
 32%|███▏      | 491/1531 [02:20<04:16,  4.05it/s][A
 32%|███▏      | 492/1531 [02:20<04:14,  4.08it/s][A
 32%|███▏      | 493/1531 [02:21<04:14,  4.07it/s][A
 32%|███▏      | 494/1531 [02:21<04:14,  4.08it/s][A
 32%|███▏      | 495/1531 [02:21<04:14,  4.07it/s][A
 32%|███▏      | 496/1531 [02:21<04:14,  4.07it/s][A
 32%|███▏      | 497/1531 [02:21<04:12,  4.09it/s][A
 33%|███▎      | 498/1531 [02:22<04:12,  4.08it/s][A
 33%|███▎      | 499/1531 [02:22<04:12,  4.09it/s][A
 33%|███▎      | 500/1531 [02:22<04:12,  4.08it/s][A
 33%|███▎      | 501/1531 [02:22<04:13,  4.07it/s][A
 33%|███▎      | 502/1531 [02:23<04:12,  4.07it/s][A
 33%|███▎      | 503/1531 [02:23<04:11,  4.08it/s][A
 33%|███▎      | 504/1531 [02:23<04:09,  4.11it/s][A
 33%|███▎      | 505/1531 [02:23<04:07,  4.14it/s][A
 33%|███▎      | 506/1531 [02:24<04:09,  4.11it/s][A
 33%|███▎      | 507/1531 [02:24<04:10,  4.09it/s][A
 33%|███▎      | 508/1531 [02:24<04:10,  4.09it/s][A
 33%|███▎      | 509/1531 [02:24<04:09,  4.10it/s][A
 33%|███▎      | 510/1531 [02:25<04:08,  4.10it/s][A
 33%|███▎      | 511/1531 [02:25<04:08,  4.10it/s][A
 33%|███▎      | 512/1531 [02:25<04:09,  4.09it/s][A
 34%|███▎      | 513/1531 [02:25<04:14,  3.99it/s][A
 34%|███▎      | 514/1531 [02:26<04:29,  3.77it/s][A
 34%|███▎      | 515/1531 [02:26<04:28,  3.79it/s][A
 34%|███▎      | 516/1531 [02:26<04:28,  3.78it/s][A
 34%|███▍      | 517/1531 [02:27<04:28,  3.78it/s][A
 34%|███▍      | 518/1531 [02:27<04:26,  3.80it/s][A
 34%|███▍      | 519/1531 [02:27<04:25,  3.81it/s][A
 34%|███▍      | 520/1531 [02:27<04:41,  3.60it/s][A
 34%|███▍      | 521/1531 [02:28<04:48,  3.51it/s][A
 34%|███▍      | 522/1531 [02:28<04:40,  3.60it/s][A
 34%|███▍      | 523/1531 [02:28<04:35,  3.66it/s][A
 34%|███▍      | 524/1531 [02:28<04:32,  3.69it/s][A
 34%|███▍      | 525/1531 [02:29<04:30,  3.72it/s][A
 34%|███▍      | 526/1531 [02:29<04:29,  3.73it/s][A
 34%|███▍      | 527/1531 [02:29<04:27,  3.75it/s][A
 34%|███▍      | 528/1531 [02:29<04:26,  3.77it/s][A
 35%|███▍      | 529/1531 [02:30<04:26,  3.77it/s][A
 35%|███▍      | 530/1531 [02:30<04:24,  3.78it/s][A
 35%|███▍      | 531/1531 [02:30<04:25,  3.77it/s][A
 35%|███▍      | 532/1531 [02:31<04:25,  3.77it/s][A
 35%|███▍      | 533/1531 [02:31<04:23,  3.79it/s][A
 35%|███▍      | 534/1531 [02:31<04:21,  3.82it/s][A
 35%|███▍      | 535/1531 [02:31<04:20,  3.83it/s][A
 35%|███▌      | 536/1531 [02:32<04:19,  3.84it/s][A
 35%|███▌      | 537/1531 [02:32<04:18,  3.84it/s][A
 35%|███▌      | 538/1531 [02:32<04:19,  3.82it/s][A
 35%|███▌      | 539/1531 [02:32<04:20,  3.81it/s][A
 35%|███▌      | 540/1531 [02:33<04:20,  3.80it/s][A
 35%|███▌      | 541/1531 [02:33<04:19,  3.81it/s][A
 35%|███▌      | 542/1531 [02:33<04:19,  3.82it/s][A
 35%|███▌      | 543/1531 [02:33<04:18,  3.82it/s][A
 36%|███▌      | 544/1531 [02:34<04:17,  3.83it/s][A
 36%|███▌      | 545/1531 [02:34<04:17,  3.82it/s][A
 36%|███▌      | 546/1531 [02:34<04:19,  3.80it/s][A
 36%|███▌      | 547/1531 [02:34<04:18,  3.80it/s][A
 36%|███▌      | 548/1531 [02:35<04:16,  3.82it/s][A
 36%|███▌      | 549/1531 [02:35<04:17,  3.81it/s][A
 36%|███▌      | 550/1531 [02:35<04:16,  3.82it/s][A
 36%|███▌      | 551/1531 [02:36<04:15,  3.83it/s][A
 36%|███▌      | 552/1531 [02:36<04:15,  3.83it/s][A
 36%|███▌      | 553/1531 [02:36<04:15,  3.83it/s][A
 36%|███▌      | 554/1531 [02:36<04:17,  3.80it/s][A
 36%|███▋      | 555/1531 [02:37<04:15,  3.82it/s][A
 36%|███▋      | 556/1531 [02:37<04:14,  3.83it/s][A
 36%|███▋      | 557/1531 [02:37<04:13,  3.84it/s][A
 36%|███▋      | 558/1531 [02:37<04:13,  3.84it/s][A
 37%|███▋      | 559/1531 [02:38<04:12,  3.84it/s][A
 37%|███▋      | 560/1531 [02:38<04:12,  3.85it/s][A
 37%|███▋      | 561/1531 [02:38<04:12,  3.85it/s][A
 37%|███▋      | 562/1531 [02:38<04:11,  3.85it/s][A
 37%|███▋      | 563/1531 [02:39<04:24,  3.65it/s][A
 37%|███▋      | 564/1531 [02:39<04:22,  3.68it/s][A
 37%|███▋      | 565/1531 [02:39<04:21,  3.69it/s][A
 37%|███▋      | 566/1531 [02:39<04:18,  3.73it/s][A
 37%|███▋      | 567/1531 [02:40<04:16,  3.75it/s][A
 37%|███▋      | 568/1531 [02:40<04:19,  3.71it/s][A
 37%|███▋      | 569/1531 [02:40<04:17,  3.73it/s][A
 37%|███▋      | 570/1531 [02:41<04:15,  3.77it/s][A
 37%|███▋      | 571/1531 [02:41<04:13,  3.79it/s][A
 37%|███▋      | 572/1531 [02:41<04:11,  3.81it/s][A
 37%|███▋      | 573/1531 [02:41<04:11,  3.81it/s][A
 37%|███▋      | 574/1531 [02:42<04:10,  3.82it/s][A
 38%|███▊      | 575/1531 [02:42<04:10,  3.81it/s][A
 38%|███▊      | 576/1531 [02:42<04:09,  3.83it/s][A
 38%|███▊      | 577/1531 [02:42<04:09,  3.82it/s][A
 38%|███▊      | 578/1531 [02:43<04:09,  3.82it/s][A
 38%|███▊      | 579/1531 [02:43<04:08,  3.83it/s][A
 38%|███▊      | 580/1531 [02:43<04:08,  3.83it/s][A
 38%|███▊      | 581/1531 [02:43<04:08,  3.82it/s][A
 38%|███▊      | 582/1531 [02:44<04:08,  3.82it/s][A
 38%|███▊      | 583/1531 [02:44<04:10,  3.79it/s][A
 38%|███▊      | 584/1531 [02:44<04:08,  3.80it/s][A
 38%|███▊      | 585/1531 [02:44<04:07,  3.82it/s][A
 38%|███▊      | 586/1531 [02:45<04:06,  3.83it/s][A
 38%|███▊      | 587/1531 [02:45<04:05,  3.84it/s][A
 38%|███▊      | 588/1531 [02:45<04:05,  3.84it/s][A
 38%|███▊      | 589/1531 [02:46<04:08,  3.79it/s][A
 39%|███▊      | 590/1531 [02:46<04:36,  3.40it/s][A
 39%|███▊      | 591/1531 [02:46<04:55,  3.18it/s][A
 39%|███▊      | 592/1531 [02:47<05:08,  3.05it/s][A
 39%|███▊      | 593/1531 [02:47<05:18,  2.95it/s][A
 39%|███▉      | 594/1531 [02:47<05:31,  2.82it/s][A
 39%|███▉      | 595/1531 [02:48<05:39,  2.76it/s][A
 39%|███▉      | 596/1531 [02:48<05:46,  2.70it/s][A
 39%|███▉      | 597/1531 [02:49<05:49,  2.67it/s][A
 39%|███▉      | 598/1531 [02:49<05:47,  2.68it/s][A
 39%|███▉      | 599/1531 [02:49<05:43,  2.71it/s][A
 39%|███▉      | 600/1531 [02:50<05:41,  2.73it/s][A
 39%|███▉      | 601/1531 [02:50<05:40,  2.73it/s][A
 39%|███▉      | 602/1531 [02:50<05:46,  2.68it/s][A
 39%|███▉      | 603/1531 [02:51<05:44,  2.69it/s][A
 39%|███▉      | 604/1531 [02:51<05:41,  2.72it/s][A
 40%|███▉      | 605/1531 [02:51<05:39,  2.73it/s][A
 40%|███▉      | 606/1531 [02:52<05:45,  2.68it/s][A
 40%|███▉      | 607/1531 [02:52<05:41,  2.71it/s][A
 40%|███▉      | 608/1531 [02:53<05:37,  2.73it/s][A
 40%|███▉      | 609/1531 [02:53<05:42,  2.69it/s][A
 40%|███▉      | 610/1531 [02:53<05:40,  2.70it/s][A
 40%|███▉      | 611/1531 [02:54<05:37,  2.73it/s][A
 40%|███▉      | 612/1531 [02:54<05:37,  2.72it/s][A
 40%|████      | 613/1531 [02:55<07:09,  2.14it/s][A
 40%|████      | 614/1531 [02:55<08:16,  1.85it/s][A
 40%|████      | 615/1531 [02:56<09:01,  1.69it/s][A
 40%|████      | 616/1531 [02:57<09:44,  1.57it/s][A
 40%|████      | 617/1531 [02:58<10:02,  1.52it/s][A
 40%|████      | 618/1531 [02:58<10:23,  1.46it/s][A
 40%|████      | 619/1531 [02:59<10:31,  1.44it/s][A
 40%|████      | 620/1531 [03:00<11:09,  1.36it/s][A
 41%|████      | 621/1531 [03:01<11:37,  1.30it/s][A
 41%|████      | 622/1531 [03:01<11:23,  1.33it/s][A
 41%|████      | 623/1531 [03:02<11:18,  1.34it/s][A
 41%|████      | 624/1531 [03:03<11:14,  1.34it/s][A
 41%|████      | 625/1531 [03:04<11:37,  1.30it/s][A
 41%|████      | 626/1531 [03:05<11:29,  1.31it/s][A
 41%|████      | 627/1531 [03:05<11:16,  1.34it/s][A
 41%|████      | 628/1531 [03:06<11:10,  1.35it/s][A
 41%|████      | 629/1531 [03:07<11:36,  1.29it/s][A
 41%|████      | 630/1531 [03:08<11:20,  1.32it/s][A
 41%|████      | 631/1531 [03:08<11:14,  1.33it/s][A
 41%|████▏     | 632/1531 [03:09<11:07,  1.35it/s][A
 41%|████▏     | 633/1531 [03:10<11:00,  1.36it/s][A
 41%|████▏     | 634/1531 [03:10<10:58,  1.36it/s][A
 41%|████▏     | 635/1531 [03:11<10:18,  1.45it/s][A
 42%|████▏     | 636/1531 [03:12<09:50,  1.52it/s][A
 42%|████▏     | 637/1531 [03:12<10:14,  1.46it/s][A
 42%|████▏     | 638/1531 [03:13<09:58,  1.49it/s][A
 42%|████▏     | 639/1531 [03:14<10:06,  1.47it/s][A
 42%|████▏     | 640/1531 [03:14<10:09,  1.46it/s][A
 42%|████▏     | 641/1531 [03:15<09:41,  1.53it/s][A
 42%|████▏     | 642/1531 [03:16<09:50,  1.51it/s][A
 42%|████▏     | 643/1531 [03:16<09:55,  1.49it/s][A
 42%|████▏     | 644/1531 [03:17<09:31,  1.55it/s][A
 42%|████▏     | 645/1531 [03:18<09:52,  1.50it/s][A
 42%|████▏     | 646/1531 [03:18<09:29,  1.55it/s][A
 42%|████▏     | 647/1531 [03:19<09:47,  1.50it/s][A
 42%|████▏     | 648/1531 [03:20<09:20,  1.58it/s][A
 42%|████▏     | 649/1531 [03:20<09:06,  1.61it/s][A
 42%|████▏     | 650/1531 [03:21<09:11,  1.60it/s][A
 43%|████▎     | 651/1531 [03:21<09:15,  1.58it/s][A
 43%|████▎     | 652/1531 [03:22<09:39,  1.52it/s][A
 43%|████▎     | 653/1531 [03:23<09:58,  1.47it/s][A
 43%|████▎     | 654/1531 [03:24<10:15,  1.43it/s][A
 43%|████▎     | 655/1531 [03:24<10:15,  1.42it/s][A
 43%|████▎     | 656/1531 [03:25<10:25,  1.40it/s][A
 43%|████▎     | 657/1531 [03:26<10:06,  1.44it/s][A
 43%|████▎     | 658/1531 [03:26<10:20,  1.41it/s][A
 43%|████▎     | 659/1531 [03:27<09:50,  1.48it/s][A
 43%|████▎     | 660/1531 [03:28<09:25,  1.54it/s][A
 43%|████▎     | 661/1531 [03:28<07:34,  1.92it/s][A
 43%|████▎     | 662/1531 [03:28<06:15,  2.32it/s][A
 43%|████▎     | 663/1531 [03:28<05:20,  2.71it/s][A
 43%|████▎     | 664/1531 [03:29<04:40,  3.09it/s][A
 43%|████▎     | 665/1531 [03:29<04:12,  3.43it/s][A
 44%|████▎     | 666/1531 [03:29<03:52,  3.71it/s][A
 44%|████▎     | 667/1531 [03:29<03:39,  3.93it/s][A
 44%|████▎     | 668/1531 [03:29<03:31,  4.09it/s][A
 44%|████▎     | 669/1531 [03:30<03:24,  4.21it/s][A
 44%|████▍     | 670/1531 [03:30<03:20,  4.29it/s][A
 44%|████▍     | 671/1531 [03:30<03:17,  4.36it/s][A
 44%|████▍     | 672/1531 [03:30<03:18,  4.34it/s][A
 44%|████▍     | 673/1531 [03:30<03:15,  4.40it/s][A
 44%|████▍     | 674/1531 [03:31<03:12,  4.46it/s][A
 44%|████▍     | 675/1531 [03:31<03:11,  4.47it/s][A
 44%|████▍     | 676/1531 [03:31<03:10,  4.48it/s][A
 44%|████▍     | 677/1531 [03:31<03:10,  4.49it/s][A
 44%|████▍     | 678/1531 [03:32<03:06,  4.56it/s][A
 44%|████▍     | 679/1531 [03:32<03:08,  4.52it/s][A
 44%|████▍     | 680/1531 [03:32<03:08,  4.52it/s][A
 44%|████▍     | 681/1531 [03:32<03:09,  4.48it/s][A
 45%|████▍     | 682/1531 [03:32<03:07,  4.52it/s][A
 45%|████▍     | 683/1531 [03:33<03:06,  4.53it/s][A
 45%|████▍     | 684/1531 [03:33<03:13,  4.39it/s][A
 45%|████▍     | 685/1531 [03:33<03:10,  4.45it/s][A
 45%|████▍     | 686/1531 [03:33<03:14,  4.35it/s][A
 45%|████▍     | 687/1531 [03:34<03:17,  4.27it/s][A
 45%|████▍     | 688/1531 [03:34<03:19,  4.23it/s][A
 45%|████▌     | 689/1531 [03:34<03:14,  4.33it/s][A
 45%|████▌     | 690/1531 [03:34<03:17,  4.27it/s][A
 45%|████▌     | 691/1531 [03:35<03:12,  4.36it/s][A
 45%|████▌     | 692/1531 [03:35<03:16,  4.28it/s][A
 45%|████▌     | 693/1531 [03:35<03:38,  3.83it/s][A
 45%|████▌     | 694/1531 [03:35<03:33,  3.92it/s][A
 45%|████▌     | 695/1531 [03:36<03:23,  4.10it/s][A
 45%|████▌     | 696/1531 [03:36<03:41,  3.78it/s][A
 46%|████▌     | 697/1531 [03:36<03:52,  3.58it/s][A
 46%|████▌     | 698/1531 [03:37<04:00,  3.46it/s][A
 46%|████▌     | 699/1531 [03:37<04:07,  3.36it/s][A
 46%|████▌     | 700/1531 [03:37<04:11,  3.30it/s][A
 46%|████▌     | 701/1531 [03:37<04:13,  3.27it/s][A
 46%|████▌     | 702/1531 [03:38<04:16,  3.23it/s][A
 46%|████▌     | 703/1531 [03:38<04:16,  3.23it/s][A
 46%|████▌     | 704/1531 [03:38<04:19,  3.19it/s][A
 46%|████▌     | 705/1531 [03:39<04:19,  3.19it/s][A
 46%|████▌     | 706/1531 [03:39<04:18,  3.19it/s][A
 46%|████▌     | 707/1531 [03:39<04:19,  3.18it/s][A
 46%|████▌     | 708/1531 [03:40<04:19,  3.17it/s][A
 46%|████▋     | 709/1531 [03:40<04:01,  3.40it/s][A
 46%|████▋     | 710/1531 [03:40<03:49,  3.57it/s][A
 46%|████▋     | 711/1531 [03:40<03:42,  3.69it/s][A
 47%|████▋     | 712/1531 [03:41<03:36,  3.79it/s][A
 47%|████▋     | 713/1531 [03:41<03:30,  3.89it/s][A
 47%|████▋     | 714/1531 [03:41<03:28,  3.92it/s][A
 47%|████▋     | 715/1531 [03:41<03:26,  3.96it/s][A
 47%|████▋     | 716/1531 [03:42<03:25,  3.97it/s][A
 47%|████▋     | 717/1531 [03:42<03:25,  3.97it/s][A
 47%|████▋     | 718/1531 [03:42<03:23,  4.00it/s][A
 47%|████▋     | 719/1531 [03:42<03:22,  4.01it/s][A
 47%|████▋     | 720/1531 [03:43<03:22,  4.01it/s][A
 47%|████▋     | 721/1531 [03:43<03:21,  4.01it/s][A
 47%|████▋     | 722/1531 [03:43<03:21,  4.02it/s][A
 47%|████▋     | 723/1531 [03:43<03:21,  4.01it/s][A
 47%|████▋     | 724/1531 [03:44<03:22,  3.98it/s][A
 47%|████▋     | 725/1531 [03:44<03:24,  3.95it/s][A
 47%|████▋     | 726/1531 [03:44<03:24,  3.93it/s][A
 47%|████▋     | 727/1531 [03:44<03:22,  3.97it/s][A
 48%|████▊     | 728/1531 [03:45<03:20,  4.00it/s][A
 48%|████▊     | 729/1531 [03:45<03:19,  4.03it/s][A
 48%|████▊     | 730/1531 [03:45<03:18,  4.04it/s][A
 48%|████▊     | 731/1531 [03:45<03:18,  4.03it/s][A
 48%|████▊     | 732/1531 [03:46<03:20,  3.99it/s][A
 48%|████▊     | 733/1531 [03:46<03:19,  3.99it/s][A
 48%|████▊     | 734/1531 [03:46<03:18,  4.01it/s][A
 48%|████▊     | 735/1531 [03:46<03:20,  3.97it/s][A
 48%|████▊     | 736/1531 [03:47<03:19,  3.98it/s][A
 48%|████▊     | 737/1531 [03:47<03:21,  3.94it/s][A
 48%|████▊     | 738/1531 [03:47<03:36,  3.66it/s][A
 48%|████▊     | 739/1531 [03:48<03:45,  3.51it/s][A
 48%|████▊     | 740/1531 [03:48<03:52,  3.41it/s][A
 48%|████▊     | 741/1531 [03:48<03:57,  3.33it/s][A
 48%|████▊     | 742/1531 [03:49<03:58,  3.31it/s][A
 49%|████▊     | 743/1531 [03:49<03:59,  3.29it/s][A
 49%|████▊     | 744/1531 [03:49<04:00,  3.27it/s][A
 49%|████▊     | 745/1531 [03:49<04:01,  3.26it/s][A
 49%|████▊     | 746/1531 [03:50<04:11,  3.12it/s][A
 49%|████▉     | 747/1531 [03:50<04:09,  3.14it/s][A
 49%|████▉     | 748/1531 [03:50<04:09,  3.13it/s][A
 49%|████▉     | 749/1531 [03:51<03:41,  3.53it/s][A
 49%|████▉     | 750/1531 [03:51<03:19,  3.91it/s][A
 49%|████▉     | 751/1531 [03:51<03:04,  4.22it/s][A
 49%|████▉     | 752/1531 [03:51<02:54,  4.47it/s][A
 49%|████▉     | 753/1531 [03:51<02:49,  4.60it/s][A
 49%|████▉     | 754/1531 [03:52<02:44,  4.73it/s][A
 49%|████▉     | 755/1531 [03:52<02:39,  4.87it/s][A
 49%|████▉     | 756/1531 [03:52<02:36,  4.94it/s][A
 49%|████▉     | 757/1531 [03:52<02:40,  4.82it/s][A
 50%|████▉     | 758/1531 [03:52<02:43,  4.73it/s][A
 50%|████▉     | 759/1531 [03:53<02:39,  4.84it/s][A
 50%|████▉     | 760/1531 [03:53<02:48,  4.57it/s][A
 50%|████▉     | 761/1531 [03:53<02:55,  4.38it/s][A
 50%|████▉     | 762/1531 [03:53<02:59,  4.29it/s][A
 50%|████▉     | 763/1531 [03:54<03:03,  4.19it/s][A
 50%|████▉     | 764/1531 [03:54<03:05,  4.14it/s][A
 50%|████▉     | 765/1531 [03:54<03:06,  4.10it/s][A
 50%|█████     | 766/1531 [03:54<03:06,  4.11it/s][A
 50%|█████     | 767/1531 [03:55<03:07,  4.08it/s][A
 50%|█████     | 768/1531 [03:55<03:08,  4.05it/s][A
 50%|█████     | 769/1531 [03:55<03:08,  4.05it/s][A
 50%|█████     | 770/1531 [03:55<03:08,  4.04it/s][A
 50%|█████     | 771/1531 [03:56<03:09,  4.01it/s][A
 50%|█████     | 772/1531 [03:56<03:09,  4.01it/s][A
 50%|█████     | 773/1531 [03:56<03:08,  4.02it/s][A
 51%|█████     | 774/1531 [03:56<03:07,  4.03it/s][A
 51%|█████     | 775/1531 [03:57<03:07,  4.04it/s][A
 51%|█████     | 776/1531 [03:57<03:07,  4.03it/s][A
 51%|█████     | 777/1531 [03:57<03:07,  4.03it/s][A
 51%|█████     | 778/1531 [03:57<03:07,  4.03it/s][A
 51%|█████     | 779/1531 [03:58<03:07,  4.01it/s][A
 51%|█████     | 780/1531 [03:58<03:08,  3.99it/s][A
 51%|█████     | 781/1531 [03:58<03:07,  4.00it/s][A
 51%|█████     | 782/1531 [03:58<03:06,  4.01it/s][A
 51%|█████     | 783/1531 [03:59<03:06,  4.01it/s][A
 51%|█████     | 784/1531 [03:59<03:05,  4.03it/s][A
 51%|█████▏    | 785/1531 [03:59<03:04,  4.04it/s][A
 51%|█████▏    | 786/1531 [03:59<03:02,  4.09it/s][A
 51%|█████▏    | 787/1531 [04:00<03:00,  4.12it/s][A
 51%|█████▏    | 788/1531 [04:00<02:55,  4.23it/s][A
 52%|█████▏    | 789/1531 [04:00<02:58,  4.15it/s][A
 52%|█████▏    | 790/1531 [04:00<03:00,  4.10it/s][A
 52%|█████▏    | 791/1531 [04:01<03:00,  4.11it/s][A
 52%|█████▏    | 792/1531 [04:01<03:00,  4.09it/s][A
 52%|█████▏    | 793/1531 [04:01<03:00,  4.08it/s][A
 52%|█████▏    | 794/1531 [04:01<02:58,  4.12it/s][A
 52%|█████▏    | 795/1531 [04:01<02:58,  4.12it/s][A
 52%|█████▏    | 796/1531 [04:02<02:53,  4.23it/s][A
 52%|█████▏    | 797/1531 [04:02<02:44,  4.46it/s][A
 52%|█████▏    | 798/1531 [04:02<02:43,  4.48it/s][A
 52%|█████▏    | 799/1531 [04:02<02:42,  4.50it/s][A
 52%|█████▏    | 800/1531 [04:03<02:40,  4.55it/s][A
 52%|█████▏    | 801/1531 [04:03<02:40,  4.56it/s][A
 52%|█████▏    | 802/1531 [04:03<02:38,  4.59it/s][A
 52%|█████▏    | 803/1531 [04:03<02:34,  4.72it/s][A
 53%|█████▎    | 804/1531 [04:03<02:29,  4.86it/s][A
 53%|█████▎    | 805/1531 [04:04<02:32,  4.77it/s][A
 53%|█████▎    | 806/1531 [04:04<02:34,  4.69it/s][A
 53%|█████▎    | 807/1531 [04:04<02:36,  4.63it/s][A
 53%|█████▎    | 808/1531 [04:04<02:37,  4.58it/s][A
 53%|█████▎    | 809/1531 [04:04<02:37,  4.59it/s][A
 53%|█████▎    | 810/1531 [04:05<02:32,  4.73it/s][A
 53%|█████▎    | 811/1531 [04:05<02:35,  4.64it/s][A
 53%|█████▎    | 812/1531 [04:05<02:36,  4.59it/s][A
 53%|█████▎    | 813/1531 [04:05<02:37,  4.57it/s][A
 53%|█████▎    | 814/1531 [04:06<02:32,  4.71it/s][A
 53%|█████▎    | 815/1531 [04:06<02:32,  4.68it/s][A
 53%|█████▎    | 816/1531 [04:06<02:33,  4.65it/s][A
 53%|█████▎    | 817/1531 [04:06<02:34,  4.63it/s][A
 53%|█████▎    | 818/1531 [04:06<02:33,  4.64it/s][A
 53%|█████▎    | 819/1531 [04:07<02:34,  4.60it/s][A
 54%|█████▎    | 820/1531 [04:07<02:35,  4.58it/s][A
 54%|█████▎    | 821/1531 [04:07<02:35,  4.56it/s][A
 54%|█████▎    | 822/1531 [04:07<02:36,  4.53it/s][A
 54%|█████▍    | 823/1531 [04:08<02:34,  4.57it/s][A
 54%|█████▍    | 824/1531 [04:08<02:34,  4.56it/s][A
 54%|█████▍    | 825/1531 [04:08<02:35,  4.55it/s][A
 54%|█████▍    | 826/1531 [04:08<02:35,  4.55it/s][A
 54%|█████▍    | 827/1531 [04:08<02:34,  4.55it/s][A
 54%|█████▍    | 828/1531 [04:09<02:34,  4.55it/s][A
 54%|█████▍    | 829/1531 [04:09<02:34,  4.55it/s][A
 54%|█████▍    | 830/1531 [04:09<02:34,  4.54it/s][A
 54%|█████▍    | 831/1531 [04:09<02:29,  4.68it/s][A
 54%|█████▍    | 832/1531 [04:09<02:30,  4.64it/s][A
 54%|█████▍    | 833/1531 [04:10<02:30,  4.63it/s][A
 54%|█████▍    | 834/1531 [04:10<02:31,  4.60it/s][A
 55%|█████▍    | 835/1531 [04:10<02:32,  4.57it/s][A
 55%|█████▍    | 836/1531 [04:10<02:32,  4.57it/s][A
 55%|█████▍    | 837/1531 [04:11<02:32,  4.55it/s][A
 55%|█████▍    | 838/1531 [04:11<02:32,  4.54it/s][A
 55%|█████▍    | 839/1531 [04:11<02:32,  4.54it/s][A
 55%|█████▍    | 840/1531 [04:11<02:31,  4.55it/s][A
 55%|█████▍    | 841/1531 [04:11<02:26,  4.71it/s][A
 55%|█████▍    | 842/1531 [04:12<02:27,  4.66it/s][A
 55%|█████▌    | 843/1531 [04:12<02:27,  4.65it/s][A
 55%|█████▌    | 844/1531 [04:12<02:28,  4.63it/s][A
 55%|█████▌    | 845/1531 [04:12<02:28,  4.63it/s][A
 55%|█████▌    | 846/1531 [04:13<02:28,  4.60it/s][A
 55%|█████▌    | 847/1531 [04:13<02:29,  4.59it/s][A
 55%|█████▌    | 848/1531 [04:13<02:33,  4.45it/s][A
 55%|█████▌    | 849/1531 [04:13<02:32,  4.48it/s][A
 56%|█████▌    | 850/1531 [04:13<02:26,  4.66it/s][A
 56%|█████▌    | 851/1531 [04:14<02:22,  4.77it/s][A
 56%|█████▌    | 852/1531 [04:14<02:24,  4.70it/s][A
 56%|█████▌    | 853/1531 [04:14<02:25,  4.65it/s][A
 56%|█████▌    | 854/1531 [04:14<02:26,  4.62it/s][A
 56%|█████▌    | 855/1531 [04:14<02:27,  4.60it/s][A
 56%|█████▌    | 856/1531 [04:15<02:27,  4.58it/s][A
 56%|█████▌    | 857/1531 [04:15<02:27,  4.58it/s][A
 56%|█████▌    | 858/1531 [04:15<02:28,  4.54it/s][A
 56%|█████▌    | 859/1531 [04:15<02:28,  4.54it/s][A
 56%|█████▌    | 860/1531 [04:16<02:27,  4.55it/s][A
 56%|█████▌    | 861/1531 [04:16<02:31,  4.41it/s][A
 56%|█████▋    | 862/1531 [04:16<02:30,  4.45it/s][A
 56%|█████▋    | 863/1531 [04:16<02:28,  4.49it/s][A
 56%|█████▋    | 864/1531 [04:16<02:27,  4.51it/s][A
 56%|█████▋    | 865/1531 [04:17<02:31,  4.39it/s][A
 57%|█████▋    | 866/1531 [04:17<02:30,  4.42it/s][A
 57%|█████▋    | 867/1531 [04:17<02:29,  4.44it/s][A
 57%|█████▋    | 868/1531 [04:17<02:28,  4.47it/s][A
 57%|█████▋    | 869/1531 [04:18<02:27,  4.49it/s][A
 57%|█████▋    | 870/1531 [04:18<02:26,  4.51it/s][A
 57%|█████▋    | 871/1531 [04:18<02:25,  4.52it/s][A
 57%|█████▋    | 872/1531 [04:18<02:25,  4.53it/s][A
 57%|█████▋    | 873/1531 [04:18<02:25,  4.51it/s][A
 57%|█████▋    | 874/1531 [04:19<02:25,  4.51it/s][A
 57%|█████▋    | 875/1531 [04:19<02:25,  4.52it/s][A
 57%|█████▋    | 876/1531 [04:19<02:25,  4.50it/s][A
 57%|█████▋    | 877/1531 [04:19<02:23,  4.56it/s][A
 57%|█████▋    | 878/1531 [04:20<02:22,  4.57it/s][A
 57%|█████▋    | 879/1531 [04:20<02:22,  4.56it/s][A
 57%|█████▋    | 880/1531 [04:20<02:22,  4.55it/s][A
 58%|█████▊    | 881/1531 [04:20<02:17,  4.71it/s][A
 58%|█████▊    | 882/1531 [04:20<02:26,  4.42it/s][A
 58%|█████▊    | 883/1531 [04:21<02:30,  4.30it/s][A
 58%|█████▊    | 884/1531 [04:21<02:35,  4.17it/s][A
 58%|█████▊    | 885/1531 [04:21<02:38,  4.08it/s][A
 58%|█████▊    | 886/1531 [04:22<02:40,  4.01it/s][A
 58%|█████▊    | 887/1531 [04:22<02:41,  3.99it/s][A
 58%|█████▊    | 888/1531 [04:22<02:41,  3.99it/s][A
 58%|█████▊    | 889/1531 [04:22<02:42,  3.95it/s][A
 58%|█████▊    | 890/1531 [04:23<02:44,  3.91it/s][A
 58%|█████▊    | 891/1531 [04:23<02:44,  3.89it/s][A
 58%|█████▊    | 892/1531 [04:23<02:44,  3.90it/s][A
 58%|█████▊    | 893/1531 [04:23<02:44,  3.89it/s][A
 58%|█████▊    | 894/1531 [04:24<02:44,  3.87it/s][A
 58%|█████▊    | 895/1531 [04:24<02:44,  3.86it/s][A
 59%|█████▊    | 896/1531 [04:24<02:42,  3.90it/s][A
 59%|█████▊    | 897/1531 [04:24<02:44,  3.84it/s][A
 59%|█████▊    | 898/1531 [04:25<02:44,  3.84it/s][A
 59%|█████▊    | 899/1531 [04:25<02:44,  3.85it/s][A
 59%|█████▉    | 900/1531 [04:25<02:44,  3.85it/s][A
 59%|█████▉    | 901/1531 [04:25<02:44,  3.84it/s][A
 59%|█████▉    | 902/1531 [04:26<02:43,  3.84it/s][A
 59%|█████▉    | 903/1531 [04:26<02:41,  3.89it/s][A
 59%|█████▉    | 904/1531 [04:26<02:40,  3.90it/s][A
 59%|█████▉    | 905/1531 [04:26<02:41,  3.89it/s][A
 59%|█████▉    | 906/1531 [04:27<02:42,  3.85it/s][A
 59%|█████▉    | 907/1531 [04:27<02:42,  3.85it/s][A
 59%|█████▉    | 908/1531 [04:27<02:40,  3.88it/s][A
 59%|█████▉    | 909/1531 [04:27<02:40,  3.87it/s][A
 59%|█████▉    | 910/1531 [04:28<02:39,  3.90it/s][A
 60%|█████▉    | 911/1531 [04:28<02:39,  3.88it/s][A
 60%|█████▉    | 912/1531 [04:28<02:40,  3.87it/s][A
 60%|█████▉    | 913/1531 [04:28<02:40,  3.86it/s][A
 60%|█████▉    | 914/1531 [04:29<02:39,  3.86it/s][A
 60%|█████▉    | 915/1531 [04:29<02:39,  3.86it/s][A
 60%|█████▉    | 916/1531 [04:29<02:39,  3.84it/s][A
 60%|█████▉    | 917/1531 [04:30<02:39,  3.85it/s][A
 60%|█████▉    | 918/1531 [04:30<02:38,  3.86it/s][A
 60%|██████    | 919/1531 [04:30<02:36,  3.90it/s][A
 60%|██████    | 920/1531 [04:30<02:48,  3.63it/s][A
 60%|██████    | 921/1531 [04:31<02:55,  3.47it/s][A
 60%|██████    | 922/1531 [04:31<03:00,  3.38it/s][A
 60%|██████    | 923/1531 [04:31<03:04,  3.29it/s][A
 60%|██████    | 924/1531 [04:32<03:06,  3.25it/s][A
 60%|██████    | 925/1531 [04:32<03:08,  3.22it/s][A
 60%|██████    | 926/1531 [04:32<03:09,  3.19it/s][A
 61%|██████    | 927/1531 [04:33<03:09,  3.18it/s][A
 61%|██████    | 928/1531 [04:33<03:10,  3.17it/s][A
 61%|██████    | 929/1531 [04:33<03:10,  3.17it/s][A
 61%|██████    | 930/1531 [04:34<03:09,  3.16it/s][A
 61%|██████    | 931/1531 [04:34<03:09,  3.16it/s][A
 61%|██████    | 932/1531 [04:34<03:09,  3.16it/s][A
 61%|██████    | 933/1531 [04:34<03:08,  3.17it/s][A
 61%|██████    | 934/1531 [04:35<03:08,  3.17it/s][A
 61%|██████    | 935/1531 [04:35<03:14,  3.06it/s][A
 61%|██████    | 936/1531 [04:35<03:12,  3.10it/s][A
 61%|██████    | 937/1531 [04:36<03:10,  3.11it/s][A
 61%|██████▏   | 938/1531 [04:36<03:10,  3.12it/s][A
 61%|██████▏   | 939/1531 [04:36<03:08,  3.14it/s][A
 61%|██████▏   | 940/1531 [04:37<03:08,  3.14it/s][A
 61%|██████▏   | 941/1531 [04:37<03:07,  3.14it/s][A
 62%|██████▏   | 942/1531 [04:37<03:06,  3.15it/s][A
 62%|██████▏   | 943/1531 [04:38<03:06,  3.15it/s][A
 62%|██████▏   | 944/1531 [04:38<03:06,  3.15it/s][A
 62%|██████▏   | 945/1531 [04:38<03:07,  3.13it/s][A
 62%|██████▏   | 946/1531 [04:39<03:05,  3.15it/s][A
 62%|██████▏   | 947/1531 [04:39<03:05,  3.16it/s][A
 62%|██████▏   | 948/1531 [04:39<03:04,  3.15it/s][A
 62%|██████▏   | 949/1531 [04:40<03:04,  3.15it/s][A
 62%|██████▏   | 950/1531 [04:40<03:04,  3.15it/s][A
 62%|██████▏   | 951/1531 [04:40<03:03,  3.16it/s][A
 62%|██████▏   | 952/1531 [04:41<03:03,  3.15it/s][A
 62%|██████▏   | 953/1531 [04:41<03:03,  3.16it/s][A
 62%|██████▏   | 954/1531 [04:41<03:02,  3.16it/s][A
 62%|██████▏   | 955/1531 [04:41<03:02,  3.16it/s][A
 62%|██████▏   | 956/1531 [04:42<03:02,  3.16it/s][A
 63%|██████▎   | 957/1531 [04:42<03:02,  3.15it/s][A
 63%|██████▎   | 958/1531 [04:42<03:00,  3.17it/s][A
 63%|██████▎   | 959/1531 [04:43<03:00,  3.17it/s][A
 63%|██████▎   | 960/1531 [04:43<03:00,  3.16it/s][A
 63%|██████▎   | 961/1531 [04:43<02:59,  3.17it/s][A
 63%|██████▎   | 962/1531 [04:44<02:59,  3.17it/s][A
 63%|██████▎   | 963/1531 [04:44<03:04,  3.08it/s][A
 63%|██████▎   | 964/1531 [04:44<03:03,  3.10it/s][A
 63%|██████▎   | 965/1531 [04:45<03:02,  3.11it/s][A
 63%|██████▎   | 966/1531 [04:45<03:00,  3.13it/s][A
 63%|██████▎   | 967/1531 [04:45<02:59,  3.14it/s][A
 63%|██████▎   | 968/1531 [04:46<02:58,  3.15it/s][A
 63%|██████▎   | 969/1531 [04:46<02:58,  3.15it/s][A
 63%|██████▎   | 970/1531 [04:46<02:57,  3.17it/s][A
 63%|██████▎   | 971/1531 [04:47<02:57,  3.16it/s][A
 63%|██████▎   | 972/1531 [04:47<02:57,  3.15it/s][A
 64%|██████▎   | 973/1531 [04:47<02:57,  3.15it/s][A
 64%|██████▎   | 974/1531 [04:48<02:56,  3.16it/s][A
 64%|██████▎   | 975/1531 [04:48<02:56,  3.16it/s][A
 64%|██████▎   | 976/1531 [04:48<02:57,  3.13it/s][A
 64%|██████▍   | 977/1531 [04:48<02:56,  3.14it/s][A
 64%|██████▍   | 978/1531 [04:49<02:55,  3.15it/s][A
 64%|██████▍   | 979/1531 [04:49<02:55,  3.15it/s][A
 64%|██████▍   | 980/1531 [04:49<02:55,  3.14it/s][A
 64%|██████▍   | 981/1531 [04:50<02:54,  3.16it/s][A
 64%|██████▍   | 982/1531 [04:50<02:53,  3.16it/s][A
 64%|██████▍   | 983/1531 [04:50<02:53,  3.15it/s][A
 64%|██████▍   | 984/1531 [04:51<02:53,  3.16it/s][A
 64%|██████▍   | 985/1531 [04:51<02:54,  3.12it/s][A
 64%|██████▍   | 986/1531 [04:51<02:53,  3.14it/s][A
 64%|██████▍   | 987/1531 [04:52<02:53,  3.14it/s][A
 65%|██████▍   | 988/1531 [04:52<02:51,  3.16it/s][A
 65%|██████▍   | 989/1531 [04:52<02:52,  3.14it/s][A
 65%|██████▍   | 990/1531 [04:53<02:52,  3.14it/s][A
 65%|██████▍   | 991/1531 [04:53<02:51,  3.15it/s][A
 65%|██████▍   | 992/1531 [04:53<02:51,  3.14it/s][A
 65%|██████▍   | 993/1531 [04:54<02:56,  3.05it/s][A
 65%|██████▍   | 994/1531 [04:54<02:54,  3.08it/s][A
 65%|██████▍   | 995/1531 [04:54<02:52,  3.11it/s][A
 65%|██████▌   | 996/1531 [04:55<02:51,  3.12it/s][A
 65%|██████▌   | 997/1531 [04:55<02:49,  3.14it/s][A
 65%|██████▌   | 998/1531 [04:55<02:49,  3.15it/s][A
 65%|██████▌   | 999/1531 [04:55<02:48,  3.16it/s][A
 65%|██████▌   | 1000/1531 [04:56<02:53,  3.05it/s][A
 65%|██████▌   | 1001/1531 [04:56<02:52,  3.07it/s][A
 65%|██████▌   | 1002/1531 [04:56<02:50,  3.10it/s][A
 66%|██████▌   | 1003/1531 [04:57<02:49,  3.11it/s][A
 66%|██████▌   | 1004/1531 [04:57<02:48,  3.13it/s][A
 66%|██████▌   | 1005/1531 [04:57<02:47,  3.15it/s][A
 66%|██████▌   | 1006/1531 [04:58<02:46,  3.15it/s][A
 66%|██████▌   | 1007/1531 [04:58<02:50,  3.07it/s][A
 66%|██████▌   | 1008/1531 [04:58<02:48,  3.10it/s][A
 66%|██████▌   | 1009/1531 [04:59<02:47,  3.11it/s][A
 66%|██████▌   | 1010/1531 [04:59<02:46,  3.13it/s][A
 66%|██████▌   | 1011/1531 [04:59<02:45,  3.14it/s][A
 66%|██████▌   | 1012/1531 [05:00<02:45,  3.14it/s][A
 66%|██████▌   | 1013/1531 [05:00<02:44,  3.15it/s][A
 66%|██████▌   | 1014/1531 [05:00<02:43,  3.15it/s][A
 66%|██████▋   | 1015/1531 [05:01<02:43,  3.15it/s][A
 66%|██████▋   | 1016/1531 [05:01<02:42,  3.17it/s][A
 66%|██████▋   | 1017/1531 [05:01<02:42,  3.17it/s][A
 66%|██████▋   | 1018/1531 [05:02<02:42,  3.16it/s][A
 67%|██████▋   | 1019/1531 [05:02<02:41,  3.16it/s][A
 67%|██████▋   | 1020/1531 [05:02<02:40,  3.18it/s][A
 67%|██████▋   | 1021/1531 [05:03<02:39,  3.19it/s][A
 67%|██████▋   | 1022/1531 [05:03<02:38,  3.21it/s][A
 67%|██████▋   | 1023/1531 [05:03<02:36,  3.25it/s][A
 67%|██████▋   | 1024/1531 [05:03<02:35,  3.25it/s][A
 67%|██████▋   | 1025/1531 [05:04<02:34,  3.27it/s][A
 67%|██████▋   | 1026/1531 [05:04<02:34,  3.27it/s][A
 67%|██████▋   | 1027/1531 [05:04<02:33,  3.28it/s][A
 67%|██████▋   | 1028/1531 [05:05<02:33,  3.28it/s][A
 67%|██████▋   | 1029/1531 [05:05<02:33,  3.28it/s][A
 67%|██████▋   | 1030/1531 [05:05<02:33,  3.27it/s][A
 67%|██████▋   | 1031/1531 [05:06<02:33,  3.26it/s][A
 67%|██████▋   | 1032/1531 [05:06<02:32,  3.27it/s][A
 67%|██████▋   | 1033/1531 [05:06<02:33,  3.25it/s][A
 68%|██████▊   | 1034/1531 [05:06<02:32,  3.27it/s][A
 68%|██████▊   | 1035/1531 [05:07<02:32,  3.26it/s][A
 68%|██████▊   | 1036/1531 [05:07<02:31,  3.27it/s][A
 68%|██████▊   | 1037/1531 [05:07<02:30,  3.28it/s][A
 68%|██████▊   | 1038/1531 [05:08<02:30,  3.27it/s][A
 68%|██████▊   | 1039/1531 [05:08<02:30,  3.27it/s][A
 68%|██████▊   | 1040/1531 [05:08<02:30,  3.27it/s][A
 68%|██████▊   | 1041/1531 [05:09<02:29,  3.28it/s][A
 68%|██████▊   | 1042/1531 [05:09<02:29,  3.28it/s][A
 68%|██████▊   | 1043/1531 [05:09<02:30,  3.25it/s][A
 68%|██████▊   | 1044/1531 [05:10<02:30,  3.23it/s][A
 68%|██████▊   | 1045/1531 [05:10<02:29,  3.25it/s][A
 68%|██████▊   | 1046/1531 [05:10<02:28,  3.26it/s][A
 68%|██████▊   | 1047/1531 [05:10<02:27,  3.28it/s][A
 68%|██████▊   | 1048/1531 [05:11<02:27,  3.27it/s][A
 69%|██████▊   | 1049/1531 [05:11<02:27,  3.27it/s][A
 69%|██████▊   | 1050/1531 [05:11<02:27,  3.26it/s][A
 69%|██████▊   | 1051/1531 [05:12<02:29,  3.21it/s][A
 69%|██████▊   | 1052/1531 [05:12<02:28,  3.23it/s][A
 69%|██████▉   | 1053/1531 [05:12<02:18,  3.45it/s][A
 69%|██████▉   | 1054/1531 [05:12<02:07,  3.74it/s][A
 69%|██████▉   | 1055/1531 [05:13<01:59,  3.97it/s][A
 69%|██████▉   | 1056/1531 [05:13<01:54,  4.13it/s][A
 69%|██████▉   | 1057/1531 [05:13<01:51,  4.24it/s][A
 69%|██████▉   | 1058/1531 [05:13<01:54,  4.14it/s][A
 69%|██████▉   | 1059/1531 [05:14<01:53,  4.14it/s][A
 69%|██████▉   | 1060/1531 [05:14<01:50,  4.27it/s][A
 69%|██████▉   | 1061/1531 [05:14<01:48,  4.34it/s][A
 69%|██████▉   | 1062/1531 [05:14<01:46,  4.39it/s][A
 69%|██████▉   | 1063/1531 [05:14<01:45,  4.45it/s][A
 69%|██████▉   | 1064/1531 [05:15<01:44,  4.46it/s][A
 70%|██████▉   | 1065/1531 [05:15<01:44,  4.47it/s][A
 70%|██████▉   | 1066/1531 [05:15<01:46,  4.36it/s][A
 70%|██████▉   | 1067/1531 [05:15<01:44,  4.43it/s][A
 70%|██████▉   | 1068/1531 [05:16<01:43,  4.48it/s][A
 70%|██████▉   | 1069/1531 [05:16<01:42,  4.51it/s][A
 70%|██████▉   | 1070/1531 [05:16<01:41,  4.54it/s][A
 70%|██████▉   | 1071/1531 [05:16<01:40,  4.57it/s][A
 70%|███████   | 1072/1531 [05:16<01:39,  4.59it/s][A
 70%|███████   | 1073/1531 [05:17<01:39,  4.58it/s][A
 70%|███████   | 1074/1531 [05:17<01:39,  4.57it/s][A
 70%|███████   | 1075/1531 [05:17<01:39,  4.58it/s][A
 70%|███████   | 1076/1531 [05:17<01:39,  4.56it/s][A
 70%|███████   | 1077/1531 [05:18<01:39,  4.56it/s][A
 70%|███████   | 1078/1531 [05:18<01:39,  4.54it/s][A
 70%|███████   | 1079/1531 [05:18<01:39,  4.52it/s][A
 71%|███████   | 1080/1531 [05:18<01:38,  4.57it/s][A
 71%|███████   | 1081/1531 [05:18<01:38,  4.57it/s][A
 71%|███████   | 1082/1531 [05:19<01:37,  4.59it/s][A
 71%|███████   | 1083/1531 [05:19<01:39,  4.49it/s][A
 71%|███████   | 1084/1531 [05:19<01:41,  4.38it/s][A
 71%|███████   | 1085/1531 [05:19<01:44,  4.27it/s][A
 71%|███████   | 1086/1531 [05:20<01:42,  4.36it/s][A
 71%|███████   | 1087/1531 [05:20<01:46,  4.19it/s][A
 71%|███████   | 1088/1531 [05:20<01:49,  4.05it/s][A
 71%|███████   | 1089/1531 [05:20<01:50,  3.98it/s][A
 71%|███████   | 1090/1531 [05:21<01:52,  3.91it/s][A
 71%|███████▏  | 1091/1531 [05:21<01:59,  3.68it/s][A
 71%|███████▏  | 1092/1531 [05:21<02:03,  3.55it/s][A
 71%|███████▏  | 1093/1531 [05:22<02:00,  3.63it/s][A
 71%|███████▏  | 1094/1531 [05:22<01:58,  3.69it/s][A
 72%|███████▏  | 1095/1531 [05:22<01:56,  3.73it/s][A
 72%|███████▏  | 1096/1531 [05:22<01:56,  3.74it/s][A
 72%|███████▏  | 1097/1531 [05:23<01:55,  3.76it/s][A
 72%|███████▏  | 1098/1531 [05:23<01:54,  3.78it/s][A
 72%|███████▏  | 1099/1531 [05:23<01:53,  3.79it/s][A
 72%|███████▏  | 1100/1531 [05:23<01:58,  3.63it/s][A
 72%|███████▏  | 1101/1531 [05:24<01:56,  3.69it/s][A
 72%|███████▏  | 1102/1531 [05:24<01:55,  3.73it/s][A
 72%|███████▏  | 1103/1531 [05:24<01:54,  3.75it/s][A
 72%|███████▏  | 1104/1531 [05:24<01:58,  3.62it/s][A
 72%|███████▏  | 1105/1531 [05:25<01:56,  3.65it/s][A
 72%|███████▏  | 1106/1531 [05:25<01:54,  3.71it/s][A
 72%|███████▏  | 1107/1531 [05:25<01:54,  3.72it/s][A
 72%|███████▏  | 1108/1531 [05:26<01:53,  3.74it/s][A
 72%|███████▏  | 1109/1531 [05:26<01:57,  3.60it/s][A
 73%|███████▎  | 1110/1531 [05:26<02:00,  3.50it/s][A
 73%|███████▎  | 1111/1531 [05:26<02:01,  3.45it/s][A
 73%|███████▎  | 1112/1531 [05:27<01:57,  3.55it/s][A
 73%|███████▎  | 1113/1531 [05:27<01:55,  3.62it/s][A
 73%|███████▎  | 1114/1531 [05:27<01:58,  3.51it/s][A
 73%|███████▎  | 1115/1531 [05:28<01:56,  3.58it/s][A
 73%|███████▎  | 1116/1531 [05:28<01:54,  3.63it/s][A
 73%|███████▎  | 1117/1531 [05:28<01:57,  3.52it/s][A
 73%|███████▎  | 1118/1531 [05:28<02:00,  3.42it/s][A
 73%|███████▎  | 1119/1531 [05:29<01:57,  3.51it/s][A
 73%|███████▎  | 1120/1531 [05:29<01:54,  3.59it/s][A
 73%|███████▎  | 1121/1531 [05:29<01:52,  3.64it/s][A
 73%|███████▎  | 1122/1531 [05:30<02:02,  3.34it/s][A
 73%|███████▎  | 1123/1531 [05:30<02:09,  3.16it/s][A
 73%|███████▎  | 1124/1531 [05:30<02:14,  3.02it/s][A
 73%|███████▎  | 1125/1531 [05:31<02:17,  2.96it/s][A
 74%|███████▎  | 1126/1531 [05:31<02:19,  2.91it/s][A
 74%|███████▎  | 1127/1531 [05:31<02:19,  2.89it/s][A
 74%|███████▎  | 1128/1531 [05:32<02:20,  2.87it/s][A
 74%|███████▎  | 1129/1531 [05:32<02:20,  2.86it/s][A
 74%|███████▍  | 1130/1531 [05:32<02:21,  2.83it/s][A
 74%|███████▍  | 1131/1531 [05:33<02:21,  2.83it/s][A
 74%|███████▍  | 1132/1531 [05:33<02:20,  2.84it/s][A
 74%|███████▍  | 1133/1531 [05:34<02:20,  2.84it/s][A
 74%|███████▍  | 1134/1531 [05:34<02:19,  2.84it/s][A
 74%|███████▍  | 1135/1531 [05:34<02:19,  2.84it/s][A
 74%|███████▍  | 1136/1531 [05:35<02:19,  2.84it/s][A
 74%|███████▍  | 1137/1531 [05:35<02:19,  2.83it/s][A
 74%|███████▍  | 1138/1531 [05:35<02:14,  2.92it/s][A
 74%|███████▍  | 1139/1531 [05:36<02:15,  2.90it/s][A
 74%|███████▍  | 1140/1531 [05:36<02:16,  2.87it/s][A
 75%|███████▍  | 1141/1531 [05:36<02:17,  2.83it/s][A
 75%|███████▍  | 1142/1531 [05:37<02:13,  2.92it/s][A
 75%|███████▍  | 1143/1531 [05:37<02:14,  2.88it/s][A
 75%|███████▍  | 1144/1531 [05:37<02:14,  2.88it/s][A
 75%|███████▍  | 1145/1531 [05:38<02:14,  2.87it/s][A
 75%|███████▍  | 1146/1531 [05:38<02:15,  2.84it/s][A
 75%|███████▍  | 1147/1531 [05:38<02:18,  2.76it/s][A
 75%|███████▍  | 1148/1531 [05:39<02:17,  2.79it/s][A
 75%|███████▌  | 1149/1531 [05:39<02:17,  2.78it/s][A
 75%|███████▌  | 1150/1531 [05:39<02:16,  2.80it/s][A
 75%|███████▌  | 1151/1531 [05:40<02:14,  2.82it/s][A
 75%|███████▌  | 1152/1531 [05:40<02:14,  2.82it/s][A
 75%|███████▌  | 1153/1531 [05:41<02:59,  2.11it/s][A
 75%|███████▌  | 1154/1531 [05:42<03:29,  1.80it/s][A
 75%|███████▌  | 1155/1531 [05:42<03:56,  1.59it/s][A
 76%|███████▌  | 1156/1531 [05:43<04:05,  1.53it/s][A
 76%|███████▌  | 1157/1531 [05:44<04:15,  1.47it/s][A
 76%|███████▌  | 1158/1531 [05:45<04:29,  1.38it/s][A
 76%|███████▌  | 1159/1531 [05:45<04:28,  1.38it/s][A
 76%|███████▌  | 1160/1531 [05:46<04:26,  1.39it/s][A
 76%|███████▌  | 1161/1531 [05:47<04:30,  1.37it/s][A
 76%|███████▌  | 1162/1531 [05:48<04:30,  1.36it/s][A
 76%|███████▌  | 1163/1531 [05:48<04:31,  1.36it/s][A
 76%|███████▌  | 1164/1531 [05:49<04:31,  1.35it/s][A
 76%|███████▌  | 1165/1531 [05:50<04:39,  1.31it/s][A
 76%|███████▌  | 1166/1531 [05:51<04:38,  1.31it/s][A
 76%|███████▌  | 1167/1531 [05:52<04:42,  1.29it/s][A
 76%|███████▋  | 1168/1531 [05:52<04:38,  1.30it/s][A
 76%|███████▋  | 1169/1531 [05:53<04:44,  1.27it/s][A
 76%|███████▋  | 1170/1531 [05:54<04:45,  1.26it/s][A
 76%|███████▋  | 1171/1531 [05:55<04:36,  1.30it/s][A
 77%|███████▋  | 1172/1531 [05:55<04:34,  1.31it/s][A
 77%|███████▋  | 1173/1531 [05:56<04:30,  1.32it/s][A
 77%|███████▋  | 1174/1531 [05:57<04:35,  1.29it/s][A
 77%|███████▋  | 1175/1531 [05:58<04:33,  1.30it/s][A
 77%|███████▋  | 1176/1531 [05:58<04:30,  1.31it/s][A
 77%|███████▋  | 1177/1531 [05:59<04:27,  1.32it/s][A
 77%|███████▋  | 1178/1531 [06:00<04:24,  1.33it/s][A
 77%|███████▋  | 1179/1531 [06:01<04:25,  1.33it/s][A
 77%|███████▋  | 1180/1531 [06:01<04:16,  1.37it/s][A
 77%|███████▋  | 1181/1531 [06:02<04:17,  1.36it/s][A
 77%|███████▋  | 1182/1531 [06:03<04:26,  1.31it/s][A
 77%|███████▋  | 1183/1531 [06:04<04:29,  1.29it/s][A
 77%|███████▋  | 1184/1531 [06:05<04:31,  1.28it/s][A
 77%|███████▋  | 1185/1531 [06:05<04:34,  1.26it/s][A
 77%|███████▋  | 1186/1531 [06:06<04:29,  1.28it/s][A
 78%|███████▊  | 1187/1531 [06:07<04:23,  1.31it/s][A
 78%|███████▊  | 1188/1531 [06:08<04:11,  1.37it/s][A
 78%|███████▊  | 1189/1531 [06:08<04:06,  1.39it/s][A
 78%|███████▊  | 1190/1531 [06:09<04:17,  1.32it/s][A
 78%|███████▊  | 1191/1531 [06:10<04:05,  1.38it/s][A
 78%|███████▊  | 1192/1531 [06:10<04:08,  1.37it/s][A
 78%|███████▊  | 1193/1531 [06:11<04:09,  1.36it/s][A
 78%|███████▊  | 1194/1531 [06:12<04:16,  1.31it/s][A
 78%|███████▊  | 1195/1531 [06:13<04:14,  1.32it/s][A
 78%|███████▊  | 1196/1531 [06:14<04:12,  1.32it/s][A
 78%|███████▊  | 1197/1531 [06:14<04:13,  1.32it/s][A
 78%|███████▊  | 1198/1531 [06:15<04:12,  1.32it/s][A
 78%|███████▊  | 1199/1531 [06:16<04:12,  1.32it/s][A
 78%|███████▊  | 1200/1531 [06:16<04:03,  1.36it/s][A
 78%|███████▊  | 1201/1531 [06:17<04:04,  1.35it/s][A
 79%|███████▊  | 1202/1531 [06:18<04:06,  1.34it/s][A
 79%|███████▊  | 1203/1531 [06:19<03:55,  1.39it/s][A
 79%|███████▊  | 1204/1531 [06:19<03:58,  1.37it/s][A
 79%|███████▊  | 1205/1531 [06:20<03:59,  1.36it/s][A
 79%|███████▉  | 1206/1531 [06:21<04:01,  1.35it/s][A
 79%|███████▉  | 1207/1531 [06:22<04:06,  1.31it/s][A
 79%|███████▉  | 1208/1531 [06:22<04:04,  1.32it/s][A
 79%|███████▉  | 1209/1531 [06:23<03:53,  1.38it/s][A
 79%|███████▉  | 1210/1531 [06:24<03:54,  1.37it/s][A
 79%|███████▉  | 1211/1531 [06:25<03:46,  1.41it/s][A
 79%|███████▉  | 1212/1531 [06:25<03:55,  1.35it/s][A
 79%|███████▉  | 1213/1531 [06:26<04:01,  1.32it/s][A
 79%|███████▉  | 1214/1531 [06:27<04:00,  1.32it/s][A
 79%|███████▉  | 1215/1531 [06:28<03:56,  1.34it/s][A
 79%|███████▉  | 1216/1531 [06:28<04:01,  1.30it/s][A
 79%|███████▉  | 1217/1531 [06:29<04:05,  1.28it/s][A
 80%|███████▉  | 1218/1531 [06:30<04:08,  1.26it/s][A
 80%|███████▉  | 1219/1531 [06:31<04:04,  1.28it/s][A
 80%|███████▉  | 1220/1531 [06:32<03:56,  1.31it/s][A
 80%|███████▉  | 1221/1531 [06:32<03:59,  1.30it/s][A
 80%|███████▉  | 1222/1531 [06:33<03:56,  1.30it/s][A
 80%|███████▉  | 1223/1531 [06:34<03:53,  1.32it/s][A
 80%|███████▉  | 1224/1531 [06:34<03:42,  1.38it/s][A
 80%|████████  | 1225/1531 [06:35<03:44,  1.36it/s][A
 80%|████████  | 1226/1531 [06:36<03:46,  1.35it/s][A
 80%|████████  | 1227/1531 [06:37<03:47,  1.34it/s][A
 80%|████████  | 1228/1531 [06:37<03:46,  1.34it/s][A
 80%|████████  | 1229/1531 [06:38<03:44,  1.35it/s][A
 80%|████████  | 1230/1531 [06:39<03:45,  1.33it/s][A
 80%|████████  | 1231/1531 [06:40<03:41,  1.36it/s][A
 80%|████████  | 1232/1531 [06:40<03:42,  1.34it/s][A
 81%|████████  | 1233/1531 [06:41<03:41,  1.34it/s][A
 81%|████████  | 1234/1531 [06:42<03:35,  1.38it/s][A
 81%|████████  | 1235/1531 [06:43<03:36,  1.37it/s][A
 81%|████████  | 1236/1531 [06:43<03:37,  1.36it/s][A
 81%|████████  | 1237/1531 [06:44<03:36,  1.36it/s][A
 81%|████████  | 1238/1531 [06:45<03:34,  1.36it/s][A
 81%|████████  | 1239/1531 [06:46<03:36,  1.35it/s][A
 81%|████████  | 1240/1531 [06:46<03:41,  1.31it/s][A
 81%|████████  | 1241/1531 [06:47<03:44,  1.29it/s][A
 81%|████████  | 1242/1531 [06:48<03:41,  1.30it/s][A
 81%|████████  | 1243/1531 [06:49<03:39,  1.31it/s][A
 81%|████████▏ | 1244/1531 [06:49<03:38,  1.31it/s][A
 81%|████████▏ | 1245/1531 [06:50<03:36,  1.32it/s][A
 81%|████████▏ | 1246/1531 [06:51<03:34,  1.33it/s][A
 81%|████████▏ | 1247/1531 [06:52<03:34,  1.32it/s][A
 82%|████████▏ | 1248/1531 [06:52<03:33,  1.33it/s][A
 82%|████████▏ | 1249/1531 [06:53<03:39,  1.28it/s][A
 82%|████████▏ | 1250/1531 [06:54<03:35,  1.31it/s][A
 82%|████████▏ | 1251/1531 [06:55<03:30,  1.33it/s][A
 82%|████████▏ | 1252/1531 [06:56<03:29,  1.33it/s][A
 82%|████████▏ | 1253/1531 [06:56<03:33,  1.30it/s][A
 82%|████████▏ | 1254/1531 [06:57<03:30,  1.32it/s][A
 82%|████████▏ | 1255/1531 [06:58<03:27,  1.33it/s][A
 82%|████████▏ | 1256/1531 [06:59<03:27,  1.32it/s][A
 82%|████████▏ | 1257/1531 [06:59<03:26,  1.33it/s][A
 82%|████████▏ | 1258/1531 [07:00<03:20,  1.36it/s][A
 82%|████████▏ | 1259/1531 [07:01<03:20,  1.36it/s][A
 82%|████████▏ | 1260/1531 [07:02<03:27,  1.31it/s][A
 82%|████████▏ | 1261/1531 [07:02<03:22,  1.33it/s][A
 82%|████████▏ | 1262/1531 [07:03<03:28,  1.29it/s][A
 82%|████████▏ | 1263/1531 [07:04<03:30,  1.28it/s][A
 83%|████████▎ | 1264/1531 [07:05<03:26,  1.29it/s][A
 83%|████████▎ | 1265/1531 [07:05<03:23,  1.31it/s][A
 83%|████████▎ | 1266/1531 [07:06<03:26,  1.28it/s][A
 83%|████████▎ | 1267/1531 [07:07<03:29,  1.26it/s][A
 83%|████████▎ | 1268/1531 [07:08<03:25,  1.28it/s][A
 83%|████████▎ | 1269/1531 [07:09<03:29,  1.25it/s][A
 83%|████████▎ | 1270/1531 [07:09<03:24,  1.28it/s][A
 83%|████████▎ | 1271/1531 [07:10<03:21,  1.29it/s][A
 83%|████████▎ | 1272/1531 [07:11<03:25,  1.26it/s][A
 83%|████████▎ | 1273/1531 [07:12<03:20,  1.29it/s][A
 83%|████████▎ | 1274/1531 [07:12<03:16,  1.31it/s][A
 83%|████████▎ | 1275/1531 [07:13<03:12,  1.33it/s][A
 83%|████████▎ | 1276/1531 [07:14<03:09,  1.35it/s][A
 83%|████████▎ | 1277/1531 [07:15<03:08,  1.35it/s][A
 83%|████████▎ | 1278/1531 [07:15<03:08,  1.34it/s][A
 84%|████████▎ | 1279/1531 [07:16<03:11,  1.31it/s][A
 84%|████████▎ | 1280/1531 [07:17<03:09,  1.32it/s][A
 84%|████████▎ | 1281/1531 [07:18<03:06,  1.34it/s][A
 84%|████████▎ | 1282/1531 [07:18<03:10,  1.31it/s][A
 84%|████████▍ | 1283/1531 [07:19<03:05,  1.33it/s][A
 84%|████████▍ | 1284/1531 [07:20<03:09,  1.30it/s][A
 84%|████████▍ | 1285/1531 [07:21<03:06,  1.32it/s][A
 84%|████████▍ | 1286/1531 [07:21<03:02,  1.34it/s][A
 84%|████████▍ | 1287/1531 [07:22<03:07,  1.30it/s][A
 84%|████████▍ | 1288/1531 [07:23<03:05,  1.31it/s][A
 84%|████████▍ | 1289/1531 [07:24<03:00,  1.34it/s][A
 84%|████████▍ | 1290/1531 [07:25<03:00,  1.33it/s][A
 84%|████████▍ | 1291/1531 [07:25<02:58,  1.35it/s][A
 84%|████████▍ | 1292/1531 [07:26<02:56,  1.36it/s][A
 84%|████████▍ | 1293/1531 [07:27<03:01,  1.31it/s][A
 85%|████████▍ | 1294/1531 [07:28<03:00,  1.31it/s][A
 85%|████████▍ | 1295/1531 [07:28<02:58,  1.33it/s][A
 85%|████████▍ | 1296/1531 [07:29<02:57,  1.33it/s][A
 85%|████████▍ | 1297/1531 [07:30<02:54,  1.34it/s][A
 85%|████████▍ | 1298/1531 [07:30<02:54,  1.34it/s][A
 85%|████████▍ | 1299/1531 [07:31<02:48,  1.37it/s][A
 85%|████████▍ | 1300/1531 [07:32<02:48,  1.37it/s][A
 85%|████████▍ | 1301/1531 [07:33<02:50,  1.35it/s][A
 85%|████████▌ | 1302/1531 [07:33<02:48,  1.36it/s][A
 85%|████████▌ | 1303/1531 [07:34<02:48,  1.35it/s][A
 85%|████████▌ | 1304/1531 [07:35<02:41,  1.41it/s][A
 85%|████████▌ | 1305/1531 [07:36<02:47,  1.35it/s][A
 85%|████████▌ | 1306/1531 [07:36<02:46,  1.35it/s][A
 85%|████████▌ | 1307/1531 [07:37<02:51,  1.31it/s][A
 85%|████████▌ | 1308/1531 [07:38<02:53,  1.29it/s][A
 85%|████████▌ | 1309/1531 [07:39<02:51,  1.30it/s][A
 86%|████████▌ | 1310/1531 [07:39<02:46,  1.32it/s][A
 86%|████████▌ | 1311/1531 [07:40<02:46,  1.32it/s][A
 86%|████████▌ | 1312/1531 [07:41<02:45,  1.32it/s][A
 86%|████████▌ | 1313/1531 [07:42<02:41,  1.35it/s][A
 86%|████████▌ | 1314/1531 [07:42<02:41,  1.34it/s][A
 86%|████████▌ | 1315/1531 [07:43<02:36,  1.38it/s][A
 86%|████████▌ | 1316/1531 [07:44<02:35,  1.38it/s][A
 86%|████████▌ | 1317/1531 [07:45<02:40,  1.33it/s][A
 86%|████████▌ | 1318/1531 [07:45<02:32,  1.39it/s][A
 86%|████████▌ | 1319/1531 [07:46<02:37,  1.34it/s][A
 86%|████████▌ | 1320/1531 [07:47<02:37,  1.34it/s][A
 86%|████████▋ | 1321/1531 [07:48<02:36,  1.34it/s][A
 86%|████████▋ | 1322/1531 [07:48<02:35,  1.34it/s][A
 86%|████████▋ | 1323/1531 [07:49<02:18,  1.51it/s][A
 86%|████████▋ | 1324/1531 [07:49<02:09,  1.60it/s][A
 87%|████████▋ | 1325/1531 [07:50<02:04,  1.66it/s][A
 87%|████████▋ | 1326/1531 [07:50<02:00,  1.69it/s][A
 87%|████████▋ | 1327/1531 [07:51<01:58,  1.73it/s][A
 87%|████████▋ | 1328/1531 [07:51<01:51,  1.82it/s][A
 87%|████████▋ | 1329/1531 [07:52<01:47,  1.88it/s][A
 87%|████████▋ | 1330/1531 [07:52<01:43,  1.95it/s][A
 87%|████████▋ | 1331/1531 [07:53<01:45,  1.90it/s][A
 87%|████████▋ | 1332/1531 [07:53<01:42,  1.94it/s][A
 87%|████████▋ | 1333/1531 [07:54<01:43,  1.91it/s][A
 87%|████████▋ | 1334/1531 [07:55<01:40,  1.95it/s][A
 87%|████████▋ | 1335/1531 [07:55<01:42,  1.91it/s][A
 87%|████████▋ | 1336/1531 [07:56<01:39,  1.95it/s][A
 87%|████████▋ | 1337/1531 [07:56<01:43,  1.88it/s][A
 87%|████████▋ | 1338/1531 [07:57<01:44,  1.85it/s][A
 87%|████████▋ | 1339/1531 [07:57<01:39,  1.93it/s][A
 88%|████████▊ | 1340/1531 [07:58<01:40,  1.91it/s][A
 88%|████████▊ | 1341/1531 [07:58<01:43,  1.84it/s][A
 88%|████████▊ | 1342/1531 [07:59<01:39,  1.90it/s][A
 88%|████████▊ | 1343/1531 [07:59<01:40,  1.87it/s][A
 88%|████████▊ | 1344/1531 [08:00<01:40,  1.86it/s][A
 88%|████████▊ | 1345/1531 [08:00<01:42,  1.82it/s][A
 88%|████████▊ | 1346/1531 [08:01<01:37,  1.89it/s][A
 88%|████████▊ | 1347/1531 [08:01<01:35,  1.93it/s][A
 88%|████████▊ | 1348/1531 [08:02<01:32,  1.97it/s][A
 88%|████████▊ | 1349/1531 [08:02<01:34,  1.92it/s][A
 88%|████████▊ | 1350/1531 [08:03<01:32,  1.96it/s][A
 88%|████████▊ | 1351/1531 [08:03<01:33,  1.93it/s][A
 88%|████████▊ | 1352/1531 [08:04<01:35,  1.88it/s][A
 88%|████████▊ | 1353/1531 [08:05<01:32,  1.92it/s][A
 88%|████████▊ | 1354/1531 [08:05<01:20,  2.20it/s][A
 89%|████████▊ | 1355/1531 [08:05<01:09,  2.52it/s][A
 89%|████████▊ | 1356/1531 [08:05<01:07,  2.61it/s][A
 89%|████████▊ | 1357/1531 [08:06<01:02,  2.77it/s][A
 89%|████████▊ | 1358/1531 [08:06<00:59,  2.90it/s][A
 89%|████████▉ | 1359/1531 [08:06<00:55,  3.11it/s][A
 89%|████████▉ | 1360/1531 [08:07<00:54,  3.16it/s][A
 89%|████████▉ | 1361/1531 [08:07<00:51,  3.32it/s][A
 89%|████████▉ | 1362/1531 [08:07<00:51,  3.31it/s][A
 89%|████████▉ | 1363/1531 [08:08<00:51,  3.28it/s][A
 89%|████████▉ | 1364/1531 [08:08<00:51,  3.27it/s][A
 89%|████████▉ | 1365/1531 [08:08<00:51,  3.25it/s][A
 89%|████████▉ | 1366/1531 [08:08<00:48,  3.38it/s][A
 89%|████████▉ | 1367/1531 [08:09<00:49,  3.33it/s][A
 89%|████████▉ | 1368/1531 [08:09<00:47,  3.46it/s][A
 89%|████████▉ | 1369/1531 [08:09<00:45,  3.56it/s][A
 89%|████████▉ | 1370/1531 [08:10<00:48,  3.30it/s][A
 90%|████████▉ | 1371/1531 [08:10<00:46,  3.41it/s][A
 90%|████████▉ | 1372/1531 [08:10<00:47,  3.32it/s][A
 90%|████████▉ | 1373/1531 [08:10<00:45,  3.45it/s][A
 90%|████████▉ | 1374/1531 [08:11<00:46,  3.39it/s][A
 90%|████████▉ | 1375/1531 [08:11<00:46,  3.36it/s][A
 90%|████████▉ | 1376/1531 [08:11<00:44,  3.50it/s][A
 90%|████████▉ | 1377/1531 [08:12<00:44,  3.43it/s][A
 90%|█████████ | 1378/1531 [08:12<00:43,  3.51it/s][A
 90%|█████████ | 1379/1531 [08:12<00:42,  3.60it/s][A
 90%|█████████ | 1380/1531 [08:12<00:43,  3.50it/s][A
 90%|█████████ | 1381/1531 [08:13<00:41,  3.58it/s][A
 90%|█████████ | 1382/1531 [08:13<00:40,  3.64it/s][A
 90%|█████████ | 1383/1531 [08:13<00:41,  3.53it/s][A
 90%|█████████ | 1384/1531 [08:14<00:42,  3.44it/s][A
 90%|█████████ | 1385/1531 [08:14<00:41,  3.50it/s][A
 91%|█████████ | 1386/1531 [08:14<00:40,  3.55it/s][A
 91%|█████████ | 1387/1531 [08:14<00:41,  3.48it/s][A
 91%|█████████ | 1388/1531 [08:15<00:40,  3.56it/s][A
 91%|█████████ | 1389/1531 [08:15<00:40,  3.47it/s][A
 91%|█████████ | 1390/1531 [08:15<00:39,  3.55it/s][A
 91%|█████████ | 1391/1531 [08:16<00:38,  3.62it/s][A
 91%|█████████ | 1392/1531 [08:16<00:39,  3.52it/s][A
 91%|█████████ | 1393/1531 [08:16<00:38,  3.59it/s][A
 91%|█████████ | 1394/1531 [08:16<00:37,  3.65it/s][A
 91%|█████████ | 1395/1531 [08:17<00:39,  3.48it/s][A
 91%|█████████ | 1396/1531 [08:17<00:37,  3.56it/s][A
 91%|█████████ | 1397/1531 [08:17<00:37,  3.62it/s][A
 91%|█████████▏| 1398/1531 [08:18<00:37,  3.52it/s][A
 91%|█████████▏| 1399/1531 [08:18<00:38,  3.45it/s][A
 91%|█████████▏| 1400/1531 [08:18<00:36,  3.55it/s][A
 92%|█████████▏| 1401/1531 [08:18<00:37,  3.43it/s][A
 92%|█████████▏| 1402/1531 [08:19<00:36,  3.53it/s][A
 92%|█████████▏| 1403/1531 [08:19<00:35,  3.60it/s][A
 92%|█████████▏| 1404/1531 [08:19<00:36,  3.49it/s][A
 92%|█████████▏| 1405/1531 [08:20<00:36,  3.43it/s][A
 92%|█████████▏| 1406/1531 [08:20<00:37,  3.37it/s][A
 92%|█████████▏| 1407/1531 [08:20<00:35,  3.49it/s][A
 92%|█████████▏| 1408/1531 [08:20<00:36,  3.41it/s][A
 92%|█████████▏| 1409/1531 [08:21<00:36,  3.35it/s][A
 92%|█████████▏| 1410/1531 [08:21<00:36,  3.34it/s][A
 92%|█████████▏| 1411/1531 [08:21<00:34,  3.46it/s][A
 92%|█████████▏| 1412/1531 [08:22<00:35,  3.40it/s][A
 92%|█████████▏| 1413/1531 [08:22<00:35,  3.34it/s][A
 92%|█████████▏| 1414/1531 [08:22<00:35,  3.33it/s][A
 92%|█████████▏| 1415/1531 [08:23<00:35,  3.30it/s][A
 92%|█████████▏| 1416/1531 [08:23<00:33,  3.43it/s][A
 93%|█████████▎| 1417/1531 [08:23<00:32,  3.54it/s][A
 93%|█████████▎| 1418/1531 [08:23<00:32,  3.45it/s][A
 93%|█████████▎| 1419/1531 [08:24<00:33,  3.36it/s][A
 93%|█████████▎| 1420/1531 [08:24<00:31,  3.47it/s][A
 93%|█████████▎| 1421/1531 [08:24<00:30,  3.55it/s][A
 93%|█████████▎| 1422/1531 [08:25<00:31,  3.48it/s][A
 93%|█████████▎| 1423/1531 [08:25<00:29,  3.63it/s][A
 93%|█████████▎| 1424/1531 [08:25<00:28,  3.75it/s][A
 93%|█████████▎| 1425/1531 [08:25<00:27,  3.85it/s][A
 93%|█████████▎| 1426/1531 [08:26<00:26,  3.93it/s][A
 93%|█████████▎| 1427/1531 [08:26<00:26,  3.98it/s][A
 93%|█████████▎| 1428/1531 [08:26<00:25,  4.00it/s][A
 93%|█████████▎| 1429/1531 [08:26<00:25,  4.01it/s][A
 93%|█████████▎| 1430/1531 [08:26<00:24,  4.05it/s][A
 93%|█████████▎| 1431/1531 [08:27<00:26,  3.79it/s][A
 94%|█████████▎| 1432/1531 [08:27<00:26,  3.80it/s][A
 94%|█████████▎| 1433/1531 [08:27<00:25,  3.86it/s][A
 94%|█████████▎| 1434/1531 [08:28<00:24,  3.91it/s][A
 94%|█████████▎| 1435/1531 [08:28<00:33,  2.89it/s][A
 94%|█████████▍| 1436/1531 [08:29<00:41,  2.30it/s][A
 94%|█████████▍| 1437/1531 [08:29<00:44,  2.11it/s][A
 94%|█████████▍| 1438/1531 [08:30<00:47,  1.96it/s][A
 94%|█████████▍| 1439/1531 [08:30<00:47,  1.92it/s][A
 94%|█████████▍| 1440/1531 [08:31<00:47,  1.90it/s][A
 94%|█████████▍| 1441/1531 [08:32<00:47,  1.88it/s][A
 94%|█████████▍| 1442/1531 [08:32<00:47,  1.87it/s][A
 94%|█████████▍| 1443/1531 [08:33<00:48,  1.82it/s][A
 94%|█████████▍| 1444/1531 [08:33<00:47,  1.83it/s][A
 94%|█████████▍| 1445/1531 [08:34<00:47,  1.82it/s][A
 94%|█████████▍| 1446/1531 [08:34<00:46,  1.82it/s][A
 95%|█████████▍| 1447/1531 [08:35<00:46,  1.82it/s][A
 95%|█████████▍| 1448/1531 [08:35<00:45,  1.83it/s][A
 95%|█████████▍| 1449/1531 [08:36<00:44,  1.83it/s][A
 95%|█████████▍| 1450/1531 [08:36<00:44,  1.82it/s][A
 95%|█████████▍| 1451/1531 [08:37<00:43,  1.83it/s][A
 95%|█████████▍| 1452/1531 [08:38<00:44,  1.79it/s][A
 95%|█████████▍| 1453/1531 [08:38<00:43,  1.79it/s][A
 95%|█████████▍| 1454/1531 [08:39<00:42,  1.81it/s][A
 95%|█████████▌| 1455/1531 [08:39<00:42,  1.77it/s][A
 95%|█████████▌| 1456/1531 [08:40<00:42,  1.78it/s][A
 95%|█████████▌| 1457/1531 [08:40<00:42,  1.76it/s][A
 95%|█████████▌| 1458/1531 [08:41<00:41,  1.75it/s][A
 95%|█████████▌| 1459/1531 [08:42<00:41,  1.74it/s][A
 95%|█████████▌| 1460/1531 [08:42<00:40,  1.76it/s][A
 95%|█████████▌| 1461/1531 [08:43<00:38,  1.84it/s][A
 95%|█████████▌| 1462/1531 [08:43<00:31,  2.21it/s][A
 96%|█████████▌| 1463/1531 [08:43<00:26,  2.55it/s][A
 96%|█████████▌| 1464/1531 [08:43<00:23,  2.87it/s][A
 96%|█████████▌| 1465/1531 [08:44<00:21,  3.14it/s][A
 96%|█████████▌| 1466/1531 [08:44<00:19,  3.36it/s][A
 96%|█████████▌| 1467/1531 [08:44<00:18,  3.51it/s][A
 96%|█████████▌| 1468/1531 [08:44<00:17,  3.64it/s][A
 96%|█████████▌| 1469/1531 [08:45<00:16,  3.77it/s][A
 96%|█████████▌| 1470/1531 [08:45<00:16,  3.80it/s][A
 96%|█████████▌| 1471/1531 [08:45<00:15,  3.86it/s][A
 96%|█████████▌| 1472/1531 [08:45<00:15,  3.90it/s][A
 96%|█████████▌| 1473/1531 [08:46<00:14,  3.90it/s][A
 96%|█████████▋| 1474/1531 [08:46<00:14,  3.94it/s][A
 96%|█████████▋| 1475/1531 [08:46<00:14,  3.96it/s][A
 96%|█████████▋| 1476/1531 [08:46<00:13,  3.98it/s][A
 96%|█████████▋| 1477/1531 [08:47<00:13,  3.99it/s][A
 97%|█████████▋| 1478/1531 [08:47<00:13,  3.97it/s][A
 97%|█████████▋| 1479/1531 [08:47<00:13,  3.98it/s][A
 97%|█████████▋| 1480/1531 [08:47<00:12,  3.99it/s][A
 97%|█████████▋| 1481/1531 [08:48<00:12,  3.97it/s][A
 97%|█████████▋| 1482/1531 [08:48<00:12,  3.98it/s][A
 97%|█████████▋| 1483/1531 [08:48<00:12,  3.99it/s][A
 97%|█████████▋| 1484/1531 [08:48<00:11,  4.00it/s][A
 97%|█████████▋| 1485/1531 [08:49<00:11,  4.00it/s][A
 97%|█████████▋| 1486/1531 [08:49<00:11,  4.01it/s][A
 97%|█████████▋| 1487/1531 [08:49<00:10,  4.02it/s][A
 97%|█████████▋| 1488/1531 [08:49<00:10,  4.03it/s][A
 97%|█████████▋| 1489/1531 [08:50<00:10,  4.02it/s][A
 97%|█████████▋| 1490/1531 [08:50<00:10,  4.02it/s][A
 97%|█████████▋| 1491/1531 [08:50<00:09,  4.02it/s][A
 97%|█████████▋| 1492/1531 [08:50<00:09,  4.03it/s][A
 98%|█████████▊| 1493/1531 [08:51<00:09,  4.03it/s][A
 98%|█████████▊| 1494/1531 [08:51<00:09,  4.05it/s][A
 98%|█████████▊| 1495/1531 [08:51<00:08,  4.07it/s][A
 98%|█████████▊| 1496/1531 [08:51<00:08,  4.00it/s][A
 98%|█████████▊| 1497/1531 [08:52<00:08,  4.03it/s][A
 98%|█████████▊| 1498/1531 [08:52<00:08,  4.06it/s][A
 98%|█████████▊| 1499/1531 [08:52<00:07,  4.10it/s][A
 98%|█████████▊| 1500/1531 [08:52<00:07,  4.05it/s][A
 98%|█████████▊| 1501/1531 [08:53<00:07,  4.07it/s][A
 98%|█████████▊| 1502/1531 [08:53<00:07,  4.09it/s][A
 98%|█████████▊| 1503/1531 [08:53<00:06,  4.11it/s][A
 98%|█████████▊| 1504/1531 [08:53<00:06,  4.11it/s][A
 98%|█████████▊| 1505/1531 [08:54<00:06,  4.07it/s][A
 98%|█████████▊| 1506/1531 [08:54<00:06,  4.14it/s][A
 98%|█████████▊| 1507/1531 [08:54<00:05,  4.24it/s][A
 98%|█████████▊| 1508/1531 [08:54<00:05,  4.22it/s][A
 99%|█████████▊| 1509/1531 [08:55<00:05,  4.10it/s][A
 99%|█████████▊| 1510/1531 [08:55<00:04,  4.24it/s][A
 99%|█████████▊| 1511/1531 [08:55<00:04,  4.21it/s][A
 99%|█████████▉| 1512/1531 [08:55<00:04,  4.17it/s][A
 99%|█████████▉| 1513/1531 [08:55<00:04,  4.41it/s][A
 99%|█████████▉| 1514/1531 [08:56<00:03,  4.58it/s][A
 99%|█████████▉| 1515/1531 [08:56<00:03,  4.72it/s][A
 99%|█████████▉| 1516/1531 [08:56<00:03,  4.83it/s][A
 99%|█████████▉| 1517/1531 [08:56<00:02,  4.94it/s][A
 99%|█████████▉| 1518/1531 [08:56<00:02,  5.01it/s][A
 99%|█████████▉| 1519/1531 [08:57<00:02,  5.07it/s][A
 99%|█████████▉| 1520/1531 [08:57<00:02,  5.10it/s][A
 99%|█████████▉| 1521/1531 [08:57<00:02,  4.98it/s][A
 99%|█████████▉| 1522/1531 [08:57<00:01,  5.03it/s][A
 99%|█████████▉| 1523/1531 [08:57<00:01,  4.93it/s][A
100%|█████████▉| 1524/1531 [08:58<00:01,  5.01it/s][A
100%|█████████▉| 1525/1531 [08:58<00:01,  5.01it/s][A
100%|█████████▉| 1526/1531 [08:58<00:00,  5.04it/s][A
100%|█████████▉| 1527/1531 [08:58<00:00,  5.07it/s][A
100%|█████████▉| 1528/1531 [08:58<00:00,  5.11it/s][A
100%|█████████▉| 1529/1531 [08:59<00:00,  5.12it/s][A
100%|█████████▉| 1530/1531 [08:59<00:00,  5.13it/s][A
100%|██████████| 1531/1531 [08:59<00:00,  5.17it/s][A100%|██████████| 1531/1531 [08:59<00:00,  2.84it/s]
                                                        10%|▉         | 187/1875 [1:47:49<10:46:31, 22.98s/it] 10%|█         | 188/1875 [1:48:26<115:49:32, 247.17s/it] 10%|█         | 189/1875 [1:49:20<88:41:21, 189.37s/it]  10%|█         | 190/1875 [1:50:01<67:48:27, 144.87s/it] 10%|█         | 191/1875 [1:50:31<51:33:06, 110.21s/it] 10%|█         | 192/1875 [1:50:49<38:40:02, 82.71s/it]  10%|█         | 193/1875 [1:51:02<28:50:40, 61.74s/it] 10%|█         | 194/1875 [1:51:26<23:35:15, 50.51s/it] 10%|█         | 195/1875 [1:52:21<24:11:32, 51.84s/it] 10%|█         | 196/1875 [1:53:03<22:48:26, 48.90s/it] 11%|█         | 197/1875 [1:53:33<20:02:19, 42.99s/it] 11%|█         | 198/1875 [1:53:54<16:57:45, 36.41s/it] 11%|█         | 199/1875 [1:54:07<13:43:49, 29.49s/it] 11%|█         | 200/1875 [1:54:19<11:14:10, 24.15s/it]                                                        11%|█         | 200/1875 [1:54:19<11:14:10, 24.15s/it] 11%|█         | 201/1875 [1:55:18<16:11:55, 34.84s/it] 11%|█         | 202/1875 [1:56:07<18:08:49, 39.05s/it] 11%|█         | 203/1875 [1:56:40<17:17:40, 37.24s/it] 11%|█         | 204/1875 [1:57:03<15:13:06, 32.79s/it] 11%|█         | 205/1875 [1:57:17<12:40:11, 27.31s/it] 11%|█         | 206/1875 [1:57:29<10:32:01, 22.72s/it] 11%|█         | 207/1875 [1:58:18<14:08:23, 30.52s/it] 11%|█         | 208/1875 [1:59:11<17:18:12, 37.37s/it] 11%|█         | 209/1875 [1:59:49<17:20:15, 37.46s/it] 11%|█         | 210/1875 [2:00:16<15:56:38, 34.47s/it] 11%|█▏        | 211/1875 [2:00:33<13:28:44, 29.16s/it] 11%|█▏        | 212/1875 [2:00:45<11:07:36, 24.09s/it] 11%|█▏        | 213/1875 [2:01:23<12:57:52, 28.08s/it] 11%|█▏        | 214/1875 [2:02:14<16:11:18, 35.09s/it] 11%|█▏        | 215/1875 [2:02:53<16:37:44, 36.06s/it] 12%|█▏        | 216/1875 [2:03:21<15:31:36, 33.69s/it] 12%|█▏        | 217/1875 [2:03:38<13:17:59, 28.88s/it] 12%|█▏        | 218/1875 [2:03:51<11:04:03, 24.05s/it] 12%|█▏        | 219/1875 [2:04:16<11:05:48, 24.12s/it]Terminated
./scripts/try.sh: 52: \: not found
